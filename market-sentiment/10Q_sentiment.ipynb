{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21448bdd",
   "metadata": {},
   "source": [
    "# Market Sentiment with Alpha Vantage\n",
    "\n",
    "So far in this code I've built the basic pipeline to just pull news articles from Alpha Vantage and then pass them into finBERT before putting all of that data into a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f5cd",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Set up all the environment variables and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "adf696f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install requests pandas numpy transformers sentence-transformers beautifulsoup4 lxml python-dotenv\n",
    "import os, re, time, math, json, requests, numpy as np, pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from bs4.builder import XMLParsedAsHTMLWarning\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# ML (FinBERT)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "load_dotenv()\n",
    "SEC_EMAIL = os.getenv(\"SEC_EMAIL\", \"email@example.com\")\n",
    "POLYGON_KEY = os.getenv(\"POLYGON_API_KEY\")  # <- put your Polygon key in .env\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251adc1",
   "metadata": {},
   "source": [
    "### Config & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "45a048eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "FINBERT = \"yiyanghkust/finbert-tone\"\n",
    "ALPHA_SECTIONS = [  # regexes to pull high-signal parts\n",
    "    (r\"item\\s+2\\.\\s*management[’']?s discussion and analysis.*?(?=item\\s+3\\.)\", \"MD&A\"),\n",
    "    (r\"item\\s+1a\\.\\s*risk factors.*?(?=item\\s+2\\.)\", \"RiskFactors\"),\n",
    "    (r\"results of operations.*?(?=liquidity|capital resources|item\\s+\\d)\", \"Results\"),\n",
    "]\n",
    "POS_PHRASES = [\n",
    "    r\"strong demand\", r\"margin expansion\", r\"raised guidance\", r\"record (revenue|earnings)\",\n",
    "    r\"cost (reductions|optimization)\", r\"share repurchase\", r\"cash flow (improved|growth)\"\n",
    "]\n",
    "NEG_PHRASES = [\n",
    "    r\"decline in (sales|revenue)\", r\"margin compression\", r\"impairment charge\",\n",
    "    r\"supply chain disruption\", r\"adversely affected\", r\"weaker demand\", r\"material weakness\"\n",
    "]\n",
    "\n",
    "def SEC_HEADERS():\n",
    "    return {\n",
    "        \"User-Agent\": f\"CrowdQuant Backtest (contact: {SEC_EMAIL})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "\n",
    "def _make_soup(html: str) -> BeautifulSoup:\n",
    "    for parser in (\"lxml\",\"html5lib\",\"html.parser\"):\n",
    "        try: return BeautifulSoup(html, parser)\n",
    "        except Exception: pass\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def _lower_clean(txt: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\",\" \", txt.lower())\n",
    "\n",
    "def _token_chunks(text: str, tokenizer, max_tokens=512, stride=32):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    step = max_tokens - stride\n",
    "    for i in range(0, len(ids), step):\n",
    "        window = ids[i:i+max_tokens]\n",
    "        if not window:\n",
    "            break\n",
    "        yield tokenizer.decode(window, skip_special_tokens=True)\n",
    "\n",
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    pipe = TextClassificationPipeline(\n",
    "        model=mdl, tokenizer=tok, top_k=None,  # returns all labels\n",
    "        truncation=True\n",
    "    )\n",
    "    return pipe, tok\n",
    "\n",
    "FINBERT_PIPE, FINBERT_TOK = load_finbert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88d71",
   "metadata": {},
   "source": [
    "### SEC stuff\n",
    "Find CIK codes per ticker, get 10Qs, fetch HTML and extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5adf6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    js = requests.get(url, headers=SEC_HEADERS(), timeout=30).json()\n",
    "    t = ticker.upper()\n",
    "    for _, rec in js.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "def list_10q_with_dates(cik: str, max_n=8):\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    rec = r.json().get(\"filings\",{}).get(\"recent\",{})\n",
    "    out = []\n",
    "    for form, acc, prim, fdate in zip(rec.get(\"form\",[]),\n",
    "                                      rec.get(\"accessionNumber\",[]),\n",
    "                                      rec.get(\"primaryDocument\",[]),\n",
    "                                      rec.get(\"filingDate\",[])):\n",
    "        if form == \"10-Q\":\n",
    "            out.append({\n",
    "                \"accession\": acc.replace(\"-\",\"\"),\n",
    "                \"primary\": prim,\n",
    "                \"filing_date\": fdate,  # YYYY-MM-DD (UTC)\n",
    "            })\n",
    "        if len(out) >= max_n: break\n",
    "    return out\n",
    "\n",
    "def fetch_filing_html(cik:str, accession:str, primary:str) -> str:\n",
    "    base = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession}\"\n",
    "    url  = f\"{base}/{primary}\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=60); r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def extract_sections(html: str, patterns=ALPHA_SECTIONS, fallback_full=True, cap=60000) -> dict:\n",
    "    soup = _make_soup(html)\n",
    "    txt  = soup.get_text(\"\\n\", strip=True)\n",
    "    low  = _lower_clean(txt)\n",
    "    out = {}\n",
    "    for pat, name in patterns:\n",
    "        m = re.search(pat, low, flags=re.S)\n",
    "        if m:\n",
    "            out[name] = low[m.start():m.end()][:cap]\n",
    "    if not out and fallback_full:\n",
    "        out[\"FullDocument\"] = low[:cap]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3defa",
   "metadata": {},
   "source": [
    "### Finbert scoring -> features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e92ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_sent(text: str) -> dict:\n",
    "    # delegate to chunked scorer with globals\n",
    "    return finbert_sent_long(text, FINBERT_PIPE, FINBERT_TOK, max_tokens=512, batch=8)\n",
    "\n",
    "def finbert_sent_long(text: str, pipe, tokenizer, max_tokens=512, batch=16):\n",
    "    # short case\n",
    "    if len(text) < 4000:  # rough char heuristic; still truncated by pipeline\n",
    "        rows = pipe([text], truncation=True, max_length=max_tokens)\n",
    "    else:\n",
    "        chunks = list(_token_chunks(text, tokenizer, max_tokens=max_tokens))\n",
    "        rows = []\n",
    "        for i in range(0, len(chunks), batch):\n",
    "            rows.extend(pipe(chunks[i:i+batch], truncation=True, max_length=max_tokens))\n",
    "    # rows is a list of [ {label,score}... ] per chunk\n",
    "    pos = neu = neg = 0.0\n",
    "    for r in rows:\n",
    "        d = {x[\"label\"].lower(): x[\"score\"] for x in r}\n",
    "        pos += d.get(\"positive\", 0.0)\n",
    "        neu += d.get(\"neutral\", 0.0)\n",
    "        neg += d.get(\"negative\", 0.0)\n",
    "    n = max(1, len(rows))\n",
    "    return {\"pos\": pos/n, \"neu\": neu/n, \"neg\": neg/n, \"sent_score\": (pos/n - neg/n)}\n",
    "\n",
    "def phrase_boost(text: str, pos_list=POS_PHRASES, neg_list=NEG_PHRASES, w=0.1) -> float:\n",
    "    boost = 0.0\n",
    "    for p in pos_list:\n",
    "        if re.search(p, text, flags=re.I): boost += w\n",
    "    for n in neg_list:\n",
    "        if re.search(n, text, flags=re.I): boost -= w\n",
    "    return boost\n",
    "\n",
    "def score_sections(sections: dict) -> dict:\n",
    "    feats = {}\n",
    "    for name, text in sections.items():\n",
    "        fb = finbert_sent(text)\n",
    "        boost = phrase_boost(text)\n",
    "        feats[f\"{name}_pos\"] = fb[\"pos\"]\n",
    "        feats[f\"{name}_neg\"] = fb[\"neg\"]\n",
    "        base = fb.get(\"sent_score\", fb.get(\"sent\", 0.0))  # support both keys\n",
    "        feats[f\"{name}_sent\"] = base + boost\n",
    "        feats[f\"{name}_boost\"] = boost\n",
    "    # aggregate (simple average over available sections)\n",
    "    sents = [v for k,v in feats.items() if k.endswith(\"_sent\")]\n",
    "    feats[\"sent_overall\"] = float(np.mean(sents)) if sents else np.nan\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1190ff",
   "metadata": {},
   "source": [
    "### Fetch Polygon Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d45e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_daily(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    # YYYY-MM-DD → YYYY-MM-DD (inclusive)\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000, \"apiKey\": POLYGON_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    js = r.json()\n",
    "    rows = js.get(\"results\", []) or []\n",
    "    if not rows: return pd.DataFrame(columns=[\"t\",\"c\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"c\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    df = df.drop(columns=[\"t\"]).rename(columns={\"c\":\"close\"}).drop_duplicates(\"date\")\n",
    "    return df\n",
    "\n",
    "def next_trading_close(df: pd.DataFrame, target_date: datetime.date):\n",
    "    # df has 'date','close' sorted asc by date\n",
    "    # return the first bar on or after target_date\n",
    "    s = df[df[\"date\"] >= target_date]\n",
    "    return None if s.empty else float(s.iloc[0][\"close\"])\n",
    "\n",
    "def event_closes(ticker: str, filing_date: str) -> dict:\n",
    "    d0 = datetime.strptime(filing_date, \"%Y-%m-%d\").date()\n",
    "    d7 = d0 + timedelta(days=7)\n",
    "    d28 = d0 + timedelta(days=28)\n",
    "    start = (d0 - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    end   = (d28 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df.empty:\n",
    "        return {\"close_0\": np.nan, \"close_7\": np.nan, \"close_28\": np.nan}\n",
    "    c0 = next_trading_close(df, d0)\n",
    "    c7 = next_trading_close(df, d7)\n",
    "    c28= next_trading_close(df, d28)\n",
    "    return {\"close_0\": c0, \"close_7\": c7, \"close_28\": c28}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd492c",
   "metadata": {},
   "source": [
    "### Stitch it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a604ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_10q_sentiment(tickers, max_filings=6, sleep_sec=0.3):\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        print(f\"[{t}] pulling 10-Qs…\")\n",
    "        cik = get_cik(t)\n",
    "        pairs = list_10q_with_dates(cik, max_n=max_filings)\n",
    "        for p in pairs:\n",
    "            try:\n",
    "                html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  skip {p['accession']} ({e})\"); continue\n",
    "            secs = extract_sections(html)\n",
    "            feats = score_sections(secs)\n",
    "            px = event_closes(t, p[\"filing_date\"])\n",
    "            row = {\n",
    "                \"ticker\": t, \"cik\": cik, \"accession\": p[\"accession\"], \"primary\": p[\"primary\"],\n",
    "                \"filing_date\": p[\"filing_date\"],\n",
    "                **feats, **px\n",
    "            }\n",
    "            # event returns\n",
    "            c0, c7, c28 = row[\"close_0\"], row[\"close_7\"], row[\"close_28\"]\n",
    "            row[\"ret_7\"]  = (c7/c0 - 1.0)*100 if c0 and c7 else np.nan\n",
    "            row[\"ret_28\"] = (c28/c0 - 1.0)*100 if c0 and c28 else np.nan\n",
    "            rows.append(row)\n",
    "            time.sleep(sleep_sec)  # be nice to SEC\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef760efb",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d54052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LULU] pulling 10-Qs…\n",
      "[MAR] pulling 10-Qs…\n",
      "[EA] pulling 10-Qs…\n",
      "[FSLR] pulling 10-Qs…\n",
      "[MLM] pulling 10-Qs…\n",
      "[TTWO] pulling 10-Qs…\n",
      "[TDY] pulling 10-Qs…\n",
      "[ENPH] pulling 10-Qs…\n",
      "[ALB] pulling 10-Qs…\n",
      "[DAL] pulling 10-Qs…\n",
      "[CHRW] pulling 10-Qs…\n",
      "[WDC] pulling 10-Qs…\n",
      "[AAP] pulling 10-Qs…\n",
      "[CZR] pulling 10-Qs…\n",
      "[CHD] pulling 10-Qs…\n",
      "[SWKS] pulling 10-Qs…\n",
      "[COHR] pulling 10-Qs…\n",
      "[PTC] pulling 10-Qs…\n",
      "[HOLX] pulling 10-Qs…\n",
      "[MKTX] pulling 10-Qs…\n",
      "  ticker         cik           accession            primary filing_date  \\\n",
      "0   LULU  0001397187  000139718725000039  lulu-20250803.htm  2025-09-04   \n",
      "1   LULU  0001397187  000139718725000027  lulu-20250504.htm  2025-06-05   \n",
      "2   LULU  0001397187  000139718724000041  lulu-20241027.htm  2024-12-05   \n",
      "3   LULU  0001397187  000139718724000034  lulu-20240728.htm  2024-08-29   \n",
      "4   LULU  0001397187  000139718724000024  lulu-20240428.htm  2024-06-05   \n",
      "\n",
      "   MD&A_pos  MD&A_neg  MD&A_sent  MD&A_boost  RiskFactors_pos  ...  \\\n",
      "0  0.000043  0.000020   0.000023         0.0         0.050360  ...   \n",
      "1  0.000037  0.000023   0.000014         0.0         0.079346  ...   \n",
      "2  0.000066  0.000016   0.000050         0.0         0.057754  ...   \n",
      "3  0.000043  0.000020   0.000023         0.0         0.062510  ...   \n",
      "4  0.000037  0.000023   0.000014         0.0         0.078688  ...   \n",
      "\n",
      "   Results_pos  Results_neg  Results_sent  Results_boost  sent_overall  \\\n",
      "0     0.000132     0.000182     -0.000050            0.0     -0.075594   \n",
      "1     0.000147     0.000221     -0.000074            0.0     -0.043650   \n",
      "2     0.000288     0.000233      0.000054            0.0     -0.036954   \n",
      "3     0.000132     0.000182     -0.000050            0.0     -0.061416   \n",
      "4     0.000147     0.000221     -0.000074            0.0     -0.088841   \n",
      "\n",
      "   close_0  close_7  close_28      ret_7     ret_28  \n",
      "0   206.09   165.78    178.20 -19.559416 -13.532923  \n",
      "1   330.78   247.03    247.68 -25.318943 -25.122438  \n",
      "2   344.81   389.33    372.31  12.911458   7.975407  \n",
      "3   259.01   253.70    268.82  -2.050114   3.787499  \n",
      "4   308.27   309.81    300.32   0.499562  -2.578908  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Saved 10q_sentiment_event_returns.csv\n"
     ]
    }
   ],
   "source": [
    "# Large-cap list: AAPL, MSFT, AMZN, NVDA, GOOGL, META, JPM, V, JNJ, PG, XOM, UNH, PEP, KO, COST, ORCL, DIS, HD, BAC, WMT\n",
    "# Mid-cap list: LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB, DAL, CHRW, WDC, AAP, CZR, CHD, SWKS, COHR, PTC, HOLX, MKTX\n",
    "# Small-cap list: BLKB, HQY, PIPR, HAYW, NVCR, SMPL, MGPI, BE, PRCT, SKYW, AVAV, INMD, VRTS, CNXN, REZI, ASTE, MHO, CELH, ABM, PCT\n",
    "# Micro-cap list: LUNA, GCTK, VTSI, HCAT, CLXT, OPRX, FUV, BGFV, CRTX, AOUT, FCEL, HITI, AWH, WKSP, GRIN, TFFP, HZO, OPTN, TIRX, STRC\n",
    "\n",
    "s = \"LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB\"\n",
    "\n",
    "TICKERS = [x.strip() for x in s.split(\",\")]\n",
    "\n",
    "#TICKERS = [\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"META\"]\n",
    "df = backtest_10q_sentiment(TICKERS, max_filings=6)\n",
    "print(df.head())\n",
    "df.to_csv(\"10q_sentiment_event_returns.csv\", index=False)\n",
    "print(\"Saved 10q_sentiment_event_returns.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af984cb5",
   "metadata": {},
   "source": [
    "### Run Analysis on Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67fe8ba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Correlation with 7-day return (winsorized) ===\n",
      "         feature  pearson_r  p_value\n",
      "    sent_overall   0.134320 0.147021\n",
      "RiskFactors_sent   0.122572 0.186072\n",
      "       MD&A_sent   0.073888 0.438785\n",
      "    Results_sent  -0.072560 0.434894\n",
      "\n",
      "=== Correlation with 28-day return (winsorized) ===\n",
      "         feature  pearson_r  p_value\n",
      "    sent_overall   0.145217 0.116648\n",
      "RiskFactors_sent   0.141071 0.127573\n",
      "       MD&A_sent   0.041742 0.662118\n",
      "    Results_sent  -0.012746 0.891036\n",
      "\n",
      "=== Δ-sentiment correlation with 7-day return ===\n",
      "          feature  pearson_r  p_value\n",
      "    Δsent_overall   0.215532 0.033056\n",
      "ΔRiskFactors_sent   0.164965 0.104535\n",
      "       ΔMD&A_sent   0.025976 0.804789\n",
      "    ΔResults_sent   0.025515 0.803065\n",
      "\n",
      "=== Δ-sentiment correlation with 28-day return ===\n",
      "          feature  pearson_r  p_value\n",
      "    Δsent_overall   0.156478 0.123884\n",
      "ΔRiskFactors_sent   0.122662 0.228874\n",
      "    ΔResults_sent   0.055156 0.589593\n",
      "       ΔMD&A_sent  -0.028748 0.784435\n",
      "\n",
      "=== Quintile spreads (Top20% - Bottom20%) ===\n",
      "         feature  spread_7d  spread_28d  top20_avg_7d  bot20_avg_7d  top20_avg_28d  bot20_avg_28d\n",
      "    sent_overall   2.827457    4.529857      1.962742     -0.864715       5.532741       1.002883\n",
      "RiskFactors_sent   2.216249    3.924028      2.233363      0.017114       6.000972       2.076945\n",
      "       MD&A_sent  -1.180624    0.541406     -2.022499     -0.841875       2.722261       2.180855\n",
      "    Results_sent   1.676503    0.063351      0.018001     -1.658502       1.245948       1.182597\n",
      "\n",
      "--- TL;DR ---\n",
      "Top raw-sentiment signal @7d: sent_overall  r=0.134  p=0.147\n",
      "Top raw-sentiment signal @28d: sent_overall r=0.145  p=0.117\n",
      "Top Δ-sentiment signal @7d: Δsent_overall  r=0.216 p=0.0331\n",
      "Top Δ-sentiment signal @28d: Δsent_overall r=0.156 p=0.124\n",
      "Best quintile spread (28d): sent_overall  spread=4.53 pp\n"
     ]
    }
   ],
   "source": [
    "# === 10-Q Sentiment Backtest — Correlation & Legitimacy Checks ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Optional: p-values (skip if you don't have SciPy)\n",
    "try:\n",
    "    from scipy.stats import pearsonr\n",
    "    HAVE_SCIPY = True\n",
    "except Exception:\n",
    "    HAVE_SCIPY = False\n",
    "\n",
    "# 0) Load\n",
    "df = pd.read_csv(\"10q_sentiment_event_returns.csv\")\n",
    "df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "\n",
    "# 1) Basic cleaning\n",
    "# Keep rows that actually have returns\n",
    "df = df.dropna(subset=[\"ret_7\",\"ret_28\"])\n",
    "# Identify all section sentiment columns (incl. sent_overall)\n",
    "sent_cols = [c for c in df.columns if c.endswith(\"_sent\")] + ([\"sent_overall\"] if \"sent_overall\" in df.columns else [])\n",
    "sent_cols = list(dict.fromkeys(sent_cols))  # unique, keep order\n",
    "\n",
    "# Optional: winsorize extreme returns to reduce outlier influence\n",
    "def winsorize(s, p=0.01):\n",
    "    lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "    return s.clip(lo, hi)\n",
    "\n",
    "df[\"ret_7_w\"]  = winsorize(df[\"ret_7\"])\n",
    "df[\"ret_28_w\"] = winsorize(df[\"ret_28\"])\n",
    "\n",
    "# 2) Correlations (raw sentiment vs returns)\n",
    "def corr_table(y_col):\n",
    "    rows = []\n",
    "    for c in sent_cols:\n",
    "        x = df[c]\n",
    "        y = df[y_col]\n",
    "        mask = x.notna() & y.notna()\n",
    "        if mask.sum() < 8:\n",
    "            rows.append((c, np.nan, np.nan))\n",
    "            continue\n",
    "        if HAVE_SCIPY:\n",
    "            r, p = pearsonr(x[mask], y[mask])\n",
    "        else:\n",
    "            r = x[mask].corr(y[mask])\n",
    "            p = np.nan\n",
    "        rows.append((c, r, p))\n",
    "    out = pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "    return out\n",
    "\n",
    "print(\"=== Correlation with 7-day return (winsorized) ===\")\n",
    "corr7 = corr_table(\"ret_7_w\");  print(corr7.to_string(index=False))\n",
    "print(\"\\n=== Correlation with 28-day return (winsorized) ===\")\n",
    "corr28 = corr_table(\"ret_28_w\"); print(corr28.to_string(index=False))\n",
    "\n",
    "# 3) Δ-sentiment (change from prior filing for the SAME ticker)\n",
    "df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "df[\"Δsent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "for c in [c for c in sent_cols if c != \"sent_overall\"]:\n",
    "    df[f\"Δ{c}\"] = df.groupby(\"ticker\")[c].diff()\n",
    "\n",
    "delta_cols = [\"Δsent_overall\"] + [f\"Δ{c}\" for c in sent_cols if c != \"sent_overall\"]\n",
    "\n",
    "def corr_delta(y_col):\n",
    "    rows = []\n",
    "    for c in delta_cols:\n",
    "        x = df[c]\n",
    "        y = df[y_col]\n",
    "        mask = x.notna() & y.notna()\n",
    "        if mask.sum() < 8:\n",
    "            rows.append((c, np.nan, np.nan))\n",
    "            continue\n",
    "        if HAVE_SCIPY:\n",
    "            r, p = pearsonr(x[mask], y[mask])\n",
    "        else:\n",
    "            r = x[mask].corr(y[mask]); p = np.nan\n",
    "        rows.append((c, r, p))\n",
    "    return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "print(\"\\n=== Δ-sentiment correlation with 7-day return ===\")\n",
    "dc7 = corr_delta(\"ret_7_w\");  print(dc7.head(12).to_string(index=False))\n",
    "print(\"\\n=== Δ-sentiment correlation with 28-day return ===\")\n",
    "dc28 = corr_delta(\"ret_28_w\"); print(dc28.head(12).to_string(index=False))\n",
    "\n",
    "# 4) Quintile test (cross-sectional): top 20% sentiment minus bottom 20%\n",
    "def quintile_spread(col, y_col):\n",
    "    d = df[[col, y_col]].dropna().copy()\n",
    "    if len(d) < 30:\n",
    "        return np.nan, np.nan, np.nan\n",
    "    d[\"q\"] = pd.qcut(d[col], 5, labels=False)\n",
    "    top = d.loc[d[\"q\"] == 4, y_col].mean()\n",
    "    bot = d.loc[d[\"q\"] == 0, y_col].mean()\n",
    "    spread = top - bot\n",
    "    return top, bot, spread\n",
    "\n",
    "print(\"\\n=== Quintile spreads (Top20% - Bottom20%) ===\")\n",
    "rows = []\n",
    "for c in [\"sent_overall\"] + [x for x in sent_cols if x != \"sent_overall\"]:\n",
    "    t7, b7, s7 = quintile_spread(c, \"ret_7_w\")\n",
    "    t28, b28, s28 = quintile_spread(c, \"ret_28_w\")\n",
    "    rows.append([c, s7, s28, t7, b7, t28, b28])\n",
    "qt = pd.DataFrame(rows, columns=[\"feature\",\"spread_7d\",\"spread_28d\",\"top20_avg_7d\",\"bot20_avg_7d\",\"top20_avg_28d\",\"bot20_avg_28d\"])\\\n",
    "       .sort_values(\"spread_28d\", ascending=False)\n",
    "print(qt.to_string(index=False))\n",
    "\n",
    "# 5) Quick interpretation helpers\n",
    "def brief_takeaway():\n",
    "    best7 = corr7.iloc[0] if not corr7[\"pearson_r\"].isna().all() else None\n",
    "    best28 = corr28.iloc[0] if not corr28[\"pearson_r\"].isna().all() else None\n",
    "    dbest7 = dc7.iloc[0] if not dc7[\"pearson_r\"].isna().all() else None\n",
    "    dbest28 = dc28.iloc[0] if not dc28[\"pearson_r\"].isna().all() else None\n",
    "    print(\"\\n--- TL;DR ---\")\n",
    "    if best7 is not None:\n",
    "        print(f\"Top raw-sentiment signal @7d: {best7['feature']}  r={best7['pearson_r']:.3f}  p={best7['p_value']:.3g}\")\n",
    "    if best28 is not None:\n",
    "        print(f\"Top raw-sentiment signal @28d: {best28['feature']} r={best28['pearson_r']:.3f}  p={best28['p_value']:.3g}\")\n",
    "    if dbest7 is not None:\n",
    "        print(f\"Top Δ-sentiment signal @7d: {dbest7['feature']}  r={dbest7['pearson_r']:.3f} p={dbest7['p_value']:.3g}\")\n",
    "    if dbest28 is not None:\n",
    "        print(f\"Top Δ-sentiment signal @28d: {dbest28['feature']} r={dbest28['pearson_r']:.3f} p={dbest28['p_value']:.3g}\")\n",
    "    if not qt.empty:\n",
    "        print(f\"Best quintile spread (28d): {qt.iloc[0]['feature']}  spread={qt.iloc[0]['spread_28d']:.2f} pp\")\n",
    "\n",
    "brief_takeaway()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
