{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21448bdd",
   "metadata": {},
   "source": [
    "# Market Sentiment with Alpha Vantage\n",
    "\n",
    "So far in this code I've built the basic pipeline to just pull news articles from Alpha Vantage and then pass them into finBERT before putting all of that data into a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd88b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment_price_pipeline.py\n",
    "# ------------------------------------------------------------\n",
    "# 1) Fetch historical news per ticker from Alpha Vantage\n",
    "# 2) Score with FinBERT (+ optional SBERT embeddings)\n",
    "# 3) Save CSV of scored sentiment\n",
    "# 4) Fetch Polygon prices\n",
    "# 5) Join, compute correlations & basic recs\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "import os, time, json, requests, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "\n",
    "# ====== CONFIG ======\n",
    "TICKERS              = [\"AAPL\",\"MSFT\",\"NVDA\",\"TSLA\",\"AMZN\",\"GOOG\",\"META\"]  # edit as needed\n",
    "MONTHS_BACK          = 3         # how far back to fetch news\n",
    "AV_LIMIT_PER_CALL    = 100       # NEWS_SENTIMENT 'limit' (keep modest on free tier)\n",
    "SLEEP_BETWEEN_CALLS  = 12        # AV free tier ~5 req/min\n",
    "INCLUDE_EMBEDDINGS   = False     # True if you want SBERT vectors saved (big CSV)\n",
    "SENTIMENT_CARRY_DAYS = 5         # forward-fill window for sentiment\n",
    "MIN_MERGED_ROWS      = 3         # min rows to keep a ticker\n",
    "PRICE_LOOKBACK_DAYS  = 180       # how much price history to pull\n",
    "OUT_CSV              = \"news_finbert_expanded.csv\"\n",
    "\n",
    "POLY_API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "AV_API_KEY   = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "POLY_BASE    = \"https://api.polygon.io\"\n",
    "AV_BASE      = \"https://www.alphavantage.co/query\"\n",
    "FINBERT_ID   = \"yiyanghkust/finbert-tone\"\n",
    "EMBED_ID     = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "# ====== UTIL ======\n",
    "def _require_env():\n",
    "    missing = []\n",
    "    if not POLY_API_KEY: missing.append(\"POLYGON_API_KEY\")\n",
    "    if not AV_API_KEY:   missing.append(\"ALPHAVANTAGE_API_KEY\")\n",
    "    if missing:\n",
    "        raise RuntimeError(f\"Missing environment variables: {', '.join(missing)}\")\n",
    "\n",
    "def _to_datestring(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# ====== NEWS (Alpha Vantage) ======\n",
    "def fetch_news_single_ticker(ticker: str, days_back: int, limit: int = AV_LIMIT_PER_CALL) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch NEWS_SENTIMENT for a single ticker back N days (one call).\n",
    "    On free tier, use a modest 'limit' and sleep between tickers.\n",
    "    \"\"\"\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime(\"%Y%m%dT%H%M\")\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"tickers\": ticker.upper(),\n",
    "        \"time_from\": start,\n",
    "        \"sort\": \"LATEST\",\n",
    "        \"limit\": int(limit),\n",
    "        \"apikey\": AV_API_KEY,\n",
    "    }\n",
    "    r = requests.get(AV_BASE, params=params, timeout=30)\n",
    "    try:\n",
    "        data = r.json()\n",
    "    except Exception:\n",
    "        raise RuntimeError(f\"Alpha Vantage non-JSON: {r.text[:200]}\")\n",
    "    if \"Note\" in data:\n",
    "        # rate limit\n",
    "        time.sleep(15)\n",
    "        return fetch_news_single_ticker(ticker, days_back, limit)\n",
    "    if \"Information\" in data:\n",
    "        raise RuntimeError(f\"Alpha Vantage info: {data['Information']}\")\n",
    "    feed = data.get(\"feed\", [])\n",
    "    rows = []\n",
    "    for item in feed:\n",
    "        for ts in item.get(\"ticker_sentiment\", []):\n",
    "            rows.append({\n",
    "                \"dt\": pd.to_datetime(item.get(\"time_published\"), format=\"%Y%m%dT%H%M%S\", utc=True, errors=\"coerce\"),\n",
    "                \"ticker\": ts.get(\"ticker\"),\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"summary\": item.get(\"summary\") or \"\",\n",
    "                \"source\": item.get(\"source\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"av_relevance\": float(ts.get(\"relevance_score\") or 0),\n",
    "                \"av_sentiment\": float(ts.get(\"ticker_sentiment_score\") or 0),\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df[\"text\"] = (df[\"title\"].fillna(\"\").str.strip() + \". \" + df[\"summary\"].fillna(\"\").str.strip()).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].drop_duplicates(subset=[\"url\",\"ticker\"])\n",
    "    return df.sort_values(\"dt\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def fetch_news_multi(tickers: list[str], months_back: int) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    days_back = months_back * 30\n",
    "    for t in tickers:\n",
    "        try:\n",
    "            print(f\"> Fetching news for {t} (last {days_back} days)…\")\n",
    "            df = fetch_news_single_ticker(t, days_back=days_back, limit=AV_LIMIT_PER_CALL)\n",
    "            if not df.empty: frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  {t}: {e}\")\n",
    "        time.sleep(SLEEP_BETWEEN_CALLS)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "# ====== FinBERT (and optional SBERT embeddings) ======\n",
    "def load_models():\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_ID)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_ID)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    emb = None\n",
    "    if INCLUDE_EMBEDDINGS:\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        emb = SentenceTransformer(EMBED_ID)\n",
    "    return pipe, emb\n",
    "\n",
    "def finbert_and_embed(df: pd.DataFrame, pipe, emb, max_len=256, batch=32) -> pd.DataFrame:\n",
    "    if df.empty:\n",
    "        return df.assign(finbert_pos=[], finbert_neu=[], finbert_neg=[], embed=[])\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores=[]\n",
    "    for i in range(0, len(texts), batch):\n",
    "        out = pipe(texts[i:i+batch], max_length=max_len)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    if INCLUDE_EMBEDDINGS:\n",
    "        vecs = emb.encode(texts, batch_size=batch, convert_to_numpy=True, normalize_embeddings=True) if texts else np.zeros((0,384))\n",
    "        df[\"embed\"] = [v.tolist() for v in vecs]\n",
    "    else:\n",
    "        df[\"embed\"] = None\n",
    "    return df\n",
    "\n",
    "# ====== Prices (Polygon) ======\n",
    "_PRICE_CACHE: dict[tuple[str, int], pd.DataFrame] = {}\n",
    "\n",
    "def _poly_get(path: str, params: dict, retry_max: int = 3) -> dict:\n",
    "    params = {**params, \"apiKey\": POLY_API_KEY}\n",
    "    for attempt in range(1, retry_max+1):\n",
    "        r = requests.get(f\"{POLY_BASE}{path}\", params=params, timeout=30)\n",
    "        if r.status_code == 429 or 500 <= r.status_code < 600:\n",
    "            if attempt < retry_max:\n",
    "                time.sleep(2.0 * attempt)\n",
    "                continue\n",
    "            raise RuntimeError(f\"Polygon HTTP {r.status_code}: {r.text[:200]}\")\n",
    "        try:\n",
    "            j = r.json()\n",
    "        except Exception:\n",
    "            raise RuntimeError(f\"Polygon non-JSON: {r.text[:200]}\")\n",
    "        if j.get(\"status\") in {\"ERROR\",\"FAILED\"} or \"error\" in j:\n",
    "            raise RuntimeError(f\"Polygon error: {j}\")\n",
    "        return j\n",
    "    return {}\n",
    "\n",
    "def fetch_prices_polygon(ticker: str, lookback_days: int = PRICE_LOOKBACK_DAYS) -> pd.DataFrame:\n",
    "    if \":\" in ticker:\n",
    "        # skip non-equity formats like FOREX:xxx etc.\n",
    "        return pd.DataFrame()\n",
    "    cache_key = (ticker.upper(), lookback_days)\n",
    "    if cache_key in _PRICE_CACHE:\n",
    "        return _PRICE_CACHE[cache_key].copy()\n",
    "    end_dt   = datetime.now(timezone.utc)\n",
    "    start_dt = end_dt - timedelta(days=lookback_days + 7)  # weekend buffer\n",
    "    path = f\"/v2/aggs/ticker/{ticker.upper()}/range/1/day/{_to_datestring(start_dt)}/{_to_datestring(end_dt)}\"\n",
    "    j = _poly_get(path, params={\"adjusted\": \"true\", \"sort\":\"asc\", \"limit\":50000})\n",
    "    res = j.get(\"results\", [])\n",
    "    if not res:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(res)\n",
    "    df[\"ts\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True)\n",
    "    df = df.rename(columns={\"c\":\"close\"})[[\"ts\",\"close\"]]\n",
    "    df[\"close\"] = pd.to_numeric(df[\"close\"], errors=\"coerce\")\n",
    "    df = df.dropna().sort_values(\"ts\").reset_index(drop=True)\n",
    "    df[\"return\"] = df[\"close\"].pct_change()\n",
    "    df[\"date\"] = df[\"ts\"].dt.date\n",
    "    df = df.set_index(\"date\")[[\"close\",\"return\"]]\n",
    "    cutoff = (datetime.now(timezone.utc) - timedelta(days=lookback_days)).date()\n",
    "    df = df.loc[df.index >= cutoff]\n",
    "    _PRICE_CACHE[cache_key] = df\n",
    "    time.sleep(0.25)  # polite\n",
    "    return df\n",
    "\n",
    "# ====== Correlation & Recs ======\n",
    "def correlate_and_recommend(df_sent: pd.DataFrame, tickers: list[str]) -> pd.DataFrame:\n",
    "    results = []\n",
    "    # prep per-ticker news\n",
    "    df_sent[\"date\"] = df_sent[\"dt\"].dt.date\n",
    "    df_sent[\"net_sentiment\"] = df_sent[\"finbert_pos\"] - df_sent[\"finbert_neg\"]\n",
    "\n",
    "    for ticker in tickers:\n",
    "        # Get prices\n",
    "        prices = fetch_prices_polygon(ticker, lookback_days=PRICE_LOOKBACK_DAYS)\n",
    "        if prices.empty:\n",
    "            print(f\"⚠️  {ticker}: no prices from Polygon in window.\")\n",
    "            continue\n",
    "\n",
    "        # News for ticker\n",
    "        dnews = df_sent[df_sent[\"ticker\"].str.upper()==ticker].groupby(\"date\", as_index=True)[[\"net_sentiment\"]].mean()\n",
    "        if dnews.empty:\n",
    "            print(f\"ℹ️  {ticker}: no sentiment rows.\")\n",
    "            continue\n",
    "\n",
    "        # Clip prices at first news date (avoid long pre-news NaNs)\n",
    "        first_news_date = dnews.index.min()\n",
    "        p = prices.loc[prices.index >= first_news_date].copy()\n",
    "        if p.empty:\n",
    "            print(f\"ℹ️  {ticker}: no prices on/after first news date.\")\n",
    "            continue\n",
    "\n",
    "        dfm = p.join(dnews, how=\"left\").sort_index()\n",
    "        dfm[\"net_sentiment\"] = dfm[\"net_sentiment\"].ffill(limit=SENTIMENT_CARRY_DAYS)\n",
    "        dfm = dfm.dropna(subset=[\"return\",\"net_sentiment\"])\n",
    "\n",
    "        if len(dfm) < MIN_MERGED_ROWS:\n",
    "            print(f\"ℹ️  {ticker}: Not enough merged rows ({len(dfm)}).\")\n",
    "            continue\n",
    "\n",
    "        corr = dfm[\"return\"].corr(dfm[\"net_sentiment\"])\n",
    "        avg_sent = (dfm[\"net_sentiment\"].iloc[-5:].mean()\n",
    "                    if len(dfm) >= 5 else dfm[\"net_sentiment\"].mean())\n",
    "        last_ret = (dfm[\"return\"].iloc[-5:].mean()\n",
    "                    if len(dfm) >= 5 else dfm[\"return\"].mean())\n",
    "\n",
    "        results.append({\n",
    "            \"ticker\": ticker,\n",
    "            \"corr\": corr,\n",
    "            \"avg_sentiment\": avg_sent,\n",
    "            \"recent_return\": last_ret,\n",
    "            \"n_rows\": len(dfm),\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def print_recommendations(df_corr: pd.DataFrame):\n",
    "    print(\"\\n=== Stock Recommendations ===\")\n",
    "    if df_corr.empty:\n",
    "        print(\"No recommendations (no valid correlation results).\")\n",
    "        return\n",
    "    for _, row in df_corr.sort_values(\"corr\", ascending=False).iterrows():\n",
    "        t, c, s = row[\"ticker\"], row[\"corr\"], row[\"avg_sentiment\"]\n",
    "        if c > 0.2 and s > 0:\n",
    "            print(f\"✅ {t}: Positive correlation & sentiment → Potential Buy\")\n",
    "        elif c < -0.2 and s < 0:\n",
    "            print(f\"⚠️ {t}: Negative correlation & sentiment → Caution\")\n",
    "        else:\n",
    "            print(f\"➖ {t}: Neutral sentiment or weak correlation\")\n",
    "\n",
    "# ====== MAIN RUN ======\n",
    "def main():\n",
    "    _require_env()\n",
    "\n",
    "    # 1) Fetch news historically (per ticker) and score\n",
    "    print(f\"Fetching news for {len(TICKERS)} tickers over ~{MONTHS_BACK} months…\")\n",
    "    df_news = fetch_news_multi(TICKERS, months_back=MONTHS_BACK)\n",
    "    print(f\"News rows: {len(df_news)}\")\n",
    "\n",
    "    if df_news.empty:\n",
    "        print(\"No news pulled. Exiting early.\")\n",
    "        return\n",
    "\n",
    "    print(\"Scoring with FinBERT…\")\n",
    "    pipe, emb = load_models()\n",
    "    df_scored = finbert_and_embed(df_news, pipe, emb)\n",
    "    # persist\n",
    "    cols = [\"dt\",\"ticker\",\"source\",\"url\",\"av_relevance\",\"av_sentiment\",\n",
    "            \"finbert_pos\",\"finbert_neu\",\"finbert_neg\",\"title\",\"summary\"]\n",
    "    if INCLUDE_EMBEDDINGS:\n",
    "        cols += [\"embed\"]\n",
    "    df_scored[cols].to_csv(OUT_CSV, index=False)\n",
    "    print(f\"Saved {OUT_CSV}\")\n",
    "\n",
    "    # 2) Correlate with prices from Polygon\n",
    "    print(\"Fetching prices and computing correlations…\")\n",
    "    df_corr = correlate_and_recommend(df_scored, TICKERS)\n",
    "    if df_corr.empty:\n",
    "        print(\"\\n(No correlation results — check diagnostics above.)\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Sentiment vs Return Correlations (Polygon) ===\")\n",
    "    print(df_corr[[\"ticker\",\"corr\",\"avg_sentiment\",\"recent_return\",\"n_rows\"]]\n",
    "          .round(3).sort_values(\"corr\", ascending=False).to_string(index=False))\n",
    "\n",
    "    # 3) Simple recs\n",
    "    print_recommendations(df_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f35e7b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 7 tickers over ~3 months…\n",
      "> Fetching news for AAPL (last 90 days)…\n",
      "> Fetching news for MSFT (last 90 days)…\n",
      "> Fetching news for NVDA (last 90 days)…\n",
      "> Fetching news for TSLA (last 90 days)…\n",
      "> Fetching news for AMZN (last 90 days)…\n",
      "> Fetching news for GOOG (last 90 days)…\n",
      "> Fetching news for META (last 90 days)…\n",
      "News rows: 1458\n",
      "Scoring with FinBERT…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps:0\n",
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved news_finbert_expanded.csv\n",
      "Fetching prices and computing correlations…\n",
      "\n",
      "=== Sentiment vs Return Correlations (Polygon) ===\n",
      "ticker   corr  avg_sentiment  recent_return  n_rows\n",
      "  MSFT  0.998          0.352         -0.004       3\n",
      "  AAPL  0.564          0.371          0.003       3\n",
      "  GOOG  0.553          0.452          0.017       3\n",
      "  TSLA -0.122          0.269         -0.007       3\n",
      "  META -0.127          0.248         -0.036       3\n",
      "  AMZN -0.499          0.373         -0.004       3\n",
      "  NVDA -0.522          0.371          0.019       3\n",
      "\n",
      "=== Stock Recommendations ===\n",
      "✅ MSFT: Positive correlation & sentiment → Potential Buy\n",
      "✅ AAPL: Positive correlation & sentiment → Potential Buy\n",
      "✅ GOOG: Positive correlation & sentiment → Potential Buy\n",
      "➖ TSLA: Neutral sentiment or weak correlation\n",
      "➖ META: Neutral sentiment or weak correlation\n",
      "➖ AMZN: Neutral sentiment or weak correlation\n",
      "➖ NVDA: Neutral sentiment or weak correlation\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
