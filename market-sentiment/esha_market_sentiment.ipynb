{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21448bdd",
   "metadata": {},
   "source": [
    "# Market Sentiment with Alpha Vantage\n",
    "\n",
    "So far in this code I've built the basic pipeline to just pull news articles from Alpha Vantage and then pass them into finBERT before putting all of that data into a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f5cd",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Set up all the environment variables and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf696f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Imports & constants (upgrade) ---\n",
    "import os, sys, time, math, json, re, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Providers\n",
    "AV_BASE  = \"https://www.alphavantage.co/query\"\n",
    "FINBERT  = \"yiyanghkust/finbert-tone\"\n",
    "EMBED    = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def SEC_HEADERS(host: str | None = None):\n",
    "    h = {\n",
    "        \"User-Agent\": f\"severin (contact: {os.getenv('SEC_EMAIL','email@example.com')})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    if host:\n",
    "        h[\"Host\"] = host\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251adc1",
   "metadata": {},
   "source": [
    "### News Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45a048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alpha Vantage news (unchanged behavior, small cleanups) ---\n",
    "def fetch_news(tickers, days_back=7, limit=50):\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime(\"%Y%m%dT%H%M\")\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"tickers\": \",\".join([t.upper() for t in tickers[:20]]),\n",
    "        \"time_from\": start,\n",
    "        \"sort\": \"LATEST\",\n",
    "        \"limit\": int(limit),\n",
    "        \"apikey\": os.getenv(\"ALPHAVANTAGE_API_KEY\"),\n",
    "    }\n",
    "    params = {k: v for k, v in params.items() if v not in (None, \"\")}\n",
    "\n",
    "    r = requests.get(AV_BASE, params=params, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code}: {r.text}\")\n",
    "    data = r.json()\n",
    "\n",
    "    if \"Information\" in data:\n",
    "        raise RuntimeError(f\"Alpha Vantage: {data['Information']}\")\n",
    "    if \"Note\" in data:\n",
    "        print(\"Rate limit hit; sleeping 15s…\")\n",
    "        time.sleep(15)\n",
    "        return fetch_news(tickers, days_back, limit)\n",
    "    if \"feed\" not in data:\n",
    "        raise RuntimeError(f\"Unexpected response: {data}\")\n",
    "\n",
    "    rows = []\n",
    "    for item in data[\"feed\"]:\n",
    "        for ts in item.get(\"ticker_sentiment\", []):\n",
    "            rows.append({\n",
    "                \"dt\": pd.to_datetime(item.get(\"time_published\"), format=\"%Y%m%dT%H%M%S\", utc=True, errors=\"coerce\"),\n",
    "                \"ticker\": ts.get(\"ticker\"),\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"summary\": item.get(\"summary\") or \"\",\n",
    "                \"source\": item.get(\"source\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"av_relevance\": float(ts.get(\"relevance_score\") or 0),\n",
    "                \"av_sentiment\": float(ts.get(\"ticker_sentiment_score\") or 0),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No articles found.\")\n",
    "        return df\n",
    "\n",
    "    df[\"text\"] = (df[\"title\"].fillna(\"\").str.strip() + \". \" + df[\"summary\"].fillna(\"\").str.strip()).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].drop_duplicates(subset=[\"url\",\"ticker\"])\n",
    "    return df.sort_values(\"dt\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def fetch_multi_tickers(ticker_list, days_back=7, limit=50, sleep_s=12):\n",
    "    \"\"\"Free tier: call one ticker at a time; respect ~5 req/min.\"\"\"\n",
    "    frames = []\n",
    "    for t in ticker_list:\n",
    "        try:\n",
    "            print(f\"> Fetching {t} …\")\n",
    "            df = fetch_news([t], days_back=days_back, limit=limit)\n",
    "            if not df.empty: frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {t}: {e}\")\n",
    "        time.sleep(sleep_s)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88d71",
   "metadata": {},
   "source": [
    "### Fundamentals + Discounted Cash Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5adf6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SEC fundamentals & valuation (NEW) ---\n",
    "\n",
    "# Ticker -> CIK\n",
    "def get_cik(ticker: str) -> str:\n",
    "    t = ticker.upper().strip()\n",
    "    map_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = requests.get(map_url, headers=SEC_HEADERS(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    for _, rec in data.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "# Company facts (US-GAAP)\n",
    "def get_company_facts(cik: str) -> dict:\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _get_units(facts: dict, key: str):\n",
    "    return facts.get(\"facts\",{}).get(\"us-gaap\",{}).get(key,{}).get(\"units\",{})\n",
    "\n",
    "def _series(values):\n",
    "    rows=[]\n",
    "    for v in values:\n",
    "        end=v.get(\"end\"); val=v.get(\"val\")\n",
    "        if end is None or val is None: continue\n",
    "        try: rows.append((datetime.fromisoformat(end), float(val)))\n",
    "        except Exception: continue\n",
    "    rows.sort(key=lambda x: x[0], reverse=True)\n",
    "    return rows\n",
    "\n",
    "def _ttm_sum(values, n=4):\n",
    "    rows=_series(values)\n",
    "    return float(np.nansum([x[1] for x in rows[:n]])) if rows else None\n",
    "\n",
    "def _latest(values):\n",
    "    rows=_series(values)\n",
    "    return rows[0][1] if rows else None\n",
    "\n",
    "EXTRA_KEYS = {\n",
    "    \"revenue\": [\"Revenues\",\"RevenueFromContractWithCustomerExcludingAssessedTax\",\"SalesRevenueNet\"],\n",
    "    \"net_income\": [\"NetIncomeLoss\"],\n",
    "    \"eps_diluted\": [\"EarningsPerShareDiluted\"],\n",
    "    \"shares_out\": [\"CommonStockSharesOutstanding\",\"WeightedAverageNumberOfDilutedSharesOutstanding\"],\n",
    "    \"cfo\": [\"NetCashProvidedByUsedInOperatingActivities\"],\n",
    "    \"capex\": [\"PaymentsToAcquirePropertyPlantAndEquipment\",\"PaymentsToAcquireProductiveAssets\"],\n",
    "    \"cash\": [\"CashAndCashEquivalentsAtCarryingValue\"],\n",
    "    \"liabilities\": [\"Liabilities\"],\n",
    "}\n",
    "\n",
    "def extract_gaap_ttm_extended(facts_json: dict) -> dict:\n",
    "    fx = facts_json\n",
    "    out = {}\n",
    "    # helper to collect by key candidates across units\n",
    "    def pick(keys, prefer=(\"USD\",\"shares\",\"USD/shares\")):\n",
    "        for k in keys:\n",
    "            units = _get_units(fx, k)\n",
    "            for u in prefer:\n",
    "                if u in units:\n",
    "                    return units[u]\n",
    "        return []\n",
    "\n",
    "    rev   = pick(EXTRA_KEYS[\"revenue\"])\n",
    "    ni    = pick(EXTRA_KEYS[\"net_income\"])\n",
    "    eps   = pick(EXTRA_KEYS[\"eps_diluted\"])\n",
    "    sh    = pick(EXTRA_KEYS[\"shares_out\"])\n",
    "    cfo   = pick(EXTRA_KEYS[\"cfo\"])\n",
    "    capex = pick(EXTRA_KEYS[\"capex\"])\n",
    "    cash  = pick(EXTRA_KEYS[\"cash\"])\n",
    "    liab  = pick(EXTRA_KEYS[\"liabilities\"])\n",
    "\n",
    "    out[\"revenue_ttm\"]       = _ttm_sum(rev)\n",
    "    out[\"net_income_ttm\"]    = _ttm_sum(ni)\n",
    "    out[\"eps_diluted_ttm\"]   = _latest(eps)\n",
    "    out[\"shares_out_latest\"] = _latest(sh)\n",
    "    out[\"cfo_ttm\"]           = _ttm_sum(cfo)\n",
    "    out[\"capex_ttm\"]         = _ttm_sum(capex)  # usually negative in GAAP files\n",
    "    # Free Cash Flow ~ CFO - CapEx (capex sign may be negative; use abs)\n",
    "    if out[\"cfo_ttm\"] is not None and out[\"capex_ttm\"] is not None:\n",
    "        out[\"fcf_ttm\"] = out[\"cfo_ttm\"] - abs(out[\"capex_ttm\"])\n",
    "    else:\n",
    "        out[\"fcf_ttm\"] = None\n",
    "    out[\"cash_latest\"]       = _latest(cash)\n",
    "    out[\"liabilities_latest\"]= _latest(liab)\n",
    "    return out\n",
    "\n",
    "# Latest price (AV)\n",
    "def latest_price(ticker: str):\n",
    "    key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "    if not key: return None\n",
    "    r = requests.get(AV_BASE, params={\n",
    "        \"function\":\"TIME_SERIES_DAILY_ADJUSTED\",\"symbol\":ticker.upper(),\n",
    "        \"outputsize\":\"compact\",\"apikey\":key\n",
    "    }, timeout=30)\n",
    "    try:\n",
    "        js=r.json(); ts=js[\"Time Series (Daily)\"]\n",
    "        last_key=sorted(ts.keys(), reverse=True)[0]\n",
    "        return float(ts[last_key][\"5. adjusted close\"])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Multiples anchor with more realistic tech defaults (tuneable)\n",
    "def fair_value_multiples(gaap: dict, pe=25.0, ps=6.0):\n",
    "    res={}\n",
    "    eps=gaap.get(\"eps_diluted_ttm\"); rev=gaap.get(\"revenue_ttm\"); sh=gaap.get(\"shares_out_latest\")\n",
    "    if eps and eps>0: res[\"pe_anchor\"]=eps*pe\n",
    "    if rev and sh and sh>0: res[\"ps_anchor\"]=(rev/sh)*ps\n",
    "    if res:\n",
    "        anchors=[v for v in (res.get(\"pe_anchor\"), res.get(\"ps_anchor\")) if v is not None]\n",
    "        res[\"fair_value_mid\"]=float(np.mean(anchors))\n",
    "        res[\"fair_value_low\"]=float(np.min(anchors))\n",
    "        res[\"fair_value_high\"]=float(np.max(anchors))\n",
    "    return res\n",
    "\n",
    "# Simple DCF on FCF\n",
    "def fair_value_dcf(gaap: dict, growth=0.07, discount=0.09, terminal=0.025, years=5):\n",
    "    fcf=gaap.get(\"fcf_ttm\"); sh=gaap.get(\"shares_out_latest\")\n",
    "    if not fcf or not sh: return None\n",
    "    pv=0.0; f=fcf\n",
    "    for yr in range(1, years+1):\n",
    "        f *= (1+growth)\n",
    "        pv += f / ((1+discount)**yr)\n",
    "    tv = (f*(1+terminal))/(discount-terminal)\n",
    "    pv += tv/((1+discount)**years)\n",
    "    return pv / sh\n",
    "\n",
    "def intrinsic_valuation(ticker: str):\n",
    "    cik  = get_cik(ticker)\n",
    "    facts= get_company_facts(cik)\n",
    "    gaap = extract_gaap_ttm_extended(facts)\n",
    "    price= latest_price(ticker)\n",
    "    mult = fair_value_multiples(gaap, pe=25.0, ps=6.0)\n",
    "    dcf  = fair_value_dcf(gaap, growth=0.07, discount=0.09, terminal=0.025, years=5)\n",
    "    out = {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"cik\": cik,\n",
    "        \"gaap_ttm\": gaap,\n",
    "        \"price\": price,\n",
    "        \"multiples\": mult,\n",
    "        \"dcf_anchor\": dcf,\n",
    "    }\n",
    "    # combine if both present\n",
    "    anchors=[]\n",
    "    if mult.get(\"fair_value_mid\"): anchors.append(mult[\"fair_value_mid\"])\n",
    "    if dcf: anchors.append(dcf)\n",
    "    if anchors:\n",
    "        out[\"fair_value_blend\"]=float(np.mean(anchors))\n",
    "        if price: out[\"upside_pct\"]=100.0*(out[\"fair_value_blend\"]/price-1.0)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3defa",
   "metadata": {},
   "source": [
    "### Finbert Loading and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e92ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FinBERT sentiment + embeddings (same as you had) ---\n",
    "def load_models():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    emb  = SentenceTransformer(EMBED)\n",
    "    return pipe, emb\n",
    "\n",
    "def finbert_and_embed(df, pipe, emb, max_len=256, batch=32):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores=[]\n",
    "    for i in range(0, len(texts), batch):\n",
    "        out = pipe(texts[i:i+batch], max_length=max_len)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    vecs = emb.encode(texts, batch_size=batch, convert_to_numpy=True, normalize_embeddings=True) if texts else np.zeros((0,384))\n",
    "    df[\"embed\"] = [v.tolist() for v in vecs]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd492c",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a604ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FinBERT sentiment + embeddings (same as you had) ---\n",
    "def load_models():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    emb  = SentenceTransformer(EMBED)\n",
    "    return pipe, emb\n",
    "\n",
    "def finbert_and_embed(df, pipe, emb, max_len=256, batch=32):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores=[]\n",
    "    for i in range(0, len(texts), batch):\n",
    "        out = pipe(texts[i:i+batch], max_length=max_len)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    vecs = emb.encode(texts, batch_size=batch, convert_to_numpy=True, normalize_embeddings=True) if texts else np.zeros((0,384))\n",
    "    df[\"embed\"] = [v.tolist() for v in vecs]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef760efb",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a3d54052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news…\n",
      "Got 260 rows\n",
      "Scoring with FinBERT + embeddings…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved news_finbert_sample.csv — ready to join with prices for labels.\n",
      "\n",
      "Intrinsic valuation snapshots:\n",
      "{\n",
      "  \"ticker\": \"AAPL\",\n",
      "  \"cik\": \"0000320193\",\n",
      "  \"gaap_ttm\": {\n",
      "    \"revenue_ttm\": 442897000000.0,\n",
      "    \"net_income_ttm\": 193868000000.0,\n",
      "    \"eps_diluted_ttm\": 5.62,\n",
      "    \"shares_out_latest\": 14856722000.0,\n",
      "    \"cfo_ttm\": 283830000000.0,\n",
      "    \"capex_ttm\": 27871000000.0,\n",
      "    \"fcf_ttm\": 255959000000.0,\n",
      "    \"cash_latest\": 36269000000.0,\n",
      "    \"liabilities_latest\": 265665000000.0\n",
      "  },\n",
      "  \"price\": null,\n",
      "  \"multiples\": {\n",
      "    \"pe_anchor\": 140.5,\n",
      "    \"ps_anchor\": 178.86731676072287,\n",
      "    \"fair_value_mid\": 159.68365838036144,\n",
      "    \"fair_value_low\": 140.5,\n",
      "    \"fair_value_high\": 178.86731676072287\n",
      "  },\n",
      "  \"dcf_anchor\": 329.168533954489,\n",
      "  \"fair_value_blend\": 244.42609616742521\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Runner: news sentiment + intrinsic value ---\n",
    "assert os.getenv(\"ALPHAVANTAGE_API_KEY\"), \"Set ALPHAVANTAGE_API_KEY in your .env\"\n",
    "if not os.getenv(\"SEC_EMAIL\"):\n",
    "    print(\"Tip: add SEC_EMAIL=you@example.com to .env for SEC API etiquette.\")\n",
    "\n",
    "tickers = sys.argv[1:] or [\"AAPL\",\"MSFT\",\"NVDA\"]\n",
    "\n",
    "# 1) News → FinBERT\n",
    "print(\"Fetching news…\")\n",
    "df_news = fetch_news([\"AAPL\"], days_back=7, limit=50)  # or fetch_multi_tickers(tickers)\n",
    "print(f\"Got {len(df_news)} rows\")\n",
    "if not df_news.empty:\n",
    "    print(\"Scoring with FinBERT + embeddings…\")\n",
    "    pipe, emb = load_models()\n",
    "    df_scored = finbert_and_embed(df_news, pipe, emb)\n",
    "    out_csv = \"news_finbert_sample.csv\"\n",
    "    df_scored[[\"dt\",\"ticker\",\"source\",\"url\",\"av_relevance\",\"av_sentiment\",\n",
    "               \"finbert_pos\",\"finbert_neu\",\"finbert_neg\",\"title\",\"summary\",\"embed\"]].to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {out_csv} — ready to join with prices for labels.\")\n",
    "\n",
    "# 2) Fundamentals → DCF & multiples\n",
    "print(\"\\nIntrinsic valuation snapshots:\")\n",
    "for t in [\"AAPL\"]:  # change to tickers\n",
    "    try:\n",
    "        val = intrinsic_valuation(t)\n",
    "        print(json.dumps(val, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"{t}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80b20eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ No price data for NVDA\n",
      "⚠️ No price data for AAPL\n",
      "⚠️ No price data for TSLA\n",
      "⚠️ No price data for LLY\n",
      "⚠️ No price data for MA\n",
      "⚠️ No price data for AMZN\n",
      "⚠️ No price data for CROX\n",
      "⚠️ No price data for FOREX:AMD\n",
      "⚠️ No price data for GOOG\n",
      "⚠️ No price data for META\n",
      "⚠️ No price data for SSNLF\n",
      "⚠️ No price data for MS\n",
      "⚠️ No price data for SMWB\n",
      "⚠️ No price data for MSFT\n",
      "⚠️ No price data for CRYPTO:SOL\n",
      "⚠️ No price data for CRYPTO:ETH\n",
      "⚠️ No price data for CRYPTO:BTC\n",
      "⚠️ No price data for IVZ\n",
      "⚠️ No price data for LRCX\n",
      "⚠️ No price data for MORN\n",
      "⚠️ No price data for AVGO\n",
      "⚠️ No price data for TSM\n",
      "⚠️ No price data for FOREX:USD\n",
      "⚠️ No price data for DKNG\n",
      "⚠️ No price data for ICE\n",
      "⚠️ No price data for COIN\n",
      "⚠️ No price data for ORCL\n",
      "⚠️ No price data for INTC\n",
      "⚠️ No price data for NOK\n",
      "⚠️ No price data for WBD\n",
      "⚠️ No price data for VZ\n",
      "⚠️ No price data for BA\n",
      "⚠️ No price data for CAT\n",
      "⚠️ No price data for AMD\n",
      "⚠️ No price data for AVY\n",
      "⚠️ No price data for BABA\n",
      "⚠️ No price data for FOREX:BGN\n",
      "⚠️ No price data for BAC\n",
      "⚠️ No price data for KO\n",
      "⚠️ No price data for AXP\n",
      "⚠️ No price data for KHC\n",
      "⚠️ No price data for BRK-A\n",
      "⚠️ No price data for CVX\n",
      "⚠️ No price data for OXY\n",
      "⚠️ No price data for SNPS\n",
      "⚠️ No price data for ANSS\n",
      "⚠️ No price data for ON\n",
      "⚠️ No price data for GS\n",
      "⚠️ No price data for USEG\n",
      "⚠️ No price data for WMT\n",
      "⚠️ No price data for GE\n",
      "⚠️ No price data for NET\n",
      "⚠️ No price data for BOC\n",
      "⚠️ No price data for SBUX\n",
      "⚠️ No price data for TBBB\n",
      "⚠️ No price data for SPOT\n",
      "⚠️ No price data for CRWD\n",
      "⚠️ No price data for ASAN\n",
      "⚠️ No price data for NFLX\n",
      "⚠️ No price data for STGW\n",
      "⚠️ No price data for LRN\n",
      "⚠️ No price data for KVUE\n",
      "⚠️ No price data for W\n",
      "⚠️ No price data for CCORF\n",
      "⚠️ No price data for CAKE\n",
      "⚠️ No price data for TZOO\n",
      "⚠️ No price data for EW\n",
      "⚠️ No price data for DB\n",
      "⚠️ No price data for MRVL\n",
      "⚠️ No price data for QRVO\n",
      "⚠️ No price data for ARM\n",
      "⚠️ No price data for SWKS\n",
      "⚠️ No price data for QCOM\n",
      "⚠️ No price data for CRWV\n",
      "⚠️ No price data for ABBV\n",
      "⚠️ No price data for UBER\n",
      "⚠️ No price data for CSCO\n",
      "⚠️ No price data for XOM\n",
      "⚠️ No price data for CMG\n",
      "⚠️ No price data for V\n",
      "⚠️ No price data for MDLZ\n",
      "⚠️ No price data for XIACY\n",
      "⚠️ No price data for EVR\n",
      "⚠️ No price data for FSLR\n",
      "⚠️ No price data for C\n",
      "⚠️ No price data for FOREX:JPY\n",
      "⚠️ No price data for STT\n",
      "⚠️ No price data for LNVGF\n",
      "⚠️ No price data for DELL\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'corr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[32m/var/folders/8r/txvrry2d4g18dhf7jkg1hhzm0000gn/T/ipykernel_26243/378623056.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     66\u001b[39m         \u001b[33m\"avg_sentiment\"\u001b[39m: avg_sent,\n\u001b[32m     67\u001b[39m         \u001b[33m\"recent_return\"\u001b[39m: last_ret,\n\u001b[32m     68\u001b[39m     })\n\u001b[32m     69\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m df_corr = pd.DataFrame(results).sort_values(\u001b[33m\"corr\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     71\u001b[39m print(\u001b[33m\"\\n=== Sentiment vs Return Correlations ===\"\u001b[39m)\n\u001b[32m     72\u001b[39m print(df_corr.round(\u001b[32m3\u001b[39m))\n\u001b[32m     73\u001b[39m \n",
      "\u001b[32m~/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/pandas/core/frame.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[39m\n\u001b[32m   7207\u001b[39m             )\n\u001b[32m   7208\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m len(by):\n\u001b[32m   7209\u001b[39m             \u001b[38;5;66;03m# len(by) == 1\u001b[39;00m\n\u001b[32m   7210\u001b[39m \n\u001b[32m-> \u001b[39m\u001b[32m7211\u001b[39m             k = self._get_label_or_level_values(by[\u001b[32m0\u001b[39m], axis=axis)\n\u001b[32m   7212\u001b[39m \n\u001b[32m   7213\u001b[39m             \u001b[38;5;66;03m# need to rewrap column in Series to apply key function\u001b[39;00m\n\u001b[32m   7214\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;28;01mis\u001b[39;00m \u001b[38;5;28;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[32m~/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/pandas/core/generic.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, key, axis)\u001b[39m\n\u001b[32m   1910\u001b[39m             values = self.xs(key, axis=other_axes[\u001b[32m0\u001b[39m])._values\n\u001b[32m   1911\u001b[39m         \u001b[38;5;28;01melif\u001b[39;00m self._is_level_reference(key, axis=axis):\n\u001b[32m   1912\u001b[39m             values = self.axes[axis].get_level_values(key)._values\n\u001b[32m   1913\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m KeyError(key)\n\u001b[32m   1915\u001b[39m \n\u001b[32m   1916\u001b[39m         \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[32m   1917\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m values.ndim > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mKeyError\u001b[39m: 'corr'"
     ]
    }
   ],
   "source": [
    "#Match sentiment to stock price data and make recommendations ---\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedeltas\n",
    "import os\n",
    "\n",
    "AV_API_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "AV_BASE = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "def fetch_prices(ticker, days_back=30):\n",
    "    \"\"\"Fetch daily adjusted prices from Alpha Vantage\"\"\"\n",
    "    params = {\n",
    "        \"function\": \"TIME_SERIES_DAILY_ADJUSTED\",\n",
    "        \"symbol\": ticker,\n",
    "        \"outputsize\": \"compact\",\n",
    "        \"apikey\": AV_API_KEY,\n",
    "    }\n",
    "    r = requests.get(AV_BASE, params=params, timeout=30)\n",
    "    data = r.json().get(\"Time Series (Daily)\", {})\n",
    "    if not data:\n",
    "        print(f\"⚠️ No price data for {ticker}\")\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame.from_dict(data, orient=\"index\").astype(float)\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df = df.rename(columns={\"4. close\": \"close\"})\n",
    "    df = df[[\"close\"]].sort_index()\n",
    "    df[\"return\"] = df[\"close\"].pct_change()\n",
    "    df = df.loc[df.index >= (datetime.utcnow() - timedelta(days=days_back))]\n",
    "    return df\n",
    "\n",
    "df_sent = pd.read_csv(\"news_finbert_sample.csv\", parse_dates=[\"dt\"])\n",
    "df_sent[\"date\"] = df_sent[\"dt\"].dt.date\n",
    "df_sent[\"net_sentiment\"] = df_sent[\"finbert_pos\"] - df_sent[\"finbert_neg\"]\n",
    "results = []\n",
    "\n",
    "for ticker in df_sent[\"ticker\"].unique():\n",
    "    prices = fetch_prices(ticker)\n",
    "    if prices.empty:\n",
    "        continue\n",
    "\n",
    "    #Aggregate sentiment by day ----\n",
    "    daily_sent = (\n",
    "        df_sent[df_sent[\"ticker\"] == ticker]\n",
    "        .groupby(\"date\")[[\"net_sentiment\"]]\n",
    "        .mean()\n",
    "        .rename_axis(\"date\")\n",
    "    )\n",
    "\n",
    "    dfm = (\n",
    "        prices.join(daily_sent, how=\"left\")\n",
    "        .fillna(method=\"ffill\")\n",
    "        .dropna(subset=[\"return\", \"net_sentiment\"])\n",
    "    )\n",
    "\n",
    "    if len(dfm) < 5:\n",
    "        continue\n",
    "\n",
    "    corr = dfm[\"return\"].corr(dfm[\"net_sentiment\"])\n",
    "    avg_sent = dfm[\"net_sentiment\"].iloc[-5:].mean()\n",
    "    last_ret = dfm[\"return\"].iloc[-5:].mean()\n",
    "\n",
    "    results.append({\n",
    "        \"ticker\": ticker,\n",
    "        \"corr\": corr,\n",
    "        \"avg_sentiment\": avg_sent,\n",
    "        \"recent_return\": last_ret,\n",
    "    })\n",
    "\n",
    "df_corr = pd.DataFrame(results).sort_values(\"corr\", ascending=False)\n",
    "print(\"\\n=== Sentiment vs Return Correlations ===\")\n",
    "print(df_corr.round(3))\n",
    "\n",
    "#Identify sources that correlate best with price movement ----\n",
    "src_corrs = []\n",
    "for src, g in df_sent.groupby(\"source\"):\n",
    "    if len(g) < 5:\n",
    "        continue\n",
    "    mean_sent = g[\"net_sentiment\"].mean()\n",
    "    src_corrs.append({\"source\": src, \"mean_sentiment\": mean_sent, \"count\": len(g)})\n",
    "\n",
    "df_src = pd.DataFrame(src_corrs).sort_values(\"mean_sentiment\", ascending=False)\n",
    "# print(\"\\n=== Top News Sources by Average Sentiment ===\")\n",
    "# print(dfsrc.head(10).round(3))\n",
    "\n",
    "#Make basic recommendations ----\n",
    "print(\"\\n=== Stock Recommendations ===\")\n",
    "for x, row in df_corr.iterrows():\n",
    "    if row[\"corr\"] > 0.2 and row[\"avg_sentiment\"] > 0:\n",
    "        print(f\"✅ {row['ticker']}: Positive correlation & sentiment → Potential Buy\")\n",
    "    elif row[\"corr\"] < -0.2 and row[\"avg_sentiment\"] < 0:\n",
    "        print(f\"⚠️ {row['ticker']}: Negative correlation & sentiment → Caution\")\n",
    "    else:\n",
    "        print(f\"➖ {row['ticker']}: Neutral sentiment or weak correlation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
