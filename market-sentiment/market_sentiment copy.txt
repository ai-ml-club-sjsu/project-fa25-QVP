{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21448bdd",
   "metadata": {},
   "source": [
    "# Market Sentiment with Alpha Vantage\n",
    "\n",
    "So far in this code I've built the basic pipeline to just pull news articles from Alpha Vantage and then pass them into finBERT before putting all of that data into a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f5cd",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Set up all the environment variables and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adf696f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# --- Imports & constants (upgrade) ---\n",
    "import os, sys, time, math, json, re, requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Providers\n",
    "AV_BASE  = \"https://www.alphavantage.co/query\"\n",
    "FINBERT  = \"yiyanghkust/finbert-tone\"\n",
    "EMBED    = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def SEC_HEADERS(host: str | None = None):\n",
    "    h = {\n",
    "        \"User-Agent\": f\"severin (contact: {os.getenv('SEC_EMAIL','email@example.com')})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "    if host:\n",
    "        h[\"Host\"] = host\n",
    "    return h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251adc1",
   "metadata": {},
   "source": [
    "### News Fetch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Alpha Vantage news (unchanged behavior, small cleanups) ---\n",
    "def fetch_news(tickers, days_back=7, limit=50):\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime(\"%Y%m%dT%H%M\")\n",
    "    params = {\n",
    "        \"function\": \"NEWS_SENTIMENT\",\n",
    "        \"tickers\": \",\".join([t.upper() for t in tickers[:20]]),\n",
    "        \"time_from\": start,\n",
    "        \"sort\": \"LATEST\",\n",
    "        \"limit\": int(limit),\n",
    "        \"apikey\": os.getenv(\"ALPHAVANTAGE_API_KEY\"),\n",
    "    }\n",
    "    params = {k: v for k, v in params.items() if v not in (None, \"\")}\n",
    "\n",
    "    r = requests.get(AV_BASE, params=params, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        raise RuntimeError(f\"HTTP {r.status_code}: {r.text}\")\n",
    "    data = r.json()\n",
    "\n",
    "    if \"Information\" in data:\n",
    "        raise RuntimeError(f\"Alpha Vantage: {data['Information']}\")\n",
    "    if \"Note\" in data:\n",
    "        print(\"Rate limit hit; sleeping 15s…\")\n",
    "        time.sleep(15)\n",
    "        return fetch_news(tickers, days_back, limit)\n",
    "    if \"feed\" not in data:\n",
    "        raise RuntimeError(f\"Unexpected response: {data}\")\n",
    "\n",
    "    rows = []\n",
    "    for item in data[\"feed\"]:\n",
    "        for ts in item.get(\"ticker_sentiment\", []):\n",
    "            rows.append({\n",
    "                \"dt\": pd.to_datetime(item.get(\"time_published\"), format=\"%Y%m%dT%H%M%S\", utc=True, errors=\"coerce\"),\n",
    "                \"ticker\": ts.get(\"ticker\"),\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"summary\": item.get(\"summary\") or \"\",\n",
    "                \"source\": item.get(\"source\"),\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"av_relevance\": float(ts.get(\"relevance_score\") or 0),\n",
    "                \"av_sentiment\": float(ts.get(\"ticker_sentiment_score\") or 0),\n",
    "            })\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        print(\"No articles found.\")\n",
    "        return df\n",
    "\n",
    "    df[\"text\"] = (df[\"title\"].fillna(\"\").str.strip() + \". \" + df[\"summary\"].fillna(\"\").str.strip()).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].drop_duplicates(subset=[\"url\",\"ticker\"])\n",
    "    return df.sort_values(\"dt\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def fetch_multi_tickers(ticker_list, days_back=7, limit=50, sleep_s=12):\n",
    "    \"\"\"Free tier: call one ticker at a time; respect ~5 req/min.\"\"\"\n",
    "    frames = []\n",
    "    for t in ticker_list:\n",
    "        try:\n",
    "            print(f\"> Fetching {t} …\")\n",
    "            df = fetch_news([t], days_back=days_back, limit=limit)\n",
    "            if not df.empty: frames.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ {t}: {e}\")\n",
    "        time.sleep(sleep_s)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88d71",
   "metadata": {},
   "source": [
    "### Fundamentals + Discounted Cash Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5adf6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SEC fundamentals & valuation (NEW) ---\n",
    "\n",
    "# Ticker -> CIK\n",
    "def get_cik(ticker: str) -> str:\n",
    "    t = ticker.upper().strip()\n",
    "    map_url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = requests.get(map_url, headers=SEC_HEADERS(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    for _, rec in data.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "# Company facts (US-GAAP)\n",
    "def get_company_facts(cik: str) -> dict:\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _get_units(facts: dict, key: str):\n",
    "    return facts.get(\"facts\",{}).get(\"us-gaap\",{}).get(key,{}).get(\"units\",{})\n",
    "\n",
    "def _series(values):\n",
    "    rows=[]\n",
    "    for v in values:\n",
    "        end=v.get(\"end\"); val=v.get(\"val\")\n",
    "        if end is None or val is None: continue\n",
    "        try: rows.append((datetime.fromisoformat(end), float(val)))\n",
    "        except Exception: continue\n",
    "    rows.sort(key=lambda x: x[0], reverse=True)\n",
    "    return rows\n",
    "\n",
    "def _ttm_sum(values, n=4):\n",
    "    rows=_series(values)\n",
    "    return float(np.nansum([x[1] for x in rows[:n]])) if rows else None\n",
    "\n",
    "def _latest(values):\n",
    "    rows=_series(values)\n",
    "    return rows[0][1] if rows else None\n",
    "\n",
    "EXTRA_KEYS = {\n",
    "    \"revenue\": [\"Revenues\",\"RevenueFromContractWithCustomerExcludingAssessedTax\",\"SalesRevenueNet\"],\n",
    "    \"net_income\": [\"NetIncomeLoss\"],\n",
    "    \"eps_diluted\": [\"EarningsPerShareDiluted\"],\n",
    "    \"shares_out\": [\"CommonStockSharesOutstanding\",\"WeightedAverageNumberOfDilutedSharesOutstanding\"],\n",
    "    \"cfo\": [\"NetCashProvidedByUsedInOperatingActivities\"],\n",
    "    \"capex\": [\"PaymentsToAcquirePropertyPlantAndEquipment\",\"PaymentsToAcquireProductiveAssets\"],\n",
    "    \"cash\": [\"CashAndCashEquivalentsAtCarryingValue\"],\n",
    "    \"liabilities\": [\"Liabilities\"],\n",
    "}\n",
    "\n",
    "def extract_gaap_ttm_extended(facts_json: dict) -> dict:\n",
    "    fx = facts_json\n",
    "    out = {}\n",
    "    # helper to collect by key candidates across units\n",
    "    def pick(keys, prefer=(\"USD\",\"shares\",\"USD/shares\")):\n",
    "        for k in keys:\n",
    "            units = _get_units(fx, k)\n",
    "            for u in prefer:\n",
    "                if u in units:\n",
    "                    return units[u]\n",
    "        return []\n",
    "\n",
    "    rev   = pick(EXTRA_KEYS[\"revenue\"])\n",
    "    ni    = pick(EXTRA_KEYS[\"net_income\"])\n",
    "    eps   = pick(EXTRA_KEYS[\"eps_diluted\"])\n",
    "    sh    = pick(EXTRA_KEYS[\"shares_out\"])\n",
    "    cfo   = pick(EXTRA_KEYS[\"cfo\"])\n",
    "    capex = pick(EXTRA_KEYS[\"capex\"])\n",
    "    cash  = pick(EXTRA_KEYS[\"cash\"])\n",
    "    liab  = pick(EXTRA_KEYS[\"liabilities\"])\n",
    "\n",
    "    out[\"revenue_ttm\"]       = _ttm_sum(rev)\n",
    "    out[\"net_income_ttm\"]    = _ttm_sum(ni)\n",
    "    out[\"eps_diluted_ttm\"]   = _latest(eps)\n",
    "    out[\"shares_out_latest\"] = _latest(sh)\n",
    "    out[\"cfo_ttm\"]           = _ttm_sum(cfo)\n",
    "    out[\"capex_ttm\"]         = _ttm_sum(capex)  # usually negative in GAAP files\n",
    "    # Free Cash Flow ~ CFO - CapEx (capex sign may be negative; use abs)\n",
    "    if out[\"cfo_ttm\"] is not None and out[\"capex_ttm\"] is not None:\n",
    "        out[\"fcf_ttm\"] = out[\"cfo_ttm\"] - abs(out[\"capex_ttm\"])\n",
    "    else:\n",
    "        out[\"fcf_ttm\"] = None\n",
    "    out[\"cash_latest\"]       = _latest(cash)\n",
    "    out[\"liabilities_latest\"]= _latest(liab)\n",
    "    return out\n",
    "\n",
    "# Latest price (AV)\n",
    "def latest_price(ticker: str):\n",
    "    key = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "    if not key: return None\n",
    "    r = requests.get(AV_BASE, params={\n",
    "        \"function\":\"TIME_SERIES_DAILY_ADJUSTED\",\"symbol\":ticker.upper(),\n",
    "        \"outputsize\":\"compact\",\"apikey\":key\n",
    "    }, timeout=30)\n",
    "    try:\n",
    "        js=r.json(); ts=js[\"Time Series (Daily)\"]\n",
    "        last_key=sorted(ts.keys(), reverse=True)[0]\n",
    "        return float(ts[last_key][\"5. adjusted close\"])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Multiples anchor with more realistic tech defaults (tuneable)\n",
    "def fair_value_multiples(gaap: dict, pe=25.0, ps=6.0):\n",
    "    res={}\n",
    "    eps=gaap.get(\"eps_diluted_ttm\"); rev=gaap.get(\"revenue_ttm\"); sh=gaap.get(\"shares_out_latest\")\n",
    "    if eps and eps>0: res[\"pe_anchor\"]=eps*pe\n",
    "    if rev and sh and sh>0: res[\"ps_anchor\"]=(rev/sh)*ps\n",
    "    if res:\n",
    "        anchors=[v for v in (res.get(\"pe_anchor\"), res.get(\"ps_anchor\")) if v is not None]\n",
    "        res[\"fair_value_mid\"]=float(np.mean(anchors))\n",
    "        res[\"fair_value_low\"]=float(np.min(anchors))\n",
    "        res[\"fair_value_high\"]=float(np.max(anchors))\n",
    "    return res\n",
    "\n",
    "# Simple DCF on FCF\n",
    "def fair_value_dcf(gaap: dict, growth=0.07, discount=0.09, terminal=0.025, years=5):\n",
    "    fcf=gaap.get(\"fcf_ttm\"); sh=gaap.get(\"shares_out_latest\")\n",
    "    if not fcf or not sh: return None\n",
    "    pv=0.0; f=fcf\n",
    "    for yr in range(1, years+1):\n",
    "        f *= (1+growth)\n",
    "        pv += f / ((1+discount)**yr)\n",
    "    tv = (f*(1+terminal))/(discount-terminal)\n",
    "    pv += tv/((1+discount)**years)\n",
    "    return pv / sh\n",
    "\n",
    "def intrinsic_valuation(ticker: str):\n",
    "    cik  = get_cik(ticker)\n",
    "    facts= get_company_facts(cik)\n",
    "    gaap = extract_gaap_ttm_extended(facts)\n",
    "    price= latest_price(ticker)\n",
    "    mult = fair_value_multiples(gaap, pe=25.0, ps=6.0)\n",
    "    dcf  = fair_value_dcf(gaap, growth=0.07, discount=0.09, terminal=0.025, years=5)\n",
    "    out = {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"cik\": cik,\n",
    "        \"gaap_ttm\": gaap,\n",
    "        \"price\": price,\n",
    "        \"multiples\": mult,\n",
    "        \"dcf_anchor\": dcf,\n",
    "    }\n",
    "    # combine if both present\n",
    "    anchors=[]\n",
    "    if mult.get(\"fair_value_mid\"): anchors.append(mult[\"fair_value_mid\"])\n",
    "    if dcf: anchors.append(dcf)\n",
    "    if anchors:\n",
    "        out[\"fair_value_blend\"]=float(np.mean(anchors))\n",
    "        if price: out[\"upside_pct\"]=100.0*(out[\"fair_value_blend\"]/price-1.0)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3defa",
   "metadata": {},
   "source": [
    "### Finbert Loading and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e92ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FinBERT sentiment + embeddings (same as you had) ---\n",
    "def load_models():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    emb  = SentenceTransformer(EMBED)\n",
    "    return pipe, emb\n",
    "\n",
    "def finbert_and_embed(df, pipe, emb, max_len=256, batch=32):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores=[]\n",
    "    for i in range(0, len(texts), batch):\n",
    "        out = pipe(texts[i:i+batch], max_length=max_len)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    vecs = emb.encode(texts, batch_size=batch, convert_to_numpy=True, normalize_embeddings=True) if texts else np.zeros((0,384))\n",
    "    df[\"embed\"] = [v.tolist() for v in vecs]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd492c",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a604ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FinBERT sentiment + embeddings (same as you had) ---\n",
    "def load_models():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    emb  = SentenceTransformer(EMBED)\n",
    "    return pipe, emb\n",
    "\n",
    "def finbert_and_embed(df, pipe, emb, max_len=256, batch=32):\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores=[]\n",
    "    for i in range(0, len(texts), batch):\n",
    "        out = pipe(texts[i:i+batch], max_length=max_len)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    vecs = emb.encode(texts, batch_size=batch, convert_to_numpy=True, normalize_embeddings=True) if texts else np.zeros((0,384))\n",
    "    df[\"embed\"] = [v.tolist() for v in vecs]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef760efb",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3d54052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news…\n",
      "Got 242 rows\n",
      "Scoring with FinBERT + embeddings…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved news_finbert_sample.csv — ready to join with prices for labels.\n",
      "\n",
      "Intrinsic valuation snapshots:\n",
      "{\n",
      "  \"ticker\": \"AAPL\",\n",
      "  \"cik\": \"0000320193\",\n",
      "  \"gaap_ttm\": {\n",
      "    \"revenue_ttm\": 442897000000.0,\n",
      "    \"net_income_ttm\": 193868000000.0,\n",
      "    \"eps_diluted_ttm\": 5.62,\n",
      "    \"shares_out_latest\": 14856722000.0,\n",
      "    \"cfo_ttm\": 283830000000.0,\n",
      "    \"capex_ttm\": 27871000000.0,\n",
      "    \"fcf_ttm\": 255959000000.0,\n",
      "    \"cash_latest\": 36269000000.0,\n",
      "    \"liabilities_latest\": 265665000000.0\n",
      "  },\n",
      "  \"price\": null,\n",
      "  \"multiples\": {\n",
      "    \"pe_anchor\": 140.5,\n",
      "    \"ps_anchor\": 178.86731676072287,\n",
      "    \"fair_value_mid\": 159.68365838036144,\n",
      "    \"fair_value_low\": 140.5,\n",
      "    \"fair_value_high\": 178.86731676072287\n",
      "  },\n",
      "  \"dcf_anchor\": 329.168533954489,\n",
      "  \"fair_value_blend\": 244.42609616742521\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# --- Runner: news sentiment + intrinsic value ---\n",
    "assert os.getenv(\"ALPHAVANTAGE_API_KEY\"), \"Set ALPHAVANTAGE_API_KEY in your .env\"\n",
    "if not os.getenv(\"SEC_EMAIL\"):\n",
    "    print(\"Tip: add SEC_EMAIL=you@example.com to .env for SEC API etiquette.\")\n",
    "\n",
    "tickers = sys.argv[1:] or [\"AAPL\",\"MSFT\",\"NVDA\"]\n",
    "\n",
    "# 1) News → FinBERT\n",
    "print(\"Fetching news…\")\n",
    "df_news = fetch_news([\"AAPL\"], days_back=7, limit=50)  # or fetch_multi_tickers(tickers)\n",
    "print(f\"Got {len(df_news)} rows\")\n",
    "if not df_news.empty:\n",
    "    print(\"Scoring with FinBERT + embeddings…\")\n",
    "    pipe, emb = load_models()\n",
    "    df_scored = finbert_and_embed(df_news, pipe, emb)\n",
    "    out_csv = \"news_finbert_sample.csv\"\n",
    "    df_scored[[\"dt\",\"ticker\",\"source\",\"url\",\"av_relevance\",\"av_sentiment\",\n",
    "               \"finbert_pos\",\"finbert_neu\",\"finbert_neg\",\"title\",\"summary\",\"embed\"]].to_csv(out_csv, index=False)\n",
    "    print(f\"Saved {out_csv} — ready to join with prices for labels.\")\n",
    "\n",
    "# 2) Fundamentals → DCF & multiples\n",
    "print(\"\\nIntrinsic valuation snapshots:\")\n",
    "for t in [\"AAPL\"]:  # change to tickers\n",
    "    try:\n",
    "        val = intrinsic_valuation(t)\n",
    "        print(json.dumps(val, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"{t}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
