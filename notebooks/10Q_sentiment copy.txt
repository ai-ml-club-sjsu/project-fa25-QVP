{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21448bdd",
   "metadata": {},
   "source": [
    "# Market Sentiment with Alpha Vantage\n",
    "\n",
    "So far in this code I've built the basic pipeline to just pull news articles from Alpha Vantage and then pass them into finBERT before putting all of that data into a CSV."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5778f5cd",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Set up all the environment variables and install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf696f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# pip install requests pandas numpy transformers sentence-transformers beautifulsoup4 lxml python-dotenv\n",
    "import os, re, time, math, json, requests, numpy as np, pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings\n",
    "from bs4.builder import XMLParsedAsHTMLWarning\n",
    "from dotenv import load_dotenv\n",
    "warnings.filterwarnings(\"ignore\", category=XMLParsedAsHTMLWarning)\n",
    "\n",
    "# ML (FinBERT)\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "load_dotenv()\n",
    "SEC_EMAIL = os.getenv(\"SEC_EMAIL\", \"email@example.com\")\n",
    "POLYGON_KEY = os.getenv(\"POLYGON_API_KEY\")  # <- put your Polygon key in .env\n",
    "assert POLYGON_KEY, \"Set POLYGON_API_KEY in your .env\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2251adc1",
   "metadata": {},
   "source": [
    "### Config & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a048eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINBERT = \"yiyanghkust/finbert-tone\"\n",
    "ALPHA_SECTIONS = [  # regexes to pull high-signal parts\n",
    "    (r\"item\\s+2\\.\\s*management[’']?s discussion and analysis.*?(?=item\\s+3\\.)\", \"MD&A\"),\n",
    "    (r\"item\\s+1a\\.\\s*risk factors.*?(?=item\\s+2\\.)\", \"RiskFactors\"),\n",
    "    (r\"results of operations.*?(?=liquidity|capital resources|item\\s+\\d)\", \"Results\"),\n",
    "]\n",
    "POS_PHRASES = [\n",
    "    r\"strong demand\", r\"margin expansion\", r\"raised guidance\", r\"record (revenue|earnings)\",\n",
    "    r\"cost (reductions|optimization)\", r\"share repurchase\", r\"cash flow (improved|growth)\"\n",
    "]\n",
    "NEG_PHRASES = [\n",
    "    r\"decline in (sales|revenue)\", r\"margin compression\", r\"impairment charge\",\n",
    "    r\"supply chain disruption\", r\"adversely affected\", r\"weaker demand\", r\"material weakness\"\n",
    "]\n",
    "\n",
    "def SEC_HEADERS():\n",
    "    return {\n",
    "        \"User-Agent\": f\"CrowdQuant Backtest (contact: {SEC_EMAIL})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "\n",
    "def _make_soup(html: str) -> BeautifulSoup:\n",
    "    for parser in (\"lxml\",\"html5lib\",\"html.parser\"):\n",
    "        try: return BeautifulSoup(html, parser)\n",
    "        except Exception: pass\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def _lower_clean(txt: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\",\" \", txt.lower())\n",
    "\n",
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT)\n",
    "    return TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "\n",
    "FINBERT_PIPE = load_finbert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a88d71",
   "metadata": {},
   "source": [
    "### SEC stuff\n",
    "Find CIK codes per ticker, get 10Qs, fetch HTML and extract relevant data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adf6e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    js = requests.get(url, headers=SEC_HEADERS(), timeout=30).json()\n",
    "    t = ticker.upper()\n",
    "    for _, rec in js.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "def list_10q_with_dates(cik: str, max_n=8):\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    rec = r.json().get(\"filings\",{}).get(\"recent\",{})\n",
    "    out = []\n",
    "    for form, acc, prim, fdate in zip(rec.get(\"form\",[]),\n",
    "                                      rec.get(\"accessionNumber\",[]),\n",
    "                                      rec.get(\"primaryDocument\",[]),\n",
    "                                      rec.get(\"filingDate\",[])):\n",
    "        if form == \"10-Q\":\n",
    "            out.append({\n",
    "                \"accession\": acc.replace(\"-\",\"\"),\n",
    "                \"primary\": prim,\n",
    "                \"filing_date\": fdate,  # YYYY-MM-DD (UTC)\n",
    "            })\n",
    "        if len(out) >= max_n: break\n",
    "    return out\n",
    "\n",
    "def fetch_filing_html(cik:str, accession:str, primary:str) -> str:\n",
    "    base = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession}\"\n",
    "    url  = f\"{base}/{primary}\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=60); r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def extract_sections(html: str, patterns=ALPHA_SECTIONS, fallback_full=True, cap=200000) -> dict:\n",
    "    soup = _make_soup(html)\n",
    "    txt  = soup.get_text(\"\\n\", strip=True)\n",
    "    low  = _lower_clean(txt)\n",
    "    out = {}\n",
    "    for pat, name in patterns:\n",
    "        m = re.search(pat, low, flags=re.S)\n",
    "        if m:\n",
    "            out[name] = low[m.start():m.end()][:cap]\n",
    "    if not out and fallback_full:\n",
    "        out[\"FullDocument\"] = low[:cap]\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba3defa",
   "metadata": {},
   "source": [
    "### Finbert scoring -> features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e92ccc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def finbert_sent(text: str) -> dict:\n",
    "    row = FINBERT_PIPE([text])[0]\n",
    "    d = {x[\"label\"].lower(): x[\"score\"] for x in row}\n",
    "    score = d.get(\"positive\",0)-d.get(\"negative\",0)  # pos - neg\n",
    "    return {\"pos\": d.get(\"positive\",0.0), \"neu\": d.get(\"neutral\",0.0), \"neg\": d.get(\"negative\",0.0), \"sent\": score}\n",
    "\n",
    "def phrase_boost(text: str, pos_list=POS_PHRASES, neg_list=NEG_PHRASES, w=0.1) -> float:\n",
    "    boost = 0.0\n",
    "    for p in pos_list:\n",
    "        if re.search(p, text, flags=re.I): boost += w\n",
    "    for n in neg_list:\n",
    "        if re.search(n, text, flags=re.I): boost -= w\n",
    "    return boost\n",
    "\n",
    "def score_sections(sections: dict) -> dict:\n",
    "    feats = {}\n",
    "    for name, text in sections.items():\n",
    "        fb = finbert_sent(text)\n",
    "        boost = phrase_boost(text)\n",
    "        feats[f\"{name}_pos\"] = fb[\"pos\"]\n",
    "        feats[f\"{name}_neg\"] = fb[\"neg\"]\n",
    "        feats[f\"{name}_sent\"] = fb[\"sent\"] + boost\n",
    "        feats[f\"{name}_boost\"] = boost\n",
    "    # aggregate (simple average over available sections)\n",
    "    sents = [v for k,v in feats.items() if k.endswith(\"_sent\")]\n",
    "    feats[\"sent_overall\"] = float(np.mean(sents)) if sents else np.nan\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1190ff",
   "metadata": {},
   "source": [
    "### Fetch Polygon Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45e3711",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_daily(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    # YYYY-MM-DD → YYYY-MM-DD (inclusive)\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\": \"true\", \"sort\": \"asc\", \"limit\": 50000, \"apiKey\": POLYGON_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    js = r.json()\n",
    "    rows = js.get(\"results\", []) or []\n",
    "    if not rows: return pd.DataFrame(columns=[\"t\",\"c\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"c\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    df = df.drop(columns=[\"t\"]).rename(columns={\"c\":\"close\"}).drop_duplicates(\"date\")\n",
    "    return df\n",
    "\n",
    "def next_trading_close(df: pd.DataFrame, target_date: datetime.date):\n",
    "    # df has 'date','close' sorted asc by date\n",
    "    # return the first bar on or after target_date\n",
    "    s = df[df[\"date\"] >= target_date]\n",
    "    return None if s.empty else float(s.iloc[0][\"close\"])\n",
    "\n",
    "def event_closes(ticker: str, filing_date: str) -> dict:\n",
    "    d0 = datetime.strptime(filing_date, \"%Y-%m-%d\").date()\n",
    "    d7 = d0 + timedelta(days=7)\n",
    "    d28 = d0 + timedelta(days=28)\n",
    "    start = (d0 - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    end   = (d28 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df.empty:\n",
    "        return {\"close_0\": np.nan, \"close_7\": np.nan, \"close_28\": np.nan}\n",
    "    c0 = next_trading_close(df, d0)\n",
    "    c7 = next_trading_close(df, d7)\n",
    "    c28= next_trading_close(df, d28)\n",
    "    return {\"close_0\": c0, \"close_7\": c7, \"close_28\": c28}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbd492c",
   "metadata": {},
   "source": [
    "### Stitch it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a604ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_10q_sentiment(tickers, max_filings=6, sleep_sec=0.3):\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        print(f\"[{t}] pulling 10-Qs…\")\n",
    "        cik = get_cik(t)\n",
    "        pairs = list_10q_with_dates(cik, max_n=max_filings)\n",
    "        for p in pairs:\n",
    "            try:\n",
    "                html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  skip {p['accession']} ({e})\"); continue\n",
    "            secs = extract_sections(html)\n",
    "            feats = score_sections(secs)\n",
    "            px = event_closes(t, p[\"filing_date\"])\n",
    "            row = {\n",
    "                \"ticker\": t, \"cik\": cik, \"accession\": p[\"accession\"], \"primary\": p[\"primary\"],\n",
    "                \"filing_date\": p[\"filing_date\"],\n",
    "                **feats, **px\n",
    "            }\n",
    "            # event returns\n",
    "            c0, c7, c28 = row[\"close_0\"], row[\"close_7\"], row[\"close_28\"]\n",
    "            row[\"ret_7\"]  = (c7/c0 - 1.0)*100 if c0 and c7 else np.nan\n",
    "            row[\"ret_28\"] = (c28/c0 - 1.0)*100 if c0 and c28 else np.nan\n",
    "            rows.append(row)\n",
    "            time.sleep(sleep_sec)  # be nice to SEC\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef760efb",
   "metadata": {},
   "source": [
    "### Run everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d54052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news…\n",
      "Got 242 rows\n",
      "Scoring with FinBERT + embeddings…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n",
      "/Users/severinspagnola/Desktop/project-fa25-QVP/venv/lib/python3.13/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved news_finbert_sample.csv — ready to join with prices for labels.\n",
      "\n",
      "Intrinsic valuation snapshots:\n",
      "{\n",
      "  \"ticker\": \"AAPL\",\n",
      "  \"cik\": \"0000320193\",\n",
      "  \"gaap_ttm\": {\n",
      "    \"revenue_ttm\": 442897000000.0,\n",
      "    \"net_income_ttm\": 193868000000.0,\n",
      "    \"eps_diluted_ttm\": 5.62,\n",
      "    \"shares_out_latest\": 14856722000.0,\n",
      "    \"cfo_ttm\": 283830000000.0,\n",
      "    \"capex_ttm\": 27871000000.0,\n",
      "    \"fcf_ttm\": 255959000000.0,\n",
      "    \"cash_latest\": 36269000000.0,\n",
      "    \"liabilities_latest\": 265665000000.0\n",
      "  },\n",
      "  \"price\": null,\n",
      "  \"multiples\": {\n",
      "    \"pe_anchor\": 140.5,\n",
      "    \"ps_anchor\": 178.86731676072287,\n",
      "    \"fair_value_mid\": 159.68365838036144,\n",
      "    \"fair_value_low\": 140.5,\n",
      "    \"fair_value_high\": 178.86731676072287\n",
      "  },\n",
      "  \"dcf_anchor\": 329.168533954489,\n",
      "  \"fair_value_blend\": 244.42609616742521\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Large-cap list: AAPL, MSFT, AMZN, NVDA, GOOGL, META, JPM, V, JNJ, PG, XOM, UNH, PEP, KO, COST, ORCL, DIS, HD, BAC, WMT\n",
    "# Mid-cap list: LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB, DAL, CHRW, WDC, AAP, CZR, CHD, SWKS, COHR, PTC, HOLX, MKTX\n",
    "# Small-cap list: BLKB, HQY, PIPR, HAYW, NVCR, SMPL, MGPI, BE, PRCT, SKYW, AVAV, INMD, VRTS, CNXN, REZI, ASTE, MHO, CELH, ABM, PCT\n",
    "# Micro-cap list: LUNA, GCTK, VTSI, HCAT, CLXT, OPRX, FUV, BGFV, CRTX, AOUT, FCEL, HITI, AWH, WKSP, GRIN, TFFP, HZO, OPTN, TIRX, STRC\n",
    "\n",
    "TICKERS = [\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\",\"META\"]  # edit freely\n",
    "df = backtest_10q_sentiment(TICKERS, max_filings=6)\n",
    "print(df.head())\n",
    "df.to_csv(\"10q_sentiment_event_returns.csv\", index=False)\n",
    "print(\"Saved 10q_sentiment_event_returns.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
