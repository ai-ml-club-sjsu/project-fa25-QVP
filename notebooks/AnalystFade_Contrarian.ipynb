{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60cca536",
   "metadata": {},
   "source": [
    "\n",
    "# Analyst-Revision Fade — Hybrid Contrarian Notebook\n",
    "This notebook implements a systematic fade of analyst price-target/rating adjustments, conditioned on **fundamental disagreement** (DCF/multiples vs target), **media/news sentiment disagreement** (FinBERT), and **technical reversal confirmation** (ATR + VWAP±σ band breaks).\n",
    "\n",
    "**Pipeline**\n",
    "1) Ingest analyst revision events (Finnhub or CSV).\n",
    "2) Pull daily OHLCV (Polygon) and SEC facts for simple DCF/multiples anchors.\n",
    "3) Pull/score news (Alpha Vantage + FinBERT).\n",
    "4) Build features (revision magnitude, disagreement metrics).\n",
    "5) Filter trades by fundamentals & sentiment disagreement.\n",
    "6) Confirm timing via ATR/VWAP band breaks + momentum flip.\n",
    "7) Simulate trades and plot equity curve.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846fc93",
   "metadata": {},
   "source": [
    "## 1) Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b10c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, re, time, math, json, warnings, requests\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "POLYGON_API_KEY   = os.getenv(\"POLYGON_API_KEY\", \"\")\n",
    "ALPHAVANTAGE_KEY  = os.getenv(\"ALPHAVANTAGE_API_KEY\", \"\")\n",
    "FINNHUB_API_KEY   = os.getenv(\"FINNHUB_API_KEY\", \"\")  # optional\n",
    "SEC_EMAIL         = os.getenv(\"SEC_EMAIL\", \"you@example.com\")\n",
    "\n",
    "NEWS_MONTHS_BACK        = 3\n",
    "SENTIMENT_CARRY_DAYS    = 5\n",
    "VWAP_WINDOW_DAYS        = 30\n",
    "VWAP_SIGMA_WINDOW_DAYS  = 20\n",
    "ATR_WINDOW_DAYS         = 14\n",
    "REV_MOM_LOOKBACK_DAYS   = 5\n",
    "MAX_HOLD_DAYS           = 28\n",
    "TRAIL_STOP_PCT          = 0.05\n",
    "FINBERT_ID              = \"yiyanghkust/finbert-tone\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d21cca",
   "metadata": {},
   "source": [
    "## 2) HTTP Session & Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd2fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "def _session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.5, status_forcelist=(429,500,502,503,504))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def _to_datestring(dt: datetime) -> str:\n",
    "    return dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "SEC_HEADERS = {\"User-Agent\": SEC_EMAIL, \"Accept\": \"application/json\"}\n",
    "POLY_BASE   = \"https://api.polygon.io\"\n",
    "AV_BASE     = \"https://www.alphavantage.co/query\"\n",
    "FINNHUB_BASE= \"https://finnhub.io/api/v1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7c7855",
   "metadata": {},
   "source": [
    "## 3) Prices — Polygon daily OHLCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956a668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def polygon_daily_ohlcv(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    url = f\"{POLY_BASE}/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\":\"true\",\"sort\":\"asc\",\"limit\":50000,\"apiKey\":POLYGON_API_KEY}\n",
    "    r = _session().get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    rows = r.json().get(\"results\", []) or []\n",
    "    if not rows:\n",
    "        return pd.DataFrame(columns=[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"o\",\"h\",\"l\",\"c\",\"v\"]].rename(\n",
    "        columns={\"t\":\"ts\",\"o\":\"open\",\"h\":\"high\",\"l\":\"low\",\"c\":\"close\",\"v\":\"volume\"})\n",
    "    df[\"date\"] = pd.to_datetime(df[\"ts\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    df = df.drop(columns=[\"ts\"]).drop_duplicates(\"date\")\n",
    "    return df[[\"date\",\"open\",\"high\",\"low\",\"close\",\"volume\"]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f78eb0e",
   "metadata": {},
   "source": [
    "## 4) Technical Features: ATR, VWAP bands, momentum flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb45036",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_atr(df: pd.DataFrame, n: int = ATR_WINDOW_DAYS) -> pd.Series:\n",
    "    if df.empty: return pd.Series(dtype=float)\n",
    "    h,l,c = df[\"high\"].astype(float), df[\"low\"].astype(float), df[\"close\"].astype(float)\n",
    "    prev_c = c.shift(1)\n",
    "    tr = pd.concat([(h-l), (h-prev_c).abs(), (l-prev_c).abs()], axis=1).max(axis=1)\n",
    "    atr = tr.rolling(n, min_periods=max(2, n//2)).mean()\n",
    "    return atr\n",
    "\n",
    "def compute_vwap_and_sigma(df: pd.DataFrame, window: int = VWAP_WINDOW_DAYS, sigma_win: int = VWAP_SIGMA_WINDOW_DAYS):\n",
    "    if df.empty: return df.assign(vwap=np.nan, vwap_sigma=np.nan, vwap_p=np.nan)\n",
    "    tp = (df[\"high\"] + df[\"low\"] + df[\"close\"]) / 3.0\n",
    "    vol = df[\"volume\"].replace(0, np.nan)\n",
    "    num = (tp * vol).rolling(window, min_periods=5).sum()\n",
    "    den = vol.rolling(window, min_periods=5).sum()\n",
    "    vwap = num / den\n",
    "    resid = df[\"close\"] - vwap\n",
    "    vwap_sigma = resid.rolling(sigma_win, min_periods=5).std(ddof=0)\n",
    "    out = df.copy()\n",
    "    out[\"vwap\"] = vwap\n",
    "    out[\"vwap_sigma\"] = vwap_sigma\n",
    "    out[\"vwap_p\"] = (df[\"close\"] - vwap) / (vwap_sigma.replace(0, np.nan))\n",
    "    return out\n",
    "\n",
    "def momentum_flip(df: pd.DataFrame, look=REV_MOM_LOOKBACK_DAYS) -> pd.Series:\n",
    "    if df.empty: return pd.Series(dtype=float)\n",
    "    roc = df[\"close\"].pct_change(look)\n",
    "    return np.sign(roc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9deb290",
   "metadata": {},
   "source": [
    "## 5) News Sentiment — Alpha Vantage + FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8038c3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_news_single_ticker(ticker: str, days_back: int, limit: int = 100) -> pd.DataFrame:\n",
    "    start = (datetime.now(timezone.utc) - timedelta(days=days_back)).strftime(\"%Y%m%dT%H%M\")\n",
    "    params = {\"function\":\"NEWS_SENTIMENT\",\"tickers\":ticker.upper(),\"time_from\":start,\"sort\":\"LATEST\",\"limit\":int(limit),\"apikey\":ALPHAVANTAGE_KEY}\n",
    "    r = _session().get(AV_BASE, params=params, timeout=30); data = r.json()\n",
    "    if \"Note\" in data:\n",
    "        time.sleep(12); return fetch_news_single_ticker(ticker, days_back, limit)\n",
    "    feed = data.get(\"feed\", []) or []\n",
    "    rows = []\n",
    "    for item in feed:\n",
    "        for ts in item.get(\"ticker_sentiment\", []):\n",
    "            rows.append({\n",
    "                \"dt\": pd.to_datetime(item.get(\"time_published\"), format=\"%Y%m%dT%H%M%S\", utc=True, errors=\"coerce\"),\n",
    "                \"ticker\": ts.get(\"ticker\"),\n",
    "                \"title\": item.get(\"title\") or \"\",\n",
    "                \"summary\": item.get(\"summary\") or \"\",\n",
    "                \"url\": item.get(\"url\"),\n",
    "                \"source\": item.get(\"source\"),\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty: return df\n",
    "    df[\"text\"] = (df[\"title\"].fillna(\"\").str.strip() + \". \" + df[\"summary\"].fillna(\"\").str.strip()).str.strip()\n",
    "    df = df[df[\"text\"].str.len() > 0].drop_duplicates(subset=[\"url\",\"ticker\"])\n",
    "    return df.sort_values(\"dt\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_ID)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_ID)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, return_all_scores=True, truncation=True)\n",
    "    return pipe\n",
    "\n",
    "def score_news_finbert(df: pd.DataFrame, pipe) -> pd.DataFrame:\n",
    "    if df.empty: return df.assign(finbert_pos=[], finbert_neu=[], finbert_neg=[], net_sentiment=[])\n",
    "    texts = df[\"text\"].tolist()\n",
    "    scores = []\n",
    "    for i in range(0, len(texts), 32):\n",
    "        out = pipe(texts[i:i+32], max_length=256)\n",
    "        for row in out:\n",
    "            d = {dct[\"label\"].lower(): dct[\"score\"] for dct in row}\n",
    "            scores.append([d.get(\"positive\",0.0), d.get(\"neutral\",0.0), d.get(\"negative\",0.0)])\n",
    "    S = np.array(scores) if scores else np.zeros((0,3))\n",
    "    df[\"finbert_pos\"], df[\"finbert_neu\"], df[\"finbert_neg\"] = S[:,0], S[:,1], S[:,2]\n",
    "    df[\"net_sentiment\"] = df[\"finbert_pos\"] - df[\"finbert_neg\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5a1db",
   "metadata": {},
   "source": [
    "## 6) SEC Facts → DCF & Multiples Anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def _zero_pad_cik(x: str) -> str:\n",
    "    return f\"{int(x):010d}\"\n",
    "\n",
    "def _load_ticker_table() -> dict:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = _session().get(url, headers=SEC_HEADERS, timeout=30); r.raise_for_status()\n",
    "    raw = r.json()\n",
    "    return {row[\"ticker\"].upper(): _zero_pad_cik(str(row[\"cik_str\"])) for _, row in raw.items()}\n",
    "\n",
    "def normalize_cik(identifier: str) -> str:\n",
    "    s = str(identifier).strip().upper()\n",
    "    if s.startswith(\"CIK\"):\n",
    "        s = s[3:].strip()\n",
    "    if re.fullmatch(r\"\\d+\", s):\n",
    "        return _zero_pad_cik(s)\n",
    "    table = _load_ticker_table()\n",
    "    if s in table:\n",
    "        return table[s]\n",
    "    raise ValueError(f\"Could not resolve identifier to CIK: {identifier}\")\n",
    "\n",
    "def get_company_facts(ticker_or_cik):\n",
    "    cik10 = normalize_cik(ticker_or_cik)\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik10}.json\"\n",
    "    r = _session().get(url, headers=SEC_HEADERS, timeout=30); r.raise_for_status()\n",
    "    time.sleep(0.2)\n",
    "    return r.json()\n",
    "\n",
    "def _ttm_sum(items, n=4):\n",
    "    if not items: return None\n",
    "    vals = [x.get(\"val\") for x in items][-n:]\n",
    "    vals = [v for v in vals if v is not None]\n",
    "    return float(np.nansum(vals)) if vals else None\n",
    "\n",
    "def build_ttm_metrics(facts: dict) -> dict:\n",
    "    usgaap = facts.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "    def get_series(tag):\n",
    "        return (usgaap.get(tag, {}).get(\"units\", {}).get(\"USD\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"USD/shares\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"shares\", []))\n",
    "    revenue_q   = get_series(\"Revenues\")\n",
    "    ni_q        = get_series(\"NetIncomeLoss\")\n",
    "    eps_q       = usgaap.get(\"EarningsPerShareDiluted\", {}).get(\"units\", {}).get(\"USD/shares\", [])\n",
    "    dil_sh_q    = get_series(\"WeightedAverageNumberOfDilutedSharesOutstanding\")\n",
    "    cfo_q       = get_series(\"NetCashProvidedByUsedInOperatingActivities\")\n",
    "    capex_q     = get_series(\"PaymentsToAcquirePropertyPlantAndEquipment\")\n",
    "\n",
    "    revenue_ttm     = _ttm_sum(revenue_q)\n",
    "    net_income_ttm  = _ttm_sum(ni_q)\n",
    "    eps_ttm         = _ttm_sum(eps_q)\n",
    "    diluted_sh_ttm  = _ttm_sum(dil_sh_q)\n",
    "    cfo_ttm         = _ttm_sum(cfo_q)\n",
    "    capex_ttm       = _ttm_sum(capex_q)\n",
    "    fcf_ttm         = (cfo_ttm or 0.0) - abs(capex_ttm or 0.0)\n",
    "\n",
    "    return dict(\n",
    "        revenue_ttm=revenue_ttm, net_income_ttm=net_income_ttm,\n",
    "        eps_diluted_ttm=eps_ttm, diluted_shares_ttm=diluted_sh_ttm,\n",
    "        cfo_ttm=cfo_ttm, capex_ttm=capex_ttm, fcf_ttm=fcf_ttm,\n",
    "        rev_per_share=(revenue_ttm / diluted_sh_ttm) if (revenue_ttm and diluted_sh_ttm) else None,\n",
    "    )\n",
    "\n",
    "SECTOR_MULTIPLES = {\"Technology\":{\"PE\":30.0,\"PS\":6.8}, \"_default\":{\"PE\":18.0,\"PS\":2.5}}\n",
    "\n",
    "def multiples_anchor(metrics:dict, sector=\"Technology\"):\n",
    "    cfg = SECTOR_MULTIPLES.get(sector, SECTOR_MULTIPLES[\"_default\"])\n",
    "    eps = metrics.get(\"eps_diluted_ttm\")\n",
    "    rps = metrics.get(\"rev_per_share\")\n",
    "    pe_anchor = eps * cfg[\"PE\"] if eps else None\n",
    "    ps_anchor = rps * cfg[\"PS\"] if rps else None\n",
    "    anchors = [x for x in (pe_anchor, ps_anchor) if x is not None and math.isfinite(x)]\n",
    "    mid = float(np.mean(anchors)) if anchors else None\n",
    "    return {\"pe_anchor\":pe_anchor, \"ps_anchor\":ps_anchor, \"fair_value_mid\":mid, \"assumptions\":cfg}\n",
    "\n",
    "def dcf_anchor(metrics:dict, years=5, g=0.04, r=0.095, g_term=0.02):\n",
    "    fcf0 = metrics.get(\"fcf_ttm\")\n",
    "    sh   = metrics.get(\"diluted_shares_ttm\")\n",
    "    if not fcf0 or not sh or sh <= 0: return None\n",
    "    pv, fcf = 0.0, fcf0\n",
    "    for t in range(1, years+1):\n",
    "        fcf *= (1+g); pv += fcf / ((1+r)**t)\n",
    "    terminal = (fcf * (1+g_term)) / (r - g_term)\n",
    "    pv_term  = terminal / ((1+r)**years)\n",
    "    return (pv + pv_term) / sh\n",
    "\n",
    "def blended_fair_value(mult_mid, dcf_val, w=0.5):\n",
    "    if mult_mid is None and dcf_val is None: return None\n",
    "    if mult_mid is None: return dcf_val\n",
    "    if dcf_val is None:  return mult_mid\n",
    "    return float(w*mult_mid + (1-w)*dcf_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889bb180",
   "metadata": {},
   "source": [
    "## 7) Analyst Revision Events — Finnhub or CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2728cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def finnhub_price_targets(ticker: str, months_back: int = 6) -> pd.DataFrame:\n",
    "    # Fetch price target info if your Finnhub plan supports it; else returns empty.\n",
    "    if not FINNHUB_API_KEY:\n",
    "        return pd.DataFrame()\n",
    "    since = (datetime.utcnow() - timedelta(days=30*months_back)).strftime(\"%Y-%m-%d\")\n",
    "    url = f\"{FINNHUB_BASE}/stock/price-target\"\n",
    "    params = {\"symbol\": ticker.upper(), \"from\": since, \"token\": FINNHUB_API_KEY}\n",
    "    r = _session().get(url, params=params, timeout=30)\n",
    "    if r.status_code != 200:\n",
    "        return pd.DataFrame()\n",
    "    j = r.json() or {}\n",
    "    # If your plan returns a snapshot-like dict, normalize basic fields\n",
    "    if isinstance(j, dict) and \"target\" in j:\n",
    "        rows = [{\n",
    "            \"ticker\": ticker.upper(),\n",
    "            \"event_dt\": datetime.utcnow().date(),\n",
    "            \"old_target\": np.nan,\n",
    "            \"new_target\": j.get(\"target\"),\n",
    "            \"rating_action\": \"update\",\n",
    "            \"analyst\": j.get(\"lastUpdatedSource\"),\n",
    "            \"source\": \"finnhub\"\n",
    "        }]\n",
    "        return pd.DataFrame(rows)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_analyst_events_csv(path: str) -> pd.DataFrame:\n",
    "    # CSV columns: ticker,event_dt,old_target,new_target,rating_action,analyst,source\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"event_dt\"] = pd.to_datetime(df[\"event_dt\"]).dt.date\n",
    "    for col in [\"old_target\",\"new_target\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "    return df\n",
    "\n",
    "def build_revision_features(df_events: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df_events.empty: return df_events\n",
    "    out = df_events.copy()\n",
    "    if \"old_target\" in out.columns and \"new_target\" in out.columns:\n",
    "        out[\"rev_mag_pct\"] = (out[\"new_target\"]/out[\"old_target\"] - 1.0) * 100.0\n",
    "    else:\n",
    "        out[\"rev_mag_pct\"] = np.nan\n",
    "    out[\"is_upgrade\"] = out.get(\"rating_action\",\"\").astype(str).str.lower().str.contains(\"upgrad\").astype(int)\n",
    "    out[\"is_downgrade\"] = out.get(\"rating_action\",\"\").astype(str).str.lower().str.contains(\"downgrad\").astype(int)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358321c1",
   "metadata": {},
   "source": [
    "## 8) Disagreement & Confirmation Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e761acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@dataclass\n",
    "class DisagreementThresholds:\n",
    "    target_over_fair_x: float = 1.2\n",
    "    min_rev_mag_pct: float   = 5.0\n",
    "    min_sent_div: float      = 0.05\n",
    "    vwap_band: float         = 1.0\n",
    "\n",
    "def compute_fair_value_snapshot(ticker: str, sector_hint=\"Technology\") -> Optional[float]:\n",
    "    try:\n",
    "        facts = get_company_facts(ticker)\n",
    "        metrics = build_ttm_metrics(facts)\n",
    "        mult_mid = multiples_anchor(metrics, sector=sector_hint)[\"fair_value_mid\"]\n",
    "        dcf_val  = dcf_anchor(metrics)\n",
    "        return blended_fair_value(mult_mid, dcf_val, w=0.5)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def compute_recent_news_sentiment(ticker: str, months_back: int = NEWS_MONTHS_BACK) -> Optional[float]:\n",
    "    try:\n",
    "        df = fetch_news_single_ticker(ticker, days_back=months_back*30, limit=100)\n",
    "        if df.empty: return None\n",
    "        pipe = load_finbert()\n",
    "        df = score_news_finbert(df, pipe)\n",
    "        df[\"date\"] = df[\"dt\"].dt.date\n",
    "        d = df.groupby(\"date\")[\"net_sentiment\"].mean().sort_index()\n",
    "        d = d.rolling(3, min_periods=1).mean()\n",
    "        return float(d.iloc[-1])\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def technical_confirmation(df_ohlcv: pd.DataFrame, event_date: date, direction: int, vwap_band: float=1.0) -> bool:\n",
    "    if df_ohlcv.empty: return False\n",
    "    start = (pd.to_datetime(event_date) - pd.Timedelta(days=60)).date()\n",
    "    df = df_ohlcv[df_ohlcv[\"date\"] >= start].copy()\n",
    "    df = compute_vwap_and_sigma(df, window=VWAP_WINDOW_DAYS, sigma_win=VWAP_SIGMA_WINDOW_DAYS)\n",
    "    mom = momentum_flip(df, look=REV_MOM_LOOKBACK_DAYS)\n",
    "    df[\"mom_flip\"] = mom\n",
    "    s = df[df[\"date\"] >= event_date]\n",
    "    if s.empty: return False\n",
    "    row = s.iloc[0]\n",
    "    if direction == -1:\n",
    "        cond_band = (row[\"close\"] <= (row[\"vwap\"] + vwap_band * row[\"vwap_sigma\"]))\n",
    "        cond_mom  = (row[\"mom_flip\"] <= 0)\n",
    "        return bool(cond_band and cond_mom)\n",
    "    else:\n",
    "        cond_band = (row[\"close\"] >= (row[\"vwap\"] - vwap_band * row[\"vwap_sigma\"]))\n",
    "        cond_mom  = (row[\"mom_flip\"] >= 0)\n",
    "        return bool(cond_band and cond_mom)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef1f67f",
   "metadata": {},
   "source": [
    "## 9) Trade Simulator (28d with trailing stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d5933f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simulate_trade_28d(df_ohlc: pd.DataFrame,\n",
    "                       start_date: date,\n",
    "                       direction: int,\n",
    "                       tsl_pct: float = TRAIL_STOP_PCT,\n",
    "                       max_days: int = MAX_HOLD_DAYS) -> dict:\n",
    "    df = df_ohlc[df_ohlc[\"date\"] >= start_date].reset_index(drop=True)\n",
    "    if df.empty:\n",
    "        return {\"filled\": False, \"pnl_pct\": np.nan}\n",
    "\n",
    "    entry_open = float(df.loc[0, \"open\"])\n",
    "    entry_day  = df.loc[0, \"date\"]\n",
    "\n",
    "    run_max = entry_open\n",
    "    run_min = entry_open\n",
    "\n",
    "    def _update_trail_long(px_max): return px_max * (1 - tsl_pct)\n",
    "    def _update_trail_short(px_min): return px_min * (1 + tsl_pct)\n",
    "\n",
    "    exit_reason = \"time\"\n",
    "    exit_price  = float(df.iloc[min(max_days-1, len(df)-1)][\"close\"])\n",
    "    exit_day    = df.iloc[min(max_days-1, len(df)-1)][\"date\"]\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        o,h,l,c, d = float(df.loc[i,\"open\"]), float(df.loc[i,\"high\"]), float(df.loc[i,\"low\"]), float(df.loc[i,\"close\"]), df.loc[i,\"date\"]\n",
    "        if direction == 1:\n",
    "            run_max = max(run_max, h)\n",
    "            stop = _update_trail_long(run_max)\n",
    "            if l <= stop:\n",
    "                exit_reason, exit_price, exit_day = \"tsl\", stop, d\n",
    "                break\n",
    "        else:\n",
    "            run_min = min(run_min, l)\n",
    "            stop = _update_trail_short(run_min)\n",
    "            if h >= stop:\n",
    "                exit_reason, exit_price, exit_day = \"tsl\", stop, d\n",
    "                break\n",
    "\n",
    "    pnl = (exit_price / entry_open - 1.0) * 100.0 if direction == 1 else (entry_open / exit_price - 1.0) * 100.0\n",
    "    return {\n",
    "        \"filled\": True,\n",
    "        \"entry_date\": entry_day,\n",
    "        \"exit_date\": exit_day,\n",
    "        \"entry_price\": entry_open,\n",
    "        \"exit_price\": exit_price,\n",
    "        \"exit_reason\": exit_reason,\n",
    "        \"pnl_pct\": float(round(pnl, 3))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d048a6b",
   "metadata": {},
   "source": [
    "## 10) Backtester Orchestrator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae39d946",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backtest_events(events: pd.DataFrame,\n",
    "                    thresholds: DisagreementThresholds,\n",
    "                    sector_hint=\"Technology\") -> pd.DataFrame:\n",
    "    recs = []\n",
    "    for _, ev in events.iterrows():\n",
    "        tkr = ev[\"ticker\"]\n",
    "        ev_dt = ev[\"event_dt\"]\n",
    "        end = ev_dt + timedelta(days=MAX_HOLD_DAYS+5)\n",
    "        start = (ev_dt - timedelta(days=80)).isoformat()\n",
    "        df_px = polygon_daily_ohlcv(tkr, start, end.isoformat())\n",
    "        if df_px.empty: \n",
    "            continue\n",
    "\n",
    "        direction = -1 if (ev.get(\"rev_mag_pct\", 0) > 0 or ev.get(\"is_upgrade\",0)==1) else 1\n",
    "\n",
    "        fair = compute_fair_value_snapshot(tkr, sector_hint=sector_hint)\n",
    "        last_close = df_px[df_px[\"date\"] <= ev_dt][\"close\"].iloc[-1] if not df_px.empty else None\n",
    "        if fair is None or last_close is None:\n",
    "            continue\n",
    "\n",
    "        tgt = ev.get(\"new_target\", np.nan)\n",
    "        if not np.isfinite(tgt): \n",
    "            continue\n",
    "        disagree = (tgt > thresholds.target_over_fair_x * fair) if direction==-1 else (tgt < (fair / thresholds.target_over_fair_x))\n",
    "        if not disagree: \n",
    "            continue\n",
    "\n",
    "        if abs(ev.get(\"rev_mag_pct\", 0.0)) < thresholds.min_rev_mag_pct:\n",
    "            continue\n",
    "\n",
    "        s_now = compute_recent_news_sentiment(tkr, months_back=NEWS_MONTHS_BACK)\n",
    "        if s_now is None:\n",
    "            continue\n",
    "        news_disagree = (s_now < -thresholds.min_sent_div) if direction==-1 else (s_now > thresholds.min_sent_div)\n",
    "        if not news_disagree:\n",
    "            continue\n",
    "\n",
    "        if not technical_confirmation(df_px, ev_dt, direction, vwap_band=thresholds.vwap_band):\n",
    "            continue\n",
    "\n",
    "        sim = simulate_trade_28d(df_px, ev_dt, direction, tsl_pct=TRAIL_STOP_PCT, max_days=MAX_HOLD_DAYS)\n",
    "        if not sim[\"filled\"]:\n",
    "            continue\n",
    "\n",
    "        rec = {\n",
    "            \"ticker\": tkr, \"event_dt\": ev_dt, \"direction\": direction,\n",
    "            \"rev_mag_pct\": ev.get(\"rev_mag_pct\", np.nan),\n",
    "            \"new_target\": tgt, \"fair_value\": fair, \"news_sent\": s_now,\n",
    "            \"entry_date\": sim[\"entry_date\"], \"exit_date\": sim[\"exit_date\"],\n",
    "            \"pnl_pct\": sim[\"pnl_pct\"], \"exit_reason\": sim[\"exit_reason\"]\n",
    "        }\n",
    "        recs.append(rec)\n",
    "        time.sleep(0.2)\n",
    "    return pd.DataFrame(recs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e4b421",
   "metadata": {},
   "source": [
    "## 11) Reporting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222ea11",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def report_results(df_trades: pd.DataFrame, title=\"Analyst Fade Backtest\"):\n",
    "    if df_trades.empty:\n",
    "        print(\"No trades.\"); return\n",
    "    df = df_trades.copy().sort_values(\"entry_date\")\n",
    "    equity = (1.0 + df[\"pnl_pct\"]/100.0).cumprod()\n",
    "    stats = {\n",
    "        \"n_trades\": len(df),\n",
    "        \"avg_pnl_pct\": float(df[\"pnl_pct\"].mean()),\n",
    "        \"win_rate\": float((df[\"pnl_pct\"]>0).mean()),\n",
    "        \"total_return_pct\": float((equity.iloc[-1]-1.0)*100.0)\n",
    "    }\n",
    "    print(\"=== Summary ===\")\n",
    "    for k,v in stats.items():\n",
    "        print(f\"{k}: {v:.3f}\" if isinstance(v, float) else f\"{k}: {v}\")\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(equity.values)\n",
    "    plt.title(title); plt.xlabel(\"Trade #\"); plt.ylabel(\"Cumulative Growth (×)\"); plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9015d1",
   "metadata": {},
   "source": [
    "## 12) Run — choose events source (CSV or Finnhub), thresholds, backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b258e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_CSV_EVENTS      = True                          # set False to try Finnhub (if plan supports)\n",
    "CSV_EVENTS_PATH     = \"analyst_events_sample.csv\"   # supply your file\n",
    "TICKERS             = [\"AAPL\",\"MSFT\",\"NVDA\",\"AMZN\"] # used only for Finnhub demo\n",
    "\n",
    "thresholds = DisagreementThresholds(\n",
    "    target_over_fair_x = 1.2,\n",
    "    min_rev_mag_pct    = 5.0,\n",
    "    min_sent_div       = 0.05,\n",
    "    vwap_band          = 1.0\n",
    ")\n",
    "\n",
    "if USE_CSV_EVENTS:\n",
    "    events = load_analyst_events_csv(CSV_EVENTS_PATH)\n",
    "else:\n",
    "    frames = []\n",
    "    for t in TICKERS:\n",
    "        df = finnhub_price_targets(t, months_back=6)\n",
    "        if not df.empty:\n",
    "            df = df.rename(columns={\"dt\":\"event_dt\"}) if \"dt\" in df.columns else df\n",
    "            frames.append(df)\n",
    "        time.sleep(0.2)\n",
    "    events = pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "if events.empty:\n",
    "    print(\"No events loaded. Point CSV_EVENTS_PATH to your file.\")\n",
    "else:\n",
    "    events = build_revision_features(events)\n",
    "    print(events.head())\n",
    "    trades = backtest_events(events, thresholds, sector_hint=\"Technology\")\n",
    "    report_results(trades, title=\"Analyst Fade — Fundamentals & Sentiment-Filtered\")\n",
    "    trades.to_csv(\"analyst_fade_trades.csv\", index=False)\n",
    "    print(\"Saved analyst_fade_trades.csv\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}