{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28f41ef",
   "metadata": {},
   "source": [
    "# 10Q Sentiment & DCF Analysis\n",
    "This notebook analyzes the DCF of a 10Q as well as the sentiment of the writings within the report for a given ticker and predicts its future price movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514451e",
   "metadata": {},
   "source": [
    "### Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, time, math, json, warnings, requests\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "SEC_EMAIL = os.getenv(\"SEC_EMAIL\")\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "AV_KEY = os.getenv(\"ALPHAVANTAGE_API_KEY\")\n",
    "\n",
    "TRADIER_ACCESS_TOKEN = os.getenv(\"TRADIER_ACCESS_TOKEN\")\n",
    "TRADIER_ACCOUNT_ID = os.getenv(\"TRADIER_ACCOUNT_ID\")\n",
    "TRADIER_BASE = os.getenv(\"TRADIER_BASE\")\n",
    "\n",
    "def _tradier_headers():\n",
    "    if not TRADIER_ACCESS_TOKEN:\n",
    "        raise RuntimeError(\"Missing TRADIER_ACCESS_TOKEN in env.\")\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {TRADIER_ACCESS_TOKEN}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "FINBERT_ID = \"yiyanghkust/finbert-tone\"\n",
    "SECTION_PATTERNS = [\n",
    "    (r\"item\\s+2\\.\\s*management[â€™']?s discussion and analysis.*?(?=item\\s+3\\.)\", \"MD&A\"),\n",
    "    (r\"item\\s+1a\\.\\s*risk factors.*?(?=item\\s+2\\.)\", \"RiskFactors\"),\n",
    "    (r\"results of operations.*?(?=liquidity|capital resources|item\\s+\\d)\", \"Results\"),\n",
    "]\n",
    "POS_PHRASES = [r\"strong demand\", r\"margin expansion\", r\"raised guidance\", r\"record (revenue|earnings)\", r\"cost (reductions|optimization)\", r\"share repurchase\", r\"cash flow (improved|growth)\"]\n",
    "NEG_PHRASES = [r\"decline in (sales|revenue)\", r\"margin compression\", r\"impairment charge\", r\"supply chain disruption\", r\"adversely affected\", r\"weaker demand\", r\"material weakness\"]\n",
    "\n",
    "def SEC_HEADERS():\n",
    "    return {\n",
    "        \"User-Agent\": f\"CrowdQuant Research (contact: {SEC_EMAIL})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "\n",
    "def cap_bucket(mc):\n",
    "    if mc is None or not np.isfinite(mc): return \"unknown\"\n",
    "    mc_b = mc / 1e9\n",
    "    if mc_b < 0.3:  return \"micro\"\n",
    "    if mc_b < 2:    return \"small\"\n",
    "    if mc_b < 10:   return \"mid\"\n",
    "    if mc_b < 200:  return \"large\"\n",
    "    return \"mega\"\n",
    "\n",
    "BACKTEST_CSV = \"10q_sentiment_event_returns.csv\"\n",
    "BASELINES_JSON = \"sentiment_baselines.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec4208",
   "metadata": {},
   "source": [
    "### SEC + HTML Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "927bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    js = requests.get(url, headers=SEC_HEADERS(), timeout=30).json()\n",
    "    t = ticker.upper()\n",
    "    for _, rec in js.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "def list_10q_with_dates(cik: str, max_n=8):\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    rec = r.json().get(\"filings\",{}).get(\"recent\",{})\n",
    "    out = []\n",
    "    for form, acc, prim, fdate in zip(rec.get(\"form\",[]), rec.get(\"accessionNumber\",[]), rec.get(\"primaryDocument\",[]), rec.get(\"filingDate\",[])):\n",
    "        if form == \"10-Q\":\n",
    "            out.append({\"accession\": acc.replace(\"-\",\"\"), \"primary\": prim, \"filing_date\": fdate})\n",
    "        if len(out) >= max_n: break\n",
    "    return out\n",
    "\n",
    "def fetch_filing_html(cik:str, accession:str, primary:str) -> str:\n",
    "    base = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession}\"\n",
    "    url  = f\"{base}/{primary}\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=60); r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def _make_soup(html: str) -> BeautifulSoup:\n",
    "    for parser in (\"lxml\", \"html5lib\", \"html.parser\"):\n",
    "        try:\n",
    "            return BeautifulSoup(html, parser)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def _lower_clean(txt: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\",\" \", txt.lower())\n",
    "\n",
    "def extract_sections(html: str, patterns=SECTION_PATTERNS, fallback_full=True, cap=60000) -> dict:\n",
    "    soup = _make_soup(html)\n",
    "    txt  = soup.get_text(\"\\n\", strip=True)\n",
    "    low  = _lower_clean(txt)\n",
    "    out = {}\n",
    "    for pat, name in patterns:\n",
    "        m = re.search(pat, low, flags=re.S)\n",
    "        if m: out[name] = low[m.start():m.end()][:cap]\n",
    "    if not out and fallback_full: out[\"FullDocument\"] = low[:cap]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f27e7",
   "metadata": {},
   "source": [
    "### FinBERT Loader + Long-Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7bdf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_ID)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_ID)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, top_k=None, truncation=True)\n",
    "    return pipe, tok\n",
    "\n",
    "def _token_chunks(text: str, tokenizer, max_tokens=512, stride=32):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    step = max_tokens - stride\n",
    "    for i in range(0, len(ids), step):\n",
    "        window = ids[i:i+max_tokens]\n",
    "        if not window: break\n",
    "        yield tokenizer.decode(window, skip_special_tokens=True)\n",
    "\n",
    "def finbert_sent_long(text: str, pipe, tokenizer, max_tokens=512, batch=16):\n",
    "    if len(text) < 4000:\n",
    "        rows = pipe([text], truncation=True, max_length=max_tokens)\n",
    "    else:\n",
    "        chunks = list(_token_chunks(text, tokenizer, max_tokens=max_tokens))\n",
    "        rows = []\n",
    "        for i in range(0, len(chunks), batch):\n",
    "            rows.extend(pipe(chunks[i:i+batch], truncation=True, max_length=max_tokens))\n",
    "    pos = neu = neg = 0.0\n",
    "    for r in rows:\n",
    "        d = {x[\"label\"].lower(): x[\"score\"] for x in r}\n",
    "        pos += d.get(\"positive\",0.0); neu += d.get(\"neutral\",0.0); neg += d.get(\"negative\",0.0)\n",
    "    n = max(1, len(rows))\n",
    "    return {\"pos\":pos/n, \"neu\":neu/n, \"neg\":neg/n, \"sent_score\":pos/n - neg/n}\n",
    "\n",
    "def phrase_boost(text: str, pos_list=POS_PHRASES, neg_list=NEG_PHRASES, w=0.1) -> float:\n",
    "    boost = 0.0\n",
    "    for p in pos_list:\n",
    "        if re.search(p, text, flags=re.I): boost += w\n",
    "    for n in neg_list:\n",
    "        if re.search(n, text, flags=re.I): boost -= w\n",
    "    return boost\n",
    "\n",
    "def score_sections(sections: dict, pipe_tok=None) -> dict:\n",
    "    pipe, tok = pipe_tok if pipe_tok else load_finbert()\n",
    "    feats = {}\n",
    "    for name, text in sections.items():\n",
    "        fb = finbert_sent_long(text, pipe, tok, max_tokens=512, batch=16)\n",
    "        boost = phrase_boost(text)\n",
    "        feats[f\"{name}_pos\"] = fb[\"pos\"]; feats[f\"{name}_neg\"] = fb[\"neg\"]\n",
    "        base = fb.get(\"sent_score\", 0.0)\n",
    "        feats[f\"{name}_sent\"] = base + boost\n",
    "        feats[f\"{name}_boost\"] = boost\n",
    "    sents = [v for k,v in feats.items() if k.endswith(\"_sent\")]\n",
    "    feats[\"sent_overall\"] = float(np.mean(sents)) if sents else np.nan\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92490187",
   "metadata": {},
   "source": [
    "### Price Data from Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_daily(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\":\"true\",\"sort\":\"asc\",\"limit\":50000,\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    rows = r.json().get(\"results\", []) or []\n",
    "    if not rows: return pd.DataFrame(columns=[\"date\",\"close\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"c\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    return df.drop(columns=[\"t\"]).rename(columns={\"c\":\"close\"}).drop_duplicates(\"date\")\n",
    "\n",
    "def polygon_latest_close(ticker: str, lookback_days: int = 14):\n",
    "    \"\"\"Get the most recent daily close from Polygon within the last N days.\"\"\"\n",
    "    end = date.today()\n",
    "    start = end - timedelta(days=lookback_days)\n",
    "    df = polygon_daily(ticker, start.isoformat(), end.isoformat())\n",
    "    if df.empty: return None\n",
    "    return float(df.iloc[-1][\"close\"])\n",
    "\n",
    "def next_trading_close(df: pd.DataFrame, target_date: date):\n",
    "    s = df[df[\"date\"] >= target_date]\n",
    "    return None if s.empty else float(s.iloc[0][\"close\"])\n",
    "\n",
    "def event_closes(ticker: str, filing_date: str) -> dict:\n",
    "    d0 = datetime.strptime(filing_date, \"%Y-%m-%d\").date()\n",
    "    d7 = d0 + timedelta(days=7)\n",
    "    d28= d0 + timedelta(days=28)\n",
    "    start = (d0 - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    end   = (d28 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df.empty: return {\"close_0\":np.nan,\"close_7\":np.nan,\"close_28\":np.nan}\n",
    "    return {\"close_0\": next_trading_close(df,d0),\n",
    "            \"close_7\": next_trading_close(df,d7),\n",
    "            \"close_28\":next_trading_close(df,d28)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8e16b",
   "metadata": {},
   "source": [
    "### CSV Backtest Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cf01121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_10q_sentiment(tickers, max_filings=6, sleep_sec=0.3):\n",
    "    pipe_tok = load_finbert()\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        print(f\"[{t}] pulling 10-Qsâ€¦\")\n",
    "        cik = get_cik(t)\n",
    "        pairs = list_10q_with_dates(cik, max_n=max_filings)\n",
    "        for p in pairs:\n",
    "            try:\n",
    "                html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  skip {p['accession']} ({e})\"); continue\n",
    "            secs  = extract_sections(html)\n",
    "            feats = score_sections(secs, pipe_tok=pipe_tok)\n",
    "            px    = event_closes(t, p[\"filing_date\"])\n",
    "            row   = {\"ticker\":t,\"cik\":cik, **p, **feats, **px}\n",
    "            c0,c7,c28 = row[\"close_0\"], row[\"close_7\"], row[\"close_28\"]\n",
    "            row[\"ret_7\"]  = (c7/c0 - 1.0)*100 if c0 and c7 else np.nan\n",
    "            row[\"ret_28\"] = (c28/c0 - 1.0)*100 if c0 and c28 else np.nan\n",
    "            rows.append(row)\n",
    "            time.sleep(sleep_sec)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(BACKTEST_CSV, index=False)\n",
    "    print(f\"Saved {BACKTEST_CSV} with {len(df)} rows.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f5272",
   "metadata": {},
   "source": [
    "### Backtest Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67c51108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def run_legitimacy_checks(path=BACKTEST_CSV):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\"])\n",
    "    sent_cols = [c for c in df.columns if c.endswith(\"_sent\")]\n",
    "    def winsorize(s, p=0.01):\n",
    "        lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "        return s.clip(lo, hi)\n",
    "    df[\"ret_7_w\"]  = winsorize(df[\"ret_7\"])\n",
    "    df[\"ret_28_w\"] = winsorize(df[\"ret_28\"])\n",
    "\n",
    "    def corr_table(y_col):\n",
    "        rows=[]\n",
    "        for c in sent_cols + ([\"sent_overall\"] if \"sent_overall\" in df.columns else []):\n",
    "            x,y = df[c], df[y_col]\n",
    "            m = x.notna() & y.notna()\n",
    "            if m.sum() < 8: rows.append((c, np.nan, np.nan)); continue\n",
    "            r,p = pearsonr(x[m], y[m])\n",
    "            rows.append((c,r,p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    corr7, corr28 = corr_table(\"ret_7_w\"), corr_table(\"ret_28_w\")\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Î”sent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    def corr_delta(y_col):\n",
    "        rows=[]\n",
    "        x,y = df[\"Î”sent_overall\"], df[y_col]\n",
    "        m = x.notna() & y.notna()\n",
    "        if m.sum() >= 8:\n",
    "            r,p = pearsonr(x[m], y[m]); rows.append((\"Î”sent_overall\", r, p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    dc7, dc28 = corr_delta(\"ret_7_w\"), corr_delta(\"ret_28_w\")\n",
    "    return {\"corr7\":corr7, \"corr28\":corr28, \"dc7\":dc7, \"dc28\":dc28}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3eb0ac",
   "metadata": {},
   "source": [
    "### Market Cap Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7ade05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_market_cap(ticker: str):\n",
    "    url = f\"https://api.polygon.io/v3/reference/tickers/{ticker.upper()}\"\n",
    "    params = {\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=20)\n",
    "    try:\n",
    "        js = r.json().get(\"results\", {})\n",
    "        return js.get(\"market_cap\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fit_bucket_baselines(path=BACKTEST_CSV, min_per_bucket=8):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\",\"sent_overall\"])\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Î”sent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    # attach current market cap + bucket\n",
    "    mcap = {}\n",
    "    for t in df[\"ticker\"].unique():\n",
    "        mcap[t] = polygon_market_cap(t)\n",
    "        time.sleep(0.2)\n",
    "    df[\"market_cap\"] = df[\"ticker\"].map(mcap)\n",
    "    df[\"bucket\"] = df[\"market_cap\"].map(cap_bucket)\n",
    "\n",
    "    out = {}\n",
    "    for b in df[\"bucket\"].dropna().unique():\n",
    "        d = df[df[\"bucket\"]==b].dropna(subset=[\"Î”sent_overall\",\"ret_7\",\"ret_28\"])\n",
    "        if len(d) < min_per_bucket: continue\n",
    "        X = np.c_[np.ones(len(d)), d[\"Î”sent_overall\"].values]\n",
    "        for horizon, ycol in ((\"ret7\",\"ret_7\"), (\"ret28\",\"ret_28\")):\n",
    "            y = d[ycol].values\n",
    "            beta = np.linalg.pinv(X).dot(y)  # [a,b]\n",
    "            a,b = float(beta[0]), float(beta[1])\n",
    "            out.setdefault(b, {})[horizon] = {\"intercept\":a, \"slope\":b, \"n\":int(len(d))}\n",
    "    with open(BASELINES_JSON,\"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved {BASELINES_JSON}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9a81b",
   "metadata": {},
   "source": [
    "### Fundamentals + DCF Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H) FUNDAMENTALS + DCF SNAPSHOT (TTM from SEC facts)\n",
    "\n",
    "def get_company_facts(cik: str) -> dict:\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _ttm_sum(items, n=4):\n",
    "    if not items: return None\n",
    "    vals = [x.get(\"val\") for x in items][-n:]\n",
    "    vals = [v for v in vals if v is not None]\n",
    "    return float(np.nansum(vals)) if vals else None\n",
    "\n",
    "def build_ttm_metrics(facts: dict) -> dict:\n",
    "    usgaap = facts.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "    def get_series(tag):\n",
    "        return (usgaap.get(tag, {}).get(\"units\", {}).get(\"USD\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"USD/shares\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"shares\", []))\n",
    "    revenue_q   = get_series(\"Revenues\")\n",
    "    ni_q        = get_series(\"NetIncomeLoss\")\n",
    "    eps_q       = (usgaap.get(\"EarningsPerShareDiluted\", {}).get(\"units\", {}).get(\"USD/shares\", []) or [])\n",
    "    dil_sh_q    = get_series(\"WeightedAverageNumberOfDilutedSharesOutstanding\")\n",
    "    cfo_q       = get_series(\"NetCashProvidedByUsedInOperatingActivities\")\n",
    "    capex_q     = get_series(\"PaymentsToAcquirePropertyPlantAndEquipment\")\n",
    "\n",
    "    revenue_ttm     = _ttm_sum(revenue_q)\n",
    "    net_income_ttm  = _ttm_sum(ni_q)\n",
    "    eps_ttm         = _ttm_sum(eps_q)\n",
    "    diluted_sh_ttm  = _ttm_sum(dil_sh_q)\n",
    "    cfo_ttm         = _ttm_sum(cfo_q)\n",
    "    capex_ttm       = _ttm_sum(capex_q)\n",
    "    fcf_ttm         = (cfo_ttm or 0.0) - abs(capex_ttm or 0.0)\n",
    "\n",
    "    if eps_ttm and diluted_sh_ttm and net_income_ttm:\n",
    "        approx = eps_ttm * diluted_sh_ttm\n",
    "        if abs(approx - net_income_ttm)/max(1.0, net_income_ttm) > 0.15:\n",
    "            print(\"[warn] EPS*Shares != NetIncome by >15%. Check tags/periods.\")\n",
    "\n",
    "    return dict(\n",
    "        revenue_ttm=revenue_ttm, net_income_ttm=net_income_ttm,\n",
    "        eps_diluted_ttm=eps_ttm, diluted_shares_ttm=diluted_sh_ttm,\n",
    "        cfo_ttm=cfo_ttm, capex_ttm=capex_ttm, fcf_ttm=fcf_ttm,\n",
    "        rev_per_share=(revenue_ttm / diluted_sh_ttm) if (revenue_ttm and diluted_sh_ttm) else None,\n",
    "    )\n",
    "\n",
    "SECTOR_MULTIPLES = {\"Technology\":{\"PE\":30.0,\"PS\":6.8}, \"_default\":{\"PE\":18.0,\"PS\":2.5}}\n",
    "\n",
    "def multiples_anchor(metrics:dict, sector=\"Technology\"):\n",
    "    cfg = SECTOR_MULTIPLES.get(sector, SECTOR_MULTIPLES[\"_default\"])\n",
    "    eps = metrics.get(\"eps_diluted_ttm\")\n",
    "    rps = metrics.get(\"rev_per_share\")\n",
    "    pe_anchor = eps * cfg[\"PE\"] if eps else None\n",
    "    ps_anchor = rps * cfg[\"PS\"] if rps else None\n",
    "    anchors = [x for x in (pe_anchor, ps_anchor) if x is not None and math.isfinite(x)]\n",
    "    mid = float(np.mean(anchors)) if anchors else None\n",
    "    return {\"pe_anchor\":pe_anchor, \"ps_anchor\":ps_anchor, \"fair_value_mid\":mid, \"assumptions\":cfg}\n",
    "\n",
    "def dcf_anchor(metrics:dict, years=5, g=0.04, r=0.095, g_term=0.02):\n",
    "    fcf0 = metrics.get(\"fcf_ttm\")\n",
    "    sh   = metrics.get(\"diluted_shares_ttm\")\n",
    "    if not fcf0 or not sh or sh <= 0: return None\n",
    "    pv, fcf = 0.0, fcf0\n",
    "    for t in range(1, years+1):\n",
    "        fcf *= (1+g)\n",
    "        pv  += fcf / ((1+r)**t)\n",
    "    terminal = (fcf * (1+g_term)) / (r - g_term)\n",
    "    pv_term  = terminal / ((1+r)**years)\n",
    "    return (pv + pv_term) / sh\n",
    "\n",
    "def blended_fair_value(mult_mid, dcf_val, w=0.5):\n",
    "    if mult_mid is None and dcf_val is None: return None\n",
    "    if mult_mid is None: return dcf_val\n",
    "    if dcf_val  is None: return mult_mid\n",
    "    return w*mult_mid + (1-w)*dcf_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ab4ed",
   "metadata": {},
   "source": [
    "### Single Ticker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8176f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_two_10q_delta_sent(ticker: str):\n",
    "    cik   = get_cik(ticker)\n",
    "    pairs = list_10q_with_dates(cik, max_n=2)\n",
    "    if not pairs: return np.nan, np.nan, np.nan\n",
    "    pipe_tok = load_finbert()\n",
    "    scores = []\n",
    "    for p in pairs:\n",
    "        html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "        secs = extract_sections(html)\n",
    "        feats= score_sections(secs, pipe_tok=pipe_tok)\n",
    "        scores.append(feats[\"sent_overall\"])\n",
    "        time.sleep(0.3)\n",
    "    if len(scores)==1: return scores[0], np.nan, np.nan\n",
    "    return scores[0], scores[1], scores[0]-scores[1]\n",
    "\n",
    "def load_baselines(path=BASELINES_JSON):\n",
    "    if not os.path.exists(path): raise FileNotFoundError(\"Run cell G to fit/save sentiment baselines first.\")\n",
    "    with open(path,\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def predict_ticker(ticker: str, w_short=0.30, sector=\"Technology\"):\n",
    "    sent_now, sent_prev, d_sent = latest_two_10q_delta_sent(ticker)\n",
    "    mc   = polygon_market_cap(ticker)\n",
    "    bkt  = cap_bucket(mc)\n",
    "    base = load_baselines().get(bkt, {})\n",
    "    a7,b7   = base.get(\"ret7\",{}).get(\"intercept\",0.0),  base.get(\"ret7\",{}).get(\"slope\",0.0)\n",
    "    a28,b28 = base.get(\"ret28\",{}).get(\"intercept\",0.0), base.get(\"ret28\",{}).get(\"slope\",0.0)\n",
    "    pred7   = a7  + b7  * (d_sent if np.isfinite(d_sent) else 0.0)\n",
    "    pred28  = a28 + b28 * (d_sent if np.isfinite(d_sent) else 0.0)\n",
    "\n",
    "    # Valuation\n",
    "    cik   = get_cik(ticker)\n",
    "    facts = get_company_facts(cik)\n",
    "    gaap  = build_ttm_metrics(facts)\n",
    "    mult  = multiples_anchor(gaap, sector=sector)\n",
    "    dcfv  = dcf_anchor(gaap, years=5, g=0.04, r=0.095, g_term=0.02)\n",
    "    blend = blended_fair_value(mult.get(\"fair_value_mid\"), dcfv, w=0.5)\n",
    "\n",
    "    # Price now from Polygon only\n",
    "    pnow = polygon_latest_close(ticker)\n",
    "    price_28d = pnow*(1+pred28/100) if pnow and np.isfinite(pred28) else None\n",
    "    final_target = (w_short*price_28d + (1-w_short)*blend) if (price_28d and blend) else (price_28d or blend)\n",
    "    direction = \"UP\" if (final_target and pnow and final_target > pnow) else \"DOWN\"\n",
    "\n",
    "    return {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"market_cap\": mc, \"cap_bucket\": bkt,\n",
    "        \"sent_overall_now\": sent_now, \"sent_overall_prev\": sent_prev, \"Î”sent_overall\": d_sent,\n",
    "        \"pred_ret_7d_pct\": pred7, \"pred_ret_28d_pct\": pred28,\n",
    "        \"price_now\": pnow, \"price_28d_target\": price_28d,\n",
    "        \"dcf_fair_value\": dcfv, \"multiples_mid\": mult.get(\"fair_value_mid\"),\n",
    "        \"final_blended_target\": final_target, \"final_direction\": direction,\n",
    "        \"baseline_used\": base\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47056749",
   "metadata": {},
   "source": [
    "### Options Spread Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9af39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradier_expirations(ticker: str):\n",
    "    \"\"\"List available expiration dates (YYYY-MM-DD) for a symbol.\"\"\"\n",
    "    url = f\"{TRADIER_BASE}/markets/options/expirations\"\n",
    "    r = requests.get(url, headers=_tradier_headers(),\n",
    "                     params={\"symbol\": ticker.upper(), \"includeAll\": \"false\"},\n",
    "                     timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json().get(\"expirations\", {})\n",
    "    exps = data.get(\"date\", [])\n",
    "    # API returns a string or list depending on count\n",
    "    return exps if isinstance(exps, list) else ([exps] if exps else [])\n",
    "\n",
    "def tradier_chain(ticker: str, expiration: str):\n",
    "    \"\"\"Get the option chain for a given expiration.\"\"\"\n",
    "    url = f\"{TRADIER_BASE}/markets/options/chains\"\n",
    "    r = requests.get(url, headers=_tradier_headers(),\n",
    "                     params={\"symbol\": ticker.upper(), \"expiration\": expiration, \"greeks\": \"false\"},\n",
    "                     timeout=30)\n",
    "    r.raise_for_status()\n",
    "    opt = r.json().get(\"options\", {}).get(\"option\", [])\n",
    "    return opt if isinstance(opt, list) else ([opt] if opt else [])\n",
    "\n",
    "def best_vertical_by_target(ticker: str, pred_pct: float, polygon_price_fn, horizon_days=28):\n",
    "    \"\"\"\n",
    "    If pred_pct > 0 => bull call vertical. If < 0 => bear put vertical.\n",
    "    'polygon_price_fn' should be a callable like polygon_latest_close(ticker)->float.\n",
    "    \"\"\"\n",
    "    # pick an expiration ~horizon_days out (nearest available)\n",
    "    today = date.today()\n",
    "    target = today + timedelta(days=horizon_days)\n",
    "    exps = tradier_expirations(ticker)\n",
    "    if not exps:\n",
    "        return None\n",
    "    # choose the expiration closest to 'target' that is >= today\n",
    "    def to_date(s): \n",
    "        y,m,d = map(int, s.split(\"-\")); \n",
    "        return date(y,m,d)\n",
    "    future_exps = [e for e in exps if to_date(e) >= today]\n",
    "    if not future_exps:\n",
    "        return None\n",
    "    exp = min(future_exps, key=lambda s: abs(to_date(s)-target))\n",
    "\n",
    "    chain = tradier_chain(ticker, exp)\n",
    "    if not chain:\n",
    "        return None\n",
    "\n",
    "    pnow = polygon_price_fn(ticker)\n",
    "    if not pnow:\n",
    "        return None\n",
    "    p_tgt = pnow * (1 + pred_pct/100.0)\n",
    "\n",
    "    # strikes universe\n",
    "    strikes = sorted({float(o[\"strike\"]) for o in chain if \"strike\" in o})\n",
    "    if not strikes:\n",
    "        return None\n",
    "    nearest = lambda x: min(strikes, key=lambda k: abs(k - x))\n",
    "\n",
    "    if pred_pct >= 0:\n",
    "        k_buy  = nearest(pnow * 0.99)\n",
    "        k_sell = nearest(p_tgt * 1.02)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bull_call\"\n",
    "    else:\n",
    "        k_buy  = nearest(pnow * 1.01)\n",
    "        k_sell = nearest(p_tgt * 0.98)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bear_put\"\n",
    "\n",
    "    if not leg_buy or not leg_sell:\n",
    "        return None\n",
    "\n",
    "    def mid(q):\n",
    "        b = float(q.get(\"bid\", 0.0)); a = float(q.get(\"ask\", 0.0))\n",
    "        return (b + a)/2 if (a and b) else float(q.get(\"last\", 0.0))\n",
    "\n",
    "    debit = max(0.01, mid(leg_buy[0]) - mid(leg_sell[0]))\n",
    "    width = abs(k_sell - k_buy)\n",
    "    max_profit = max(0.0, width - debit)\n",
    "    rr = (max_profit / debit) if debit > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"type\": spread_type,\n",
    "        \"expiration\": exp,\n",
    "        \"buy_strike\": float(k_buy),\n",
    "        \"sell_strike\": float(k_sell),\n",
    "        \"debit\": round(debit, 2),\n",
    "        \"width\": float(width),\n",
    "        \"max_profit\": round(max_profit, 2),\n",
    "        \"R_by_Risk\": round(rr, 2) if rr else None,\n",
    "        \"price_now\": float(pnow),\n",
    "        \"price_target\": float(p_tgt),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd55847",
   "metadata": {},
   "source": [
    "### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7a2bcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_baseline():\n",
    "    \"\"\"\n",
    "    1. Builds backtest CSV across a mixed-cap universe.\n",
    "    2. Runs legitimacy checks (optional).\n",
    "    3. Fits per-cap baselines and saves to sentiment_baselines.json.\n",
    "    \"\"\"\n",
    "    micro = \"LUNA, GCTK, VTSI, HCAT, CLXT, OPRX, FUV, BGFV, CRTX, AOUT, FCEL, HITI, AWH, WKSP, GRIN, TFFP, HZO, OPTN, TIRX, STRC\".split(\",\")\n",
    "    small = \"BLKB, HQY, PIPR, HAYW, NVCR, SMPL, MGPI, BE, PRCT, SKYW, AVAV, INMD, VRTS, CNXN, REZI, ASTE, MHO, CELH, ABM, PCT\".split(\",\")\n",
    "    mid   = \"LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB, DAL, CHRW, WDC, AAP, CZR, CHD, SWKS, COHR, PTC, HOLX, MKTX\".split(\",\")\n",
    "    large = \"AAPL, MSFT, AMZN, NVDA, GOOGL, META, JPM, V, JNJ, PG, XOM, UNH, PEP, KO, COST, ORCL, DIS, HD, BAC, WMT\".split(\",\")\n",
    "    TICKERS = [x.strip() for s in (micro+small+mid+large) for x in [s] if x.strip()]\n",
    "    df = backtest_10q_sentiment(TICKERS, max_filings=6)\n",
    "    print(df.head())\n",
    "    _ = run_legitimacy_checks(BACKTEST_CSV)\n",
    "    baselines = fit_bucket_baselines(BACKTEST_CSV)\n",
    "    return baselines\n",
    "\n",
    "def run_predict(tickers, w_short=0.30, sector=\"Technology\", suggest_spread=True):\n",
    "    \"\"\"\n",
    "    1. Loads latest Î”-sentiment for each ticker and applies cap-bucket baselines.\n",
    "    2. Computes DCF + multiples blend and final blended target.\n",
    "    3. Optionally suggests a debit vertical via Tradier.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for t in tickers:\n",
    "        res = predict_ticker(t, w_short=w_short, sector=sector)\n",
    "        if suggest_spread and res.get(\"pred_ret_28d_pct\") is not None:\n",
    "            res[\"options_vertical\"] = best_vertical_by_target(t, res[\"pred_ret_28d_pct\"])\n",
    "        out.append(res)\n",
    "    return pd.DataFrame(out)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
