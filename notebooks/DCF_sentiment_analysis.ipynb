{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28f41ef",
   "metadata": {},
   "source": [
    "# 10Q Sentiment & DCF Analysis\n",
    "This notebook analyzes the DCF of a 10Q as well as the sentiment of the writings within the report for a given ticker and predicts its future price movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514451e",
   "metadata": {},
   "source": [
    "### Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "9dbd2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, time, math, json, warnings, requests\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "from functools import lru_cache\n",
    "import requests, time\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from pathlib import Path\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "log = logging.getLogger(\"10Q-DCF\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "SEC_EMAIL = os.getenv(\"SEC_EMAIL\")\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "TRADIER_ACCESS_TOKEN = os.getenv(\"TRADIER_ACCESS_TOKEN\")\n",
    "TRADIER_ACCOUNT_ID = os.getenv(\"TRADIER_ACCOUNT_ID\")\n",
    "TRADIER_BASE = os.getenv(\"TRADIER_BASE\")\n",
    "\n",
    "def _tradier_headers():\n",
    "    # Ensure TRADIER_ACCESS_TOKEN and TRADIER_BASE are set in .env\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {TRADIER_ACCESS_TOKEN}\",\n",
    "        \"Accept\": \"application/json\"\n",
    "    }\n",
    "\n",
    "FINBERT_ID = \"yiyanghkust/finbert-tone\"\n",
    "SECTION_PATTERNS = [\n",
    "    (r\"item\\s+2\\.\\s*management[â€™']?s discussion and analysis.*?(?=item\\s+3\\.)\", \"MD&A\"),\n",
    "    (r\"item\\s+1a\\.\\s*risk factors.*?(?=item\\s+2\\.)\", \"RiskFactors\"),\n",
    "    (r\"results of operations.*?(?=liquidity|capital resources|item\\s+\\d)\", \"Results\"),\n",
    "]\n",
    "POS_PHRASES = [r\"strong demand\", r\"margin expansion\", r\"raised guidance\", r\"record (revenue|earnings)\", r\"cost (reductions|optimization)\", r\"share repurchase\", r\"cash flow (improved|growth)\"]\n",
    "NEG_PHRASES = [r\"decline in (sales|revenue)\", r\"margin compression\", r\"impairment charge\", r\"supply chain disruption\", r\"adversely affected\", r\"weaker demand\", r\"material weakness\"]\n",
    "\n",
    "SEC_HEADERS = {\n",
    "    \"User-Agent\": \"severin.spagnola@sjsu.edu\",\n",
    "    \"Accept\": \"application/json\",\n",
    "}\n",
    "\n",
    "_SEC_TICKER_CACHE = None  # lazy-loaded\n",
    "\n",
    "def cap_bucket(mc):\n",
    "    if mc is None or not np.isfinite(mc): return \"unknown\"\n",
    "    mc_b = mc / 1e9\n",
    "    if mc_b < 0.3:  return \"micro\"\n",
    "    if mc_b < 2:    return \"small\"\n",
    "    if mc_b < 10:   return \"mid\"\n",
    "    if mc_b < 200:  return \"large\"\n",
    "    return \"mega\"\n",
    "\n",
    "BACKTEST_CSV = \"10q_sentiment_event_returns.csv\"\n",
    "BASELINES_JSON = \"sentiment_baselines.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec4208",
   "metadata": {},
   "source": [
    "### SEC + HTML Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "927bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _zero_pad_cik(x: str) -> str:\n",
    "    \"\"\"Return 10-digit zero-padded CIK from a numeric string.\"\"\"\n",
    "    return f\"{int(x):010d}\"\n",
    "\n",
    "def _load_ticker_table() -> dict:\n",
    "    \"\"\"\n",
    "    Load the official SEC ticker->CIK table once:\n",
    "    https://www.sec.gov/files/company_tickers.json\n",
    "    Returns dict: { 'AAPL': '0000320193', ... }\n",
    "    \"\"\"\n",
    "    global _SEC_TICKER_CACHE\n",
    "    if _SEC_TICKER_CACHE is not None:\n",
    "        return _SEC_TICKER_CACHE\n",
    "\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    raw = r.json()  # { \"0\": {\"ticker\":\"A\",\"cik_str\":47217,\"title\":\"Agilent Technologies, Inc.\"}, ... }\n",
    "\n",
    "    table = {}\n",
    "    for _, row in raw.items():\n",
    "        table[str(row[\"ticker\"]).upper()] = _zero_pad_cik(str(row[\"cik_str\"]))\n",
    "    _SEC_TICKER_CACHE = table\n",
    "    return table\n",
    "\n",
    "def normalize_cik(identifier: str) -> str:\n",
    "    \"\"\"\n",
    "    Accepts a ticker like 'WHR', a bare CIK '106640', or strings like 'CIK0000106640'\n",
    "    Returns a 10-digit, zero-padded CIK string, e.g. '0000106640'\n",
    "    \"\"\"\n",
    "    s = str(identifier).strip().upper()\n",
    "\n",
    "    # If they passed 'CIK....', strip the prefix\n",
    "    if s.startswith(\"CIK\"):\n",
    "        s = s[3:].strip()\n",
    "\n",
    "    # Pure digits -> pad\n",
    "    if re.fullmatch(r\"\\d+\", s):\n",
    "        return _zero_pad_cik(s)\n",
    "\n",
    "    # Otherwise treat as ticker\n",
    "    tickers = _load_ticker_table()\n",
    "    if s in tickers:\n",
    "        return tickers[s]\n",
    "\n",
    "    raise ValueError(f\"Could not resolve identifier to CIK: {identifier}\")\n",
    "\n",
    "\n",
    "def sec_get_submissions(ticker_or_cik: str) -> dict:\n",
    "    cik10 = normalize_cik(ticker_or_cik)\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik10}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    if r.status_code == 429:\n",
    "        time.sleep(1.2)  # gentle backoff if rate-limited\n",
    "        r = requests.get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def list_10q_with_dates(ticker_or_cik: str, max_n: int | None = None):\n",
    "    \"\"\"\n",
    "    Return recent 10-Q metadata as a list of dicts:\n",
    "    [{ 'filing_date', 'accession', 'primary', 'url', 'cik' }, ...] (newest first)\n",
    "    \"\"\"\n",
    "    sub = sec_submissions_json(ticker_or_cik)\n",
    "    forms = sub[\"filings\"][\"recent\"][\"form\"]\n",
    "    accs  = sub[\"filings\"][\"recent\"][\"accessionNumber\"]\n",
    "    dates = sub[\"filings\"][\"recent\"][\"filingDate\"]\n",
    "    prims = sub[\"filings\"][\"recent\"].get(\"primaryDocument\", [\"\"]*len(forms))\n",
    "\n",
    "    cik_int = int(sub[\"cik\"])\n",
    "    rows = []\n",
    "    for f, a, d, p in zip(forms, accs, dates, prims):\n",
    "        if f == \"10-Q\":\n",
    "            acc_nodash = a.replace(\"-\", \"\")\n",
    "            primary = p or \"index.html\"\n",
    "            url = f\"https://www.sec.gov/Archives/edgar/data/{cik_int}/{acc_nodash}/{primary}\"\n",
    "            rows.append({\n",
    "                \"filing_date\": d,\n",
    "                \"accession\": a,\n",
    "                \"primary\": primary,\n",
    "                \"url\": url,\n",
    "                \"cik\": cik_int,\n",
    "            })\n",
    "    # newest first\n",
    "    rows = sorted(rows, key=lambda r: r[\"filing_date\"], reverse=True)\n",
    "    return rows[:max_n] if max_n else rows\n",
    "\n",
    "def fetch_filing_html(*args) -> str:\n",
    "    \"\"\"\n",
    "    Accepts either:\n",
    "      (cik, accession, primary)  -> builds the URL\n",
    "      (url,)                     -> uses the given URL directly\n",
    "    \"\"\"\n",
    "    if len(args) == 1:\n",
    "        url = args[0]\n",
    "    elif len(args) == 3:\n",
    "        cik, accession, primary = args\n",
    "        acc_nodash = str(accession).replace(\"-\", \"\")\n",
    "        url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{acc_nodash}/{primary}\"\n",
    "    else:\n",
    "        raise TypeError(\"fetch_filing_html expects (url) or (cik, accession, primary)\")\n",
    "    r = requests.get(url, headers=SEC_HEADERS, timeout=60)\n",
    "    r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def _make_soup(html: str) -> BeautifulSoup:\n",
    "    for parser in (\"lxml\", \"html5lib\", \"html.parser\"):\n",
    "        try:\n",
    "            return BeautifulSoup(html, parser)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def _lower_clean(txt: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\",\" \", txt.lower())\n",
    "\n",
    "def extract_sections(html: str, patterns=SECTION_PATTERNS, fallback_full=True, cap=60000) -> dict:\n",
    "    soup = _make_soup(html)\n",
    "    txt  = soup.get_text(\"\\n\", strip=True)\n",
    "    low  = _lower_clean(txt)\n",
    "    out = {}\n",
    "    for pat, name in patterns:\n",
    "        m = re.search(pat, low, flags=re.S)\n",
    "        if m: out[name] = low[m.start():m.end()][:cap]\n",
    "    if not out and fallback_full: out[\"FullDocument\"] = low[:cap]\n",
    "    return out\n",
    "\n",
    "def _session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(total=3, backoff_factor=0.5,\n",
    "                    status_forcelist=(429,500,502,503,504),\n",
    "                    allowed_methods=frozenset([\"GET\"]))\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def _ticker_to_cik_map():\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    r = _session().get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "    out = {}\n",
    "    for _, row in data.items():\n",
    "        out[row[\"ticker\"].upper()] = int(row[\"cik_str\"])\n",
    "    return out\n",
    "\n",
    "def get_cik(ticker_or_cik):\n",
    "    # accepts \"AAPL\" or 320193; returns int CIK\n",
    "    if isinstance(ticker_or_cik, str) and not ticker_or_cik.isdigit():\n",
    "        t = ticker_or_cik.upper()\n",
    "        if t not in _ticker_to_cik_map():\n",
    "            raise ValueError(f\"Unknown ticker for SEC map: {t}\")\n",
    "        return _ticker_to_cik_map()[t]\n",
    "    return int(ticker_or_cik)\n",
    "\n",
    "def _cik10(cik_int: int) -> str:\n",
    "    return f\"{int(cik_int):010d}\"\n",
    "\n",
    "def sec_submissions_url(ticker_or_cik) -> str:\n",
    "    # ALWAYS builds the correct zero-padded URL\n",
    "    cik = get_cik(ticker_or_cik)\n",
    "    return f\"https://data.sec.gov/submissions/CIK{_cik10(cik)}.json\"\n",
    "\n",
    "def sec_submissions_json(ticker_or_cik):\n",
    "    url = sec_submissions_url(ticker_or_cik)\n",
    "    r = _session().get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    time.sleep(0.2)  # be polite\n",
    "    return r.json()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f27e7",
   "metadata": {},
   "source": [
    "### FinBERT Loader + Long-Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "e7bdf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_ID)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_ID)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, top_k=None, truncation=True)\n",
    "    return pipe, tok\n",
    "\n",
    "def _token_chunks(text: str, tokenizer, max_tokens=512, stride=32):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    step = max_tokens - stride\n",
    "    for i in range(0, len(ids), step):\n",
    "        window = ids[i:i+max_tokens]\n",
    "        if not window: break\n",
    "        yield tokenizer.decode(window, skip_special_tokens=True)\n",
    "\n",
    "def finbert_sent_long(text: str, pipe, tokenizer, max_tokens=512, batch=16):\n",
    "    if len(text) < 4000:\n",
    "        rows = pipe([text], truncation=True, max_length=max_tokens)\n",
    "    else:\n",
    "        chunks = list(_token_chunks(text, tokenizer, max_tokens=max_tokens))\n",
    "        rows = []\n",
    "        for i in range(0, len(chunks), batch):\n",
    "            rows.extend(pipe(chunks[i:i+batch], truncation=True, max_length=max_tokens))\n",
    "    pos = neu = neg = 0.0\n",
    "    for r in rows:\n",
    "        d = {x[\"label\"].lower(): x[\"score\"] for x in r}\n",
    "        pos += d.get(\"positive\",0.0); neu += d.get(\"neutral\",0.0); neg += d.get(\"negative\",0.0)\n",
    "    n = max(1, len(rows))\n",
    "    return {\"pos\":pos/n, \"neu\":neu/n, \"neg\":neg/n, \"sent_score\":pos/n - neg/n}\n",
    "\n",
    "def phrase_boost(text: str, pos_list=POS_PHRASES, neg_list=NEG_PHRASES, w=0.1) -> float:\n",
    "    boost = 0.0\n",
    "    for p in pos_list:\n",
    "        if re.search(p, text, flags=re.I): boost += w\n",
    "    for n in neg_list:\n",
    "        if re.search(n, text, flags=re.I): boost -= w\n",
    "    return boost\n",
    "\n",
    "def score_sections(sections: dict, pipe_tok=None) -> dict:\n",
    "    pipe, tok = pipe_tok if pipe_tok else load_finbert()\n",
    "    feats = {}\n",
    "    for name, text in sections.items():\n",
    "        fb = finbert_sent_long(text, pipe, tok, max_tokens=512, batch=16)\n",
    "        boost = phrase_boost(text)\n",
    "        feats[f\"{name}_pos\"] = fb[\"pos\"]; feats[f\"{name}_neg\"] = fb[\"neg\"]\n",
    "        base = fb.get(\"sent_score\", 0.0)\n",
    "        feats[f\"{name}_sent\"] = base + boost\n",
    "        feats[f\"{name}_boost\"] = boost\n",
    "    sents = [v for k,v in feats.items() if k.endswith(\"_sent\")]\n",
    "    feats[\"sent_overall\"] = float(np.mean(sents)) if sents else np.nan\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92490187",
   "metadata": {},
   "source": [
    "### Price Data from Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "1a7a2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_daily(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\":\"true\",\"sort\":\"asc\",\"limit\":50000,\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    rows = r.json().get(\"results\", []) or []\n",
    "    if not rows: return pd.DataFrame(columns=[\"date\",\"close\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"c\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    return df.drop(columns=[\"t\"]).rename(columns={\"c\":\"close\"}).drop_duplicates(\"date\")\n",
    "\n",
    "def polygon_latest_close(ticker: str, lookback_days: int = 14):\n",
    "    \"\"\"Get the most recent daily close from Polygon within the last N days.\"\"\"\n",
    "    end = date.today()\n",
    "    start = end - timedelta(days=lookback_days)\n",
    "    df = polygon_daily(ticker, start.isoformat(), end.isoformat())\n",
    "    if df.empty: return None\n",
    "    return float(df.iloc[-1][\"close\"])\n",
    "\n",
    "def next_trading_close(df: pd.DataFrame, target_date: date):\n",
    "    s = df[df[\"date\"] >= target_date]\n",
    "    return None if s.empty else float(s.iloc[0][\"close\"])\n",
    "\n",
    "def event_closes(ticker: str, filing_date: str) -> dict:\n",
    "    d0 = datetime.strptime(filing_date, \"%Y-%m-%d\").date()\n",
    "    d7 = d0 + timedelta(days=7)\n",
    "    d28= d0 + timedelta(days=28)\n",
    "    start = (d0 - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    end   = (d28 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df.empty: return {\"close_0\":np.nan,\"close_7\":np.nan,\"close_28\":np.nan}\n",
    "    return {\"close_0\": next_trading_close(df,d0),\n",
    "            \"close_7\": next_trading_close(df,d7),\n",
    "            \"close_28\":next_trading_close(df,d28)}\n",
    "\n",
    "# === D2) Annualized volatility from Polygon daily closes ===\n",
    "def annualized_vol_from_polygon(ticker: str, lookback_days: int = 252, winsor_pct: float = 0.0):\n",
    "    \"\"\"\n",
    "    Compute annualized volatility from daily close-to-close returns.\n",
    "    - Pulls ~1.6x lookback to survive holidays/missing days, then trims.\n",
    "    - Optional winsorization of returns to reduce single-day outlier impact.\n",
    "    Returns: float (annualized vol) or None if insufficient data.\n",
    "    \"\"\"\n",
    "    # fetch a bit more than needed to handle gaps\n",
    "    start = (date.today() - timedelta(days=int(lookback_days * 1.6))).isoformat()\n",
    "    end   = date.today().isoformat()\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df is None or df.empty or \"close\" not in df.columns:\n",
    "        return None\n",
    "\n",
    "    # ensure chronological order and trim to last N trading rows\n",
    "    df = df.sort_values(\"date\").tail(lookback_days + 5)  # keep a small cushion\n",
    "    px = df[\"close\"].astype(float)\n",
    "    rets = px.pct_change().dropna()\n",
    "    if rets.empty:\n",
    "        return None\n",
    "\n",
    "    # optional winsorization of returns\n",
    "    if winsor_pct and 0 < winsor_pct < 0.5:\n",
    "        lo, hi = rets.quantile(winsor_pct), rets.quantile(1 - winsor_pct)\n",
    "        rets = rets.clip(lo, hi)\n",
    "\n",
    "    daily_std = float(rets.std(ddof=0))\n",
    "    ann_vol = daily_std * (252 ** 0.5)\n",
    "    return ann_vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8e16b",
   "metadata": {},
   "source": [
    "### CSV Backtest Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "0cf01121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_10q_sentiment(tickers, max_filings=6, sleep_sec=0.3):\n",
    "    pipe_tok = load_finbert()\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        print(f\"[{t}] pulling 10-Qsâ€¦\")\n",
    "        cik = get_cik(t)\n",
    "        pairs = list_10q_with_dates(cik, max_n=max_filings)  # list of dicts\n",
    "        for p in pairs:\n",
    "            try:\n",
    "                # you can call with URL directly\n",
    "                html = fetch_filing_html(p[\"url\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  skip {p['accession']} ({e})\"); continue\n",
    "            secs  = extract_sections(html)\n",
    "            feats = score_sections(secs, pipe_tok=pipe_tok)\n",
    "            px    = event_closes(t, p[\"filing_date\"])\n",
    "            row   = {\"ticker\":t,\"cik\":cik, **p, **feats, **px}\n",
    "            c0,c7,c28 = row[\"close_0\"], row[\"close_7\"], row[\"close_28\"]\n",
    "            row[\"ret_7\"]  = (c7/c0 - 1.0)*100 if c0 and c7 else np.nan\n",
    "            row[\"ret_28\"] = (c28/c0 - 1.0)*100 if c0 and c28 else np.nan\n",
    "            rows.append(row)\n",
    "            time.sleep(sleep_sec)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(BACKTEST_CSV, index=False)\n",
    "    print(f\"Saved {BACKTEST_CSV} with {len(df)} rows.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f5272",
   "metadata": {},
   "source": [
    "### Backtest Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "67c51108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def run_legitimacy_checks(path=BACKTEST_CSV):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\"])\n",
    "    sent_cols = [c for c in df.columns if c.endswith(\"_sent\")]\n",
    "    def winsorize(s, p=0.01):\n",
    "        lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "        return s.clip(lo, hi)\n",
    "    df[\"ret_7_w\"]  = winsorize(df[\"ret_7\"])\n",
    "    df[\"ret_28_w\"] = winsorize(df[\"ret_28\"])\n",
    "\n",
    "    def corr_table(y_col):\n",
    "        rows=[]\n",
    "        for c in sent_cols + ([\"sent_overall\"] if \"sent_overall\" in df.columns else []):\n",
    "            x,y = df[c], df[y_col]\n",
    "            m = x.notna() & y.notna()\n",
    "            if m.sum() < 8: rows.append((c, np.nan, np.nan)); continue\n",
    "            r,p = pearsonr(x[m], y[m])\n",
    "            rows.append((c,r,p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    corr7, corr28 = corr_table(\"ret_7_w\"), corr_table(\"ret_28_w\")\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Î”sent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    def corr_delta(y_col):\n",
    "        rows=[]\n",
    "        x,y = df[\"Î”sent_overall\"], df[y_col]\n",
    "        m = x.notna() & y.notna()\n",
    "        if m.sum() >= 8:\n",
    "            r,p = pearsonr(x[m], y[m]); rows.append((\"Î”sent_overall\", r, p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    dc7, dc28 = corr_delta(\"ret_7_w\"), corr_delta(\"ret_28_w\")\n",
    "    return {\"corr7\":corr7, \"corr28\":corr28, \"dc7\":dc7, \"dc28\":dc28}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3eb0ac",
   "metadata": {},
   "source": [
    "### Market Cap Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "b7ade05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_market_cap(ticker: str):\n",
    "    url = f\"https://api.polygon.io/v3/reference/tickers/{ticker.upper()}\"\n",
    "    params = {\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=20)\n",
    "    try:\n",
    "        js = r.json().get(\"results\", {})\n",
    "        return js.get(\"market_cap\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fit_bucket_baselines(path=BACKTEST_CSV, min_per_bucket=8,\n",
    "                         ret_clip_pct=0.01,             # winsorize tails of returns (1% each side)\n",
    "                         dsent_abs_cap=0.12             # hard cap on |Î”sent_overall| to limit leverage\n",
    "                        ):\n",
    "    \"\"\"\n",
    "    Train per-cap baselines using robust-ish preprocessing:\n",
    "      - compute Î”sent_overall per ticker\n",
    "      - attach current market cap + bucket\n",
    "      - winsorize returns within each bucket (ret_7, ret_28)\n",
    "      - cap |Î”sent_overall| at dsent_abs_cap\n",
    "    Saves sentiment_baselines.json\n",
    "    \"\"\"\n",
    "    def _winsorize(series: pd.Series, p=0.01):\n",
    "        lo, hi = series.quantile(p), series.quantile(1-p)\n",
    "        return series.clip(lo, hi)\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\",\"sent_overall\"]).copy()\n",
    "    df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Î”sent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    # attach current market cap + bucket\n",
    "    mcap = {}\n",
    "    for t in df[\"ticker\"].unique():\n",
    "        try:\n",
    "            mcap[t] = polygon_market_cap(t)\n",
    "            time.sleep(0.2)\n",
    "        except Exception:\n",
    "            mcap[t] = None\n",
    "    df[\"market_cap\"] = df[\"ticker\"].map(mcap)\n",
    "    df[\"bucket\"] = df[\"market_cap\"].map(cap_bucket)\n",
    "\n",
    "    out = {}\n",
    "    for bucket in sorted(df[\"bucket\"].dropna().unique()):\n",
    "        d = df[df[\"bucket\"] == bucket].dropna(subset=[\"Î”sent_overall\",\"ret_7\",\"ret_28\"]).copy()\n",
    "        if len(d) < min_per_bucket:\n",
    "            continue\n",
    "\n",
    "        # --- winsorize per-bucket ---\n",
    "        d[\"ret_7_w\"]  = _winsorize(d[\"ret_7\"],  p=ret_clip_pct)\n",
    "        d[\"ret_28_w\"] = _winsorize(d[\"ret_28\"], p=ret_clip_pct)\n",
    "\n",
    "        # cap Î”sent_overall to reduce leverage from rare huge text swings\n",
    "        d[\"Î”sent_overall_c\"] = d[\"Î”sent_overall\"].clip(-dsent_abs_cap, dsent_abs_cap)\n",
    "\n",
    "        # fit simple OLS: ret ~ a + b * Î”sent_overall_c\n",
    "        for horizon, ycol in ((\"ret7\",\"ret_7_w\"), (\"ret28\",\"ret_28_w\")):\n",
    "            X = np.c_[np.ones(len(d)), d[\"Î”sent_overall_c\"].values]\n",
    "            y = d[ycol].values\n",
    "            beta = np.linalg.pinv(X).dot(y)  # [intercept, slope]\n",
    "            intercept, slope = float(beta[0]), float(beta[1])\n",
    "            out.setdefault(bucket, {})[horizon] = {\n",
    "                \"intercept\": intercept,\n",
    "                \"slope\": slope,\n",
    "                \"n\": int(len(d)),\n",
    "                \"ret_clip_pct\": ret_clip_pct,\n",
    "                \"dsent_abs_cap\": dsent_abs_cap\n",
    "            }\n",
    "\n",
    "    with open(BASELINES_JSON,\"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved {BASELINES_JSON} (winsorized)\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9a81b",
   "metadata": {},
   "source": [
    "### Fundamentals + DCF Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "7ee4e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H) FUNDAMENTALS + DCF SNAPSHOT (TTM from SEC facts)\n",
    "\n",
    "def get_company_facts(ticker_or_cik):\n",
    "    cik = get_cik(ticker_or_cik)\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{_cik10(cik)}.json\"\n",
    "    r = _session().get(url, headers=SEC_HEADERS, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    time.sleep(0.2)\n",
    "    return r.json()\n",
    "\n",
    "def _ttm_sum(items, n=4):\n",
    "    if not items: return None\n",
    "    vals = [x.get(\"val\") for x in items][-n:]\n",
    "    vals = [v for v in vals if v is not None]\n",
    "    return float(np.nansum(vals)) if vals else None\n",
    "\n",
    "def build_ttm_metrics(facts: dict) -> dict:\n",
    "    usgaap = facts.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "    def get_series(tag):\n",
    "        return (usgaap.get(tag, {}).get(\"units\", {}).get(\"USD\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"USD/shares\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"shares\", []))\n",
    "    revenue_q   = get_series(\"Revenues\")\n",
    "    ni_q        = get_series(\"NetIncomeLoss\")\n",
    "    eps_q       = (usgaap.get(\"EarningsPerShareDiluted\", {}).get(\"units\", {}).get(\"USD/shares\", []) or [])\n",
    "    dil_sh_q    = get_series(\"WeightedAverageNumberOfDilutedSharesOutstanding\")\n",
    "    cfo_q       = get_series(\"NetCashProvidedByUsedInOperatingActivities\")\n",
    "    capex_q     = get_series(\"PaymentsToAcquirePropertyPlantAndEquipment\")\n",
    "\n",
    "    revenue_ttm     = _ttm_sum(revenue_q)\n",
    "    net_income_ttm  = _ttm_sum(ni_q)\n",
    "    eps_ttm         = _ttm_sum(eps_q)\n",
    "    diluted_sh_ttm  = _ttm_sum(dil_sh_q)\n",
    "    cfo_ttm         = _ttm_sum(cfo_q)\n",
    "    capex_ttm       = _ttm_sum(capex_q)\n",
    "    fcf_ttm         = (cfo_ttm or 0.0) - abs(capex_ttm or 0.0)\n",
    "\n",
    "    if eps_ttm and diluted_sh_ttm and net_income_ttm:\n",
    "        approx = eps_ttm * diluted_sh_ttm\n",
    "        if abs(approx - net_income_ttm)/max(1.0, net_income_ttm) > 0.15:\n",
    "            print(\"[warn] EPS*Shares != NetIncome by >15%. Check tags/periods.\")\n",
    "\n",
    "    return dict(\n",
    "        revenue_ttm=revenue_ttm, net_income_ttm=net_income_ttm,\n",
    "        eps_diluted_ttm=eps_ttm, diluted_shares_ttm=diluted_sh_ttm,\n",
    "        cfo_ttm=cfo_ttm, capex_ttm=capex_ttm, fcf_ttm=fcf_ttm,\n",
    "        rev_per_share=(revenue_ttm / diluted_sh_ttm) if (revenue_ttm and diluted_sh_ttm) else None,\n",
    "    )\n",
    "\n",
    "SECTOR_MULTIPLES = {\"Technology\":{\"PE\":30.0,\"PS\":6.8}, \"_default\":{\"PE\":18.0,\"PS\":2.5}}\n",
    "\n",
    "def multiples_anchor(metrics:dict, sector=\"Technology\"):\n",
    "    cfg = SECTOR_MULTIPLES.get(sector, SECTOR_MULTIPLES[\"_default\"])\n",
    "    eps = metrics.get(\"eps_diluted_ttm\")\n",
    "    rps = metrics.get(\"rev_per_share\")\n",
    "    pe_anchor = eps * cfg[\"PE\"] if eps else None\n",
    "    ps_anchor = rps * cfg[\"PS\"] if rps else None\n",
    "    anchors = [x for x in (pe_anchor, ps_anchor) if x is not None and math.isfinite(x)]\n",
    "    mid = float(np.mean(anchors)) if anchors else None\n",
    "    return {\"pe_anchor\":pe_anchor, \"ps_anchor\":ps_anchor, \"fair_value_mid\":mid, \"assumptions\":cfg}\n",
    "\n",
    "def dcf_anchor(metrics:dict, years=5, g=0.04, r=0.095, g_term=0.02):\n",
    "    fcf0 = metrics.get(\"fcf_ttm\")\n",
    "    sh   = metrics.get(\"diluted_shares_ttm\")\n",
    "    if not fcf0 or not sh or sh <= 0: return None\n",
    "    pv, fcf = 0.0, fcf0\n",
    "    for t in range(1, years+1):\n",
    "        fcf *= (1+g)\n",
    "        pv  += fcf / ((1+r)**t)\n",
    "    terminal = (fcf * (1+g_term)) / (r - g_term)\n",
    "    pv_term  = terminal / ((1+r)**years)\n",
    "    return (pv + pv_term) / sh\n",
    "\n",
    "def blended_fair_value(mult_mid, dcf_val, w=0.5):\n",
    "    if mult_mid is None and dcf_val is None: return None\n",
    "    if mult_mid is None: return dcf_val\n",
    "    if dcf_val  is None: return mult_mid\n",
    "    return w*mult_mid + (1-w)*dcf_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbc51f8",
   "metadata": {},
   "source": [
    "### Enhanced DCF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "4c8c2bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Enhanced DCF Engine ===\n",
    "\n",
    "def estimate_discount_rate(ticker, base_rf=0.045, equity_risk_prem=0.055):\n",
    "    \"\"\"\n",
    "    Approximate discount rate r for DCF:\n",
    "    r = rf + beta * ERP + volatility adjustment\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = polygon_daily(ticker, (date.today() - timedelta(days=250)).isoformat(), date.today().isoformat())\n",
    "        df[\"ret\"] = df[\"close\"].pct_change()\n",
    "        vol = df[\"ret\"].std() * (252**0.5)  # annualized\n",
    "    except Exception:\n",
    "        vol = 0.25\n",
    "    beta_est = min(2.0, 0.8 + (vol / 0.25))        # simple proxy\n",
    "    r = base_rf + beta_est * equity_risk_prem\n",
    "    # small upward adjustment for very high vol\n",
    "    if vol > 0.6: r += 0.01\n",
    "    return round(r, 4)\n",
    "\n",
    "def estimate_growth_rate(ticker, delta_sent=0.0, base_g=0.04):\n",
    "    \"\"\"\n",
    "    Adjust growth rate from sentiment change and past revenue growth.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        facts = get_company_facts(get_cik(ticker))\n",
    "        usgaap = facts[\"facts\"][\"us-gaap\"]\n",
    "        rev = usgaap.get(\"Revenues\", {}).get(\"units\", {}).get(\"USD\", [])\n",
    "        vals = [x[\"val\"] for x in rev[-8:] if x.get(\"val\") is not None]\n",
    "        if len(vals) >= 4:\n",
    "            hist_g = (vals[-1]/vals[0])**(1/(len(vals)-1)) - 1\n",
    "            g = 0.5*base_g + 0.5*hist_g\n",
    "        else:\n",
    "            g = base_g\n",
    "    except Exception:\n",
    "        g = base_g\n",
    "    # slight sentiment tweak\n",
    "    g += 0.15 * delta_sent\n",
    "    return max(0.0, round(g, 4))\n",
    "\n",
    "def estimate_terminal_rate(ticker, sector=\"Technology\"):\n",
    "    \"\"\"\n",
    "    Estimate terminal growth by sector, defaulting near inflation (1â€“3%)\n",
    "    \"\"\"\n",
    "    sector_defaults = {\n",
    "        \"Technology\": 0.025, \"Healthcare\": 0.02, \"Consumer Staples\": 0.015,\n",
    "        \"Financials\": 0.02, \"Energy\": 0.01, \"_default\": 0.02\n",
    "    }\n",
    "    return sector_defaults.get(sector, sector_defaults[\"_default\"])\n",
    "\n",
    "def raw_dcf_ev(metrics, g, r, g_term=0.02, years=5):\n",
    "    \"\"\"\n",
    "    Compute raw enterprise value (EV) directly from free cash flow to firm (FCFF).\n",
    "    \"\"\"\n",
    "    fcf0 = metrics.get(\"fcf_ttm\")\n",
    "    if not fcf0 or not np.isfinite(fcf0): return None\n",
    "    fcf, pv = fcf0, 0.0\n",
    "    for t in range(1, years+1):\n",
    "        fcf *= (1+g)\n",
    "        pv += fcf / ((1+r)**t)\n",
    "    terminal = (fcf * (1+g_term)) / (r - g_term)\n",
    "    pv_term = terminal / ((1+r)**years)\n",
    "    ev = pv + pv_term\n",
    "    return round(ev, 2)\n",
    "\n",
    "def make_dcf_bands(metrics, g_base, r_base, g_term, vol, sens_coeff=0.25):\n",
    "    \"\"\"\n",
    "    Generate low/mid/high valuation bands based on ticker volatility and sensitivity.\n",
    "    \"\"\"\n",
    "    fcf = metrics.get(\"fcf_ttm\"); sh = metrics.get(\"diluted_shares_ttm\")\n",
    "    if not fcf or not sh: return {}\n",
    "    vol_adj = min(1.5, 1 + sens_coeff * vol)  # higher vol widens spread\n",
    "\n",
    "    scenarios = {\n",
    "        \"low\":  {\"g\": g_base * 0.5, \"r\": r_base + 0.02*vol_adj},\n",
    "        \"mid\":  {\"g\": g_base,       \"r\": r_base},\n",
    "        \"high\": {\"g\": g_base * (1+0.5*vol_adj), \"r\": max(0.01, r_base - 0.02*vol_adj)}\n",
    "    }\n",
    "\n",
    "    bands = {}\n",
    "    for name, params in scenarios.items():\n",
    "        ev = raw_dcf_ev(metrics, params[\"g\"], params[\"r\"], g_term)\n",
    "        if ev:\n",
    "            bands[name] = round(ev / sh, 2)  # per-share fair value\n",
    "    return bands"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53034fd",
   "metadata": {},
   "source": [
    "### Options Spread Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "1bbedc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradier_expirations(ticker: str):\n",
    "    \"\"\"\n",
    "    List available expiration dates (YYYY-MM-DD) for a symbol.\n",
    "    Be robust to empty bodies, HTML error pages (401/403), and rate limits.\n",
    "    \"\"\"\n",
    "    url = f\"{TRADIER_BASE}/markets/options/expirations\"\n",
    "    try:\n",
    "        r = requests.get(\n",
    "            url,\n",
    "            headers=_tradier_headers(),\n",
    "            params={\"symbol\": ticker.upper(), \"includeAll\": \"false\"},\n",
    "            timeout=20\n",
    "        )\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[Tradier] Network error: {e}\")\n",
    "        return []\n",
    "\n",
    "    # Handle non-200 OR empty body up front\n",
    "    if r.status_code != 200 or not r.text or not r.text.strip():\n",
    "        print(f\"[Tradier] Error {r.status_code} {r.reason}: {r.text[:200]}\")\n",
    "        return []\n",
    "\n",
    "    # Try to parse JSON safely\n",
    "    try:\n",
    "        data = r.json().get(\"expirations\", {})\n",
    "        exps = data.get(\"date\", [])\n",
    "        # API may return a single string instead of a list\n",
    "        return exps if isinstance(exps, list) else ([exps] if exps else [])\n",
    "    except Exception as e:\n",
    "        print(f\"[Tradier] JSON error (expirations) for {ticker}: {e}\")\n",
    "        print(r.text[:400])  # show first part of body for debugging (often HTML/401)\n",
    "        return []\n",
    "\n",
    "def tradier_chain(ticker: str, expiration: str):\n",
    "    url = f\"{TRADIER_BASE}/markets/options/chains\"\n",
    "    params = {\"symbol\": ticker.upper(), \"expiration\": expiration, \"greeks\": \"false\"}\n",
    "    try:\n",
    "        r = requests.get(url, headers=_tradier_headers(), params=params, timeout=20)\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"[Tradier] Network error: {e}\")\n",
    "        return []\n",
    "    if r.status_code != 200 or not r.text.strip():\n",
    "        print(f\"[Tradier] Error {r.status_code} {r.reason}: {r.text[:200]}\")\n",
    "        return []\n",
    "    try:\n",
    "        js = r.json()\n",
    "        opts = js.get(\"options\", {}).get(\"option\", [])\n",
    "        return opts if isinstance(opts, list) else [opts]\n",
    "    except Exception as e:\n",
    "        print(f\"[Tradier] JSON error for {ticker}: {e}\")\n",
    "        print(r.text[:400])\n",
    "        return []\n",
    "\n",
    "def best_vertical_by_target(ticker: str, pred_pct: float, polygon_price_fn, horizon_days=28):\n",
    "    \"\"\"\n",
    "    If pred_pct > 0 => bull call vertical. If < 0 => bear put vertical.\n",
    "    'polygon_price_fn' should be a callable like polygon_latest_close(ticker)->float.\n",
    "    \"\"\"\n",
    "    # pick an expiration ~horizon_days out (nearest available)\n",
    "    today = date.today()\n",
    "    target = today + timedelta(days=horizon_days)\n",
    "    exps = tradier_expirations(ticker)\n",
    "    if not exps:\n",
    "        return None\n",
    "    # choose the expiration closest to 'target' that is >= today\n",
    "    def to_date(s): \n",
    "        y,m,d = map(int, s.split(\"-\")); \n",
    "        return date(y,m,d)\n",
    "    future_exps = [e for e in exps if to_date(e) >= today]\n",
    "    if not future_exps:\n",
    "        return None\n",
    "    exp = min(future_exps, key=lambda s: abs(to_date(s)-target))\n",
    "\n",
    "    chain = tradier_chain(ticker, exp)\n",
    "    if not chain:\n",
    "        return None\n",
    "\n",
    "    pnow = polygon_price_fn(ticker)\n",
    "    if not pnow:\n",
    "        return None\n",
    "    p_tgt = pnow * (1 + pred_pct/100.0)\n",
    "\n",
    "    # strikes universe\n",
    "    strikes = sorted({float(o[\"strike\"]) for o in chain if \"strike\" in o})\n",
    "    if not strikes:\n",
    "        return None\n",
    "    nearest = lambda x: min(strikes, key=lambda k: abs(k - x))\n",
    "\n",
    "    if pred_pct >= 0:\n",
    "        k_buy  = nearest(pnow * 0.99)\n",
    "        k_sell = nearest(p_tgt * 1.02)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bull_call\"\n",
    "    else:\n",
    "        k_buy  = nearest(pnow * 1.01)\n",
    "        k_sell = nearest(p_tgt * 0.98)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bear_put\"\n",
    "\n",
    "    if not leg_buy or not leg_sell:\n",
    "        return None\n",
    "\n",
    "    def mid(q):\n",
    "        b = float(q.get(\"bid\", 0.0)); a = float(q.get(\"ask\", 0.0))\n",
    "        return (b + a)/2 if (a and b) else float(q.get(\"last\", 0.0))\n",
    "\n",
    "    debit = max(0.01, mid(leg_buy[0]) - mid(leg_sell[0]))\n",
    "    width = abs(k_sell - k_buy)\n",
    "    max_profit = max(0.0, width - debit)\n",
    "    rr = (max_profit / debit) if debit > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"type\": spread_type,\n",
    "        \"expiration\": exp,\n",
    "        \"buy_strike\": float(k_buy),\n",
    "        \"sell_strike\": float(k_sell),\n",
    "        \"debit\": round(debit, 2),\n",
    "        \"width\": float(width),\n",
    "        \"max_profit\": round(max_profit, 2),\n",
    "        \"R_by_Risk\": round(rr, 2) if rr else None,\n",
    "        \"price_now\": float(pnow),\n",
    "        \"price_target\": float(p_tgt),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7212a63c",
   "metadata": {},
   "source": [
    "### Auto-detect Sector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "0db9264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests, re\n",
    "\n",
    "POLYGON_API_KEY = os.environ.get(\"POLYGON_API_KEY\")\n",
    "POLY_BASE = \"https://api.polygon.io\"\n",
    "\n",
    "# Minimal SIC â†’ GICS-ish mapping by description keywords (extend as needed)\n",
    "_SIC_TO_SECTOR_MAP = [\n",
    "    (r\"(telecom|telephone|broadcast|media|stream|social)\", \"Communication Services\"),\n",
    "    (r\"(apparel|retail|hotel|restaurant|leisure|furnish|furniture|household|auto|travel|entertainment|footwear|luxury)\", \"Consumer Discretionary\"),\n",
    "    (r\"(food|beverage|supermarket|grocery|tobacco|household products)\", \"Consumer Staples\"),\n",
    "    (r\"(oil|gas|coal|energy|exploration|refinery|drilling)\", \"Energy\"),\n",
    "    (r\"(bank|insurance|broker|asset|capital markets|lending|financial)\", \"Financials\"),\n",
    "    (r\"(pharma|biotech|health|medical|hospital|diagnostic|life science|device)\", \"Health Care\"),\n",
    "    (r\"(aero|defense|machinery|transport|logistics|construction|industrial|capital goods)\", \"Industrials\"),\n",
    "    (r\"(software|semiconductor|hardware|it|information technology|computer|electronics)\", \"Information Technology\"),\n",
    "    (r\"(chem|metal|mining|paper|forest|glass|cement|commodity|materials)\", \"Materials\"),\n",
    "    (r\"(reit|real estate|property|trust)\", \"Real Estate\"),\n",
    "    (r\"(utility|electric|water|gas utility|power)\", \"Utilities\"),\n",
    "]\n",
    "\n",
    "def _map_sic_desc_to_sector(sic_desc: str) -> str:\n",
    "    if not sic_desc:\n",
    "        return \"Industrials\"  # neutral default\n",
    "    s = sic_desc.lower()\n",
    "    for pat, sector in _SIC_TO_SECTOR_MAP:\n",
    "        if re.search(pat, s):\n",
    "            return sector\n",
    "    # simple catch-alls\n",
    "    if \"furnish\" in s or \"fixture\" in s:\n",
    "        return \"Consumer Discretionary\"\n",
    "    return \"Industrials\"\n",
    "\n",
    "def detect_sector_polygon(ticker: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses Polygon v3 reference/tickers/{ticker} to pull SIC description\n",
    "    and map it into the 11 GICS sectors above.\n",
    "    \"\"\"\n",
    "    url = f\"{POLY_BASE}/v3/reference/tickers/{ticker.upper()}\"\n",
    "    params = {\"apiKey\": POLYGON_API_KEY}\n",
    "    try:\n",
    "        r = requests.get(url, params=params, timeout=15)\n",
    "        if r.status_code != 200 or not r.text.strip():\n",
    "            return \"Industrials\"\n",
    "        js = r.json() or {}\n",
    "        sic_desc = js.get(\"results\", {}).get(\"sic_description\") or \"\"\n",
    "        # fallback: try NAICS description if present\n",
    "        if not sic_desc:\n",
    "            sic_desc = js.get(\"results\", {}).get(\"naics_description\", \"\")\n",
    "        return _map_sic_desc_to_sector(sic_desc)\n",
    "    except Exception:\n",
    "        return \"Industrials\"\n",
    "    \n",
    "def auto_w_short(cap_bucket: str, baselines: dict) -> float:\n",
    "    \"\"\"\n",
    "    Choose short-horizon weight from your 28d baseline quality.\n",
    "    Heuristic uses slope magnitude and sample size as a proxy for confidence.\n",
    "    Returns a value in [0.15, 0.45].\n",
    "    \"\"\"\n",
    "    b = (baselines.get(cap_bucket, {}) or {}).get(\"ret28\", {}) or {}\n",
    "    slope = abs(b.get(\"slope\", 0.0))\n",
    "    n = b.get(\"n\", 0)\n",
    "\n",
    "    # crude confidence âˆˆ [0,1]: steeper slopes + bigger samples â†’ higher weight\n",
    "    conf = min(1.0, (slope * 8.0) * (n / 300.0))\n",
    "\n",
    "    return float(max(0.15, min(0.45, 0.20 + 0.25 * conf)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ab4ed",
   "metadata": {},
   "source": [
    "### Single Ticker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "c8176f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_two_10q_delta_sent(ticker: str):\n",
    "    cik   = get_cik(ticker)\n",
    "    pairs = list_10q_with_dates(cik, max_n=2)\n",
    "    if not pairs: return np.nan, np.nan, np.nan\n",
    "    pipe_tok = load_finbert()\n",
    "    scores = []\n",
    "    for p in pairs:\n",
    "        html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "        secs = extract_sections(html)\n",
    "        feats= score_sections(secs, pipe_tok=pipe_tok)\n",
    "        scores.append(feats[\"sent_overall\"])\n",
    "        time.sleep(0.3)\n",
    "    if len(scores)==1: return scores[0], np.nan, np.nan\n",
    "    return scores[0], scores[1], scores[0]-scores[1]\n",
    "\n",
    "def load_baselines(path=BASELINES_JSON):\n",
    "    if not os.path.exists(path): raise FileNotFoundError(\"Run cell G to fit/save sentiment baselines first.\")\n",
    "    with open(path,\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# === I) SINGLE-TICKER PREDICTOR â€” multi-10Q support + auto w_short + auto sector + raw EV DCF + bands ===\n",
    "def predict_ticker(\n",
    "    ticker: str,\n",
    "    w_short=\"auto\",                 # \"auto\" or float in [0,1]\n",
    "    sector=None,                    # None -> detect via Polygon helper\n",
    "    baselines=None,                 # pass preloaded baselines to avoid reloading\n",
    "    suggest_spread: bool = False,   # keep off while debugging\n",
    "    n_filings: int = 2              # number of recent 10-Qs to use (2â€“4 typical)\n",
    "):\n",
    "    import numpy as np, json, os\n",
    "\n",
    "    # ---------- helpers (local so this cell is self-contained) ----------\n",
    "    def _load_sentiment_baselines(path: str = \"sentiment_baselines.json\") -> dict:\n",
    "        try:\n",
    "            with open(path, \"r\") as f:\n",
    "                return json.load(f)\n",
    "        except Exception:\n",
    "            return {}\n",
    "\n",
    "    def _auto_w_short(cap_bucket: str, bl: dict) -> float:\n",
    "        \"\"\"\n",
    "        Choose short-horizon weight from the 28d baseline quality.\n",
    "        Uses slope magnitude and sample size as a proxy for confidence.\n",
    "        Returns a value in [0.15, 0.45].\n",
    "        \"\"\"\n",
    "        b = (bl.get(cap_bucket, {}) or {}).get(\"ret28\", {}) or {}\n",
    "        slope = abs(b.get(\"slope\", 0.0))\n",
    "        n = b.get(\"n\", 0)\n",
    "        conf = min(1.0, (slope * 8.0) * (n / 300.0))  # crude confidence proxy\n",
    "        return float(max(0.15, min(0.45, 0.20 + 0.25 * conf)))\n",
    "\n",
    "    def _get_all_baselines(passed):\n",
    "        if isinstance(passed, dict) and passed:\n",
    "            return passed\n",
    "        try:\n",
    "            return load_baselines()  # if older helper exists\n",
    "        except Exception:\n",
    "            return _load_sentiment_baselines()\n",
    "\n",
    "    # ---------- 1) Î”sentiment from multiple recent 10-Qs ----------\n",
    "    def _score_recent_10q_sentiments(ticker: str, max_recent: int = 4):\n",
    "        filings = list_10q_with_dates(ticker, max_n=max_recent)  # newest first\n",
    "        rows = []\n",
    "        for p in filings:\n",
    "            html = fetch_filing_html(p[\"url\"])\n",
    "            secs = extract_sections(html)\n",
    "            sc   = score_sections(secs)\n",
    "            rows.append({\n",
    "                \"date\": p[\"filing_date\"],\n",
    "                \"sent_overall\": float(sc.get(\"sent_overall\", np.nan))\n",
    "            })\n",
    "        # oldest -> newest for diffs\n",
    "        rows = sorted(rows, key=lambda r: r[\"date\"])\n",
    "        return rows\n",
    "\n",
    "    def _latest_k_delta(ticker: str, n_filings: int = 2):\n",
    "        rows = _score_recent_10q_sentiments(ticker, max_recent=max(n_filings, 2))\n",
    "        vals = [r[\"sent_overall\"] for r in rows if np.isfinite(r[\"sent_overall\"])]\n",
    "        if len(vals) < 2:\n",
    "            raise ValueError(f\"Need at least 2 valid sentiment points for {ticker}\")\n",
    "        # keep last n\n",
    "        vals = vals[-n_filings:]\n",
    "        series = [(rows[-len(vals)+i][\"date\"], float(v)) for i, v in enumerate(vals)]\n",
    "        latest = vals[-1]\n",
    "        base   = float(np.mean(vals[:-1]))\n",
    "        slope  = float(np.mean(np.diff(vals))) if len(vals) >= 3 else 0.0\n",
    "        # return tuple to destructure as used below\n",
    "        return latest, vals[-2], (latest - base), series, slope\n",
    "\n",
    "    # Use new multi-10Q delta\n",
    "    sent_now, sent_prev, d_sent, sent_series, sent_slope = _latest_k_delta(ticker, n_filings)\n",
    "    d_sent_safe = float(np.nan_to_num(d_sent if np.isfinite(d_sent) else np.nan, nan=0.0))\n",
    "\n",
    "    # ---------- 2) Market-cap bucket & short-term return prediction ----------\n",
    "    mc = polygon_market_cap(ticker)\n",
    "    bkt = cap_bucket(mc)\n",
    "    base = _get_all_baselines(baselines).get(bkt, {})\n",
    "    a7, b7 = base.get(\"ret7\", {}).get(\"intercept\", 0.0), base.get(\"ret7\", {}).get(\"slope\", 0.0)\n",
    "    a28, b28 = base.get(\"ret28\", {}).get(\"intercept\", 0.0), base.get(\"ret28\", {}).get(\"slope\", 0.0)\n",
    "    pred7 = a7 + b7 * d_sent_safe\n",
    "    pred28 = a28 + b28 * d_sent_safe\n",
    "\n",
    "    # Auto size w_short if requested\n",
    "    if (w_short is None) or (isinstance(w_short, str) and w_short.lower() == \"auto\"):\n",
    "        w_short = _auto_w_short(bkt, _get_all_baselines(baselines))\n",
    "    w_short = float(w_short)\n",
    "\n",
    "    # ---------- 3) Fundamentals + DCF (static + dynamic) ----------\n",
    "    sec_used = sector or detect_sector_polygon(ticker)\n",
    "    cik = get_cik(ticker)\n",
    "    facts = get_company_facts(cik)\n",
    "    gaap = build_ttm_metrics(facts)\n",
    "\n",
    "    mult = multiples_anchor(gaap, sector=sec_used)\n",
    "    dcf_static = dcf_anchor(gaap, years=5, g=0.04, r=0.095, g_term=0.02)\n",
    "\n",
    "    r_dyn = estimate_discount_rate(ticker)\n",
    "    g_dyn = estimate_growth_rate(ticker, d_sent_safe)\n",
    "    g_term = estimate_terminal_rate(ticker, sec_used)\n",
    "\n",
    "    ev_raw = raw_dcf_ev(gaap, g_dyn, r_dyn, g_term)\n",
    "    sh = gaap.get(\"diluted_shares_ttm\") or np.nan\n",
    "    dcf_ps = (ev_raw / sh) if (ev_raw is not None and np.isfinite(sh) and sh > 0) else None\n",
    "\n",
    "    vol = annualized_vol_from_polygon(ticker, lookback_days=252, winsor_pct=0.01) or 0.30\n",
    "    bands = make_dcf_bands(gaap, g_dyn, r_dyn, g_term, vol)\n",
    "\n",
    "    # ---------- 4) Prices + final blend ----------\n",
    "    pnow = polygon_latest_close(ticker)\n",
    "    price_28d = pnow * (1 + pred28 / 100.0) if (pnow and np.isfinite(pred28)) else None\n",
    "\n",
    "    fair_value_blend_old = blended_fair_value(mult.get(\"fair_value_mid\"), dcf_static, w=0.5)\n",
    "    fair_value_blend = blended_fair_value(mult.get(\"fair_value_mid\"), dcf_ps, w=0.5)\n",
    "\n",
    "    if (price_28d is not None) and (fair_value_blend is not None):\n",
    "        final_target = w_short * price_28d + (1.0 - w_short) * fair_value_blend\n",
    "    else:\n",
    "        final_target = price_28d if price_28d is not None else fair_value_blend\n",
    "\n",
    "    direction = \"UP\" if (final_target and pnow and final_target > pnow) else \"DOWN\"\n",
    "\n",
    "    # ---------- 5) Optional: options vertical suggestion ----------\n",
    "    options_vertical = None\n",
    "    if suggest_spread and pnow and final_target:\n",
    "        try:\n",
    "            options_vertical = best_vertical_by_target(ticker, pnow, final_target)\n",
    "        except Exception as e:\n",
    "            print(f\"[options] skipped for {ticker}: {e}\")\n",
    "\n",
    "    # ---------- 6) Return ----------\n",
    "    return {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"sector_used\": sec_used,\n",
    "        \"market_cap\": mc,\n",
    "        \"cap_bucket\": bkt,\n",
    "\n",
    "        # sentiment diagnostics\n",
    "        \"sent_overall_now\": sent_now,\n",
    "        \"sent_overall_prev\": sent_prev,\n",
    "        \"Î”sent_overall\": d_sent,\n",
    "        \"asent_overall\": d_sent_safe,\n",
    "        \"sent_series\": sent_series,\n",
    "        \"sent_slope\": sent_slope,\n",
    "\n",
    "        # short-term predictions\n",
    "        \"pred_ret_7d_pct\": pred7,\n",
    "        \"pred_ret_28d_pct\": pred28,\n",
    "        \"price_now\": pnow,\n",
    "        \"price_28d_target\": price_28d,\n",
    "\n",
    "        # valuation anchors\n",
    "        \"multiples_mid\": mult.get(\"fair_value_mid\"),\n",
    "        \"dcf_static\": dcf_static,\n",
    "        \"fair_value_blend_old\": fair_value_blend_old,\n",
    "\n",
    "        # dynamic DCF & bands\n",
    "        \"dcf_ev_raw\": ev_raw,\n",
    "        \"dcf_per_share\": dcf_ps,\n",
    "        \"disc_rate_r\": r_dyn,\n",
    "        \"growth_g\": g_dyn,\n",
    "        \"terminal_g\": g_term,\n",
    "        \"vol_annual\": vol,\n",
    "        \"dcf_bands\": bands,\n",
    "\n",
    "        # final decision\n",
    "        \"w_short\": w_short,\n",
    "        \"final_blended_target\": final_target,\n",
    "        \"final_direction\": direction,\n",
    "\n",
    "        # extras\n",
    "        \"baseline_used\": base,\n",
    "        \"options_vertical\": options_vertical,\n",
    "        \"n_filings_used\": int(n_filings),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd55847",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "7a2bcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_baseline():\n",
    "    \"\"\"\n",
    "    1. Builds backtest CSV across a mixed-cap universe.\n",
    "    2. Runs legitimacy checks (optional).\n",
    "    3. Fits per-cap baselines and saves to sentiment_baselines.json.\n",
    "    \"\"\"\n",
    "    micro = \"HZO, TIRX, STRC, GWRS, UPXI, GCTK, VTSI, HCAT, OPRX, AOUT, FCEL, HITI, WKSP\".split(\",\")\n",
    "    small = \"BLKB, HQY, PIPR, HAYW, NVCR, SMPL, MGPI, BE, PRCT, SKYW, AVAV, INMD, VRTS, CNXN, REZI, ASTE, MHO, CELH, ABM, PCT\".split(\",\")\n",
    "    mid   = \"LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB, DAL, CHRW, WDC, AAP, CZR, CHD, SWKS, COHR, PTC, HOLX, MKTX\".split(\",\")\n",
    "    large = \"AAPL, MSFT, AMZN, NVDA, GOOGL, META, JPM, V, JNJ, PG, XOM, UNH, PEP, KO, COST, ORCL, DIS, HD, BAC, WMT\".split(\",\")\n",
    "    TICKERS = [x.strip() for s in (micro+small+mid+large) for x in [s] if x.strip()]\n",
    "    df = backtest_10q_sentiment(TICKERS, max_filings=6)\n",
    "    print(df.head())\n",
    "    _ = run_legitimacy_checks(BACKTEST_CSV)\n",
    "    baselines = fit_bucket_baselines(BACKTEST_CSV)\n",
    "    # Save JSON and also return it for convenience\n",
    "    return baselines\n",
    "\n",
    "def load_sentiment_baselines(path: str = \"sentiment_baselines.json\") -> dict:\n",
    "    \"\"\"\n",
    "    Read the saved per-cap baselines JSON produced by fit_bucket_baselines().\n",
    "    Fallback to empty dict if not found.\n",
    "    \"\"\"\n",
    "    import json, os\n",
    "    try:\n",
    "        if not os.path.exists(path):\n",
    "            return {}\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception:\n",
    "        return {}\n",
    "\n",
    "def run_predict(\n",
    "    tickers,\n",
    "    w_short=\"auto\",\n",
    "    sector=None,\n",
    "    suggest_spread=False,\n",
    "    baselines=None,\n",
    "    n_filings: int = 4\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate 1+ tickers with sentiment + DCF.\n",
    "    - w_short: \"auto\" chooses weight from baseline quality; float overrides (0..1).\n",
    "    - sector:  None => auto-detect via Polygon; or pass a string.\n",
    "    - baselines: pass a loaded dict to avoid re-reading per call.\n",
    "    - n_filings: number of recent 10-Qs to use (2..4). 2 keeps prior behavior.\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    if baselines is None:\n",
    "        baselines = load_sentiment_baselines()   # loader, not fitter\n",
    "    for t in tickers:\n",
    "        sec_used = sector or detect_sector_polygon(t)\n",
    "        res = predict_ticker(\n",
    "            t,\n",
    "            w_short=w_short,           # \"auto\" or a float\n",
    "            sector=sec_used,\n",
    "            baselines=baselines,       # pass once (no repeated file IO)\n",
    "            n_filings=n_filings\n",
    "        )\n",
    "        if suggest_spread and res.get(\"pred_ret_28d_pct\") is not None:\n",
    "            res[\"options_vertical\"] = best_vertical_by_target(\n",
    "                t, res[\"pred_ret_28d_pct\"], polygon_price_fn=polygon_latest_close\n",
    "            )\n",
    "        rows.append(res)\n",
    "    cols_order = [\n",
    "        \"ticker\",\"sector_used\",\"cap_bucket\",\"market_cap\",\"price_now\",\n",
    "        \"asent_overall\",\"Î”sent_overall\",\"pred_ret_7d_pct\",\"pred_ret_28d_pct\",\"price_28d_target\",\n",
    "        \"multiples_mid\",\"dcf_static\",\"dcf_ev_raw\",\"dcf_per_share\",\"dcf_bands\",\n",
    "        \"disc_rate_r\",\"growth_g\",\"terminal_g\",\"vol_annual\",\n",
    "        \"w_short\",\"final_blended_target\",\"final_direction\",\"n_filings_used\"\n",
    "    ]\n",
    "    return pd.DataFrame(rows)[cols_order]\n",
    "\n",
    "def _baseline_params(bl: dict, bucket: str):\n",
    "    \"\"\"\n",
    "    Returns (i7, s7, i28, s28, clip, cap, n) for the given cap bucket,\n",
    "    supporting both flat and nested baseline schemas.\n",
    "    Units are in percent space to match your file.\n",
    "    \"\"\"\n",
    "    b = bl[bucket]\n",
    "    if \"ret7\" in b:  # nested schema\n",
    "        i7   = float(b[\"ret7\"][\"intercept\"])\n",
    "        s7   = float(b[\"ret7\"][\"slope\"])\n",
    "        i28  = float(b[\"ret28\"][\"intercept\"])\n",
    "        s28  = float(b[\"ret28\"][\"slope\"])\n",
    "        clip = float(b[\"ret7\"].get(\"ret_clip_pct\", 0.01))\n",
    "        cap  = float(b[\"ret7\"].get(\"dsent_abs_cap\", 0.12))\n",
    "        n    = int(b[\"ret7\"].get(\"n\", 0))\n",
    "    else:            # flat schema\n",
    "        i7   = float(b[\"intercept7\"]);   s7  = float(b[\"slope7\"])\n",
    "        i28  = float(b[\"intercept28\"]);  s28 = float(b[\"slope28\"])\n",
    "        clip = float(b.get(\"ret_clip_pct\", 0.01))\n",
    "        cap  = float(b.get(\"dsent_abs_cap\", 0.12))\n",
    "        n    = int(b.get(\"n\", 0))\n",
    "    return i7, s7, i28, s28, clip, cap, n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbf216",
   "metadata": {},
   "source": [
    "### Create Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "cef60c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baselines = run_full_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b72688",
   "metadata": {},
   "source": [
    "### Analyze Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "262b339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === Visual check for micro/small buckets: scatter + baseline line + outlier labels ===\n",
    "# import json, math, matplotlib.pyplot as plt\n",
    "\n",
    "# HORIZON = \"ret_28\"   # change to \"ret_7\" if you want 7-day\n",
    "# CSV_PATH = BACKTEST_CSV\n",
    "# BASE_JSON = BASELINES_JSON\n",
    "\n",
    "# def _winsorize(s, p=0.01):\n",
    "#     lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "#     return s.clip(lo, hi)\n",
    "\n",
    "# # 1) Load backtest rows and rebuild Î”sent_overall\n",
    "# df = pd.read_csv(CSV_PATH)\n",
    "# df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "# df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "# df[\"Î”sent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "# # 2) Attach market caps + buckets (light Polygon calls; NOT re-scraping filings)\n",
    "# #    (Reuses your polygon_market_cap + cap_bucket helpers defined earlier)\n",
    "# mc_map = {t: polygon_market_cap(t) for t in df[\"ticker\"].unique()}\n",
    "# df[\"market_cap\"] = df[\"ticker\"].map(mc_map)\n",
    "# df[\"bucket\"] = df[\"market_cap\"].map(cap_bucket)\n",
    "\n",
    "# # 3) Pick bucket subset(s) and winsorize returns to reduce distortion\n",
    "# use = df.dropna(subset=[\"Î”sent_overall\", HORIZON]).copy()\n",
    "# use[HORIZON+\"_w\"] = _winsorize(use[HORIZON], p=0.01)\n",
    "\n",
    "# # 4) Load saved baselines (so we can plot the learned line on top)\n",
    "# with open(BASE_JSON, \"r\") as f:\n",
    "#     base = json.load(f)\n",
    "\n",
    "# def _plot_bucket(bucket_name):\n",
    "#     d = use[use[\"bucket\"] == bucket_name].dropna(subset=[\"Î”sent_overall\", HORIZON+\"_w\"]).copy()\n",
    "#     if d.empty:\n",
    "#         print(f\"[info] No rows for bucket={bucket_name}\")\n",
    "#         return\n",
    "\n",
    "#     # OLS on this subset\n",
    "#     X = np.c_[np.ones(len(d)), d[\"Î”sent_overall\"].values]\n",
    "#     y = d[HORIZON+\"_w\"].values\n",
    "#     beta = np.linalg.pinv(X).dot(y)\n",
    "#     a_hat, b_hat = float(beta[0]), float(beta[1])\n",
    "\n",
    "#     # Residuals as a Series aligned to d's index\n",
    "#     y_hat = a_hat + b_hat * d[\"Î”sent_overall\"].values\n",
    "#     resid = pd.Series(y - y_hat, index=d.index, name=\"_resid\")\n",
    "\n",
    "#     # Top outliers by |resid|\n",
    "#     out_idx = resid.abs().sort_values(ascending=False).head(8).index\n",
    "#     out = d.loc[out_idx].copy()\n",
    "#     out[\"_resid\"] = resid.loc[out_idx]\n",
    "\n",
    "#     # Saved baseline line\n",
    "#     saved = base.get(bucket_name, {}).get(\"ret28\" if HORIZON == \"ret_28\" else \"ret7\", {})\n",
    "#     a0 = saved.get(\"intercept\", np.nan)\n",
    "#     b0 = saved.get(\"slope\", np.nan)\n",
    "#     n0 = saved.get(\"n\", 0)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(7, 5))\n",
    "#     plt.scatter(d[\"Î”sent_overall\"], d[HORIZON+\"_w\"], s=25, alpha=0.65)\n",
    "#     xs = np.linspace(d[\"Î”sent_overall\"].min(), d[\"Î”sent_overall\"].max(), 200)\n",
    "\n",
    "#     # This-subset OLS fit\n",
    "#     plt.plot(xs, a_hat + b_hat * xs, linewidth=2, label=f\"OLS subset: a={a_hat:.2f}, b={b_hat:.2f}\")\n",
    "\n",
    "#     # Saved baseline fit (if available)\n",
    "#     if np.isfinite(a0) and np.isfinite(b0):\n",
    "#         plt.plot(xs, a0 + b0 * xs, linestyle=\"--\", linewidth=2,\n",
    "#                  label=f\"Saved baseline: a={a0:.2f}, b={b0:.2f}, n={n0}\")\n",
    "\n",
    "#     plt.title(f\"{bucket_name.upper()} â€” Î”sent_overall vs {HORIZON} (winsorized)\")\n",
    "#     plt.xlabel(\"Î”sent_overall\")\n",
    "#     plt.ylabel(f\"{HORIZON} return (%)\")\n",
    "#     plt.grid(True, alpha=0.25)\n",
    "#     plt.legend(loc=\"best\")\n",
    "\n",
    "#     # Annotate top outliers\n",
    "#     for _, r in out.iterrows():\n",
    "#         plt.annotate(r[\"ticker\"], (r[\"Î”sent_overall\"], r[HORIZON+\"_w\"]),\n",
    "#                      xytext=(5, 5), textcoords=\"offset points\", fontsize=8)\n",
    "\n",
    "#     plt.show()\n",
    "\n",
    "#     # Table of those outliers\n",
    "#     display(\n",
    "#         out[[\"ticker\", \"filing_date\", \"Î”sent_overall\", HORIZON, HORIZON+\"_w\", \"_resid\"]]\n",
    "#         .assign(resid_abs=lambda x: x[\"_resid\"].abs())\n",
    "#         .sort_values(\"resid_abs\", ascending=False)\n",
    "#     )\n",
    "\n",
    "# # 5) Generate plots for micro and small\n",
    "# _plot_bucket(\"micro\")\n",
    "# _plot_bucket(\"small\")\n",
    "# _plot_bucket(\"mid\")\n",
    "# _plot_bucket(\"large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270d12e",
   "metadata": {},
   "source": [
    "### Run Predictions on Given Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "9f96f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] loaded sentiment baselines from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] EPS*Shares != NetIncome by >15%. Check tags/periods.\n",
      "sector: Health Care | cap bucket: small | w_short: 0.45\n",
      "=== Prediction Summary ===\n",
      "Ticker: ANGO\n",
      "Cap bucket: small | Market cap: 506,346,082.76\n",
      "Asent_overall: -0.0010\n",
      "Pred 7d: 2.27% | Pred 28d: 3.04%\n",
      "Price now: 12.39\n",
      "DCF (static) fair value: -7.37\n",
      "Multiples mid: -11.96\n",
      "28d target: 12.77\n",
      "DCF bands â€” Low: -3.15 | Mid: -3.86 | High: -4.93\n",
      "r: 15.50% | g: 2.68% | g_term: 2.00% | vol_ann: 50.33%\n",
      "Final blended target: 1.39, DOWN\n",
      "\n",
      "(No options suggestion, ensure TRADIER_ACCESS_TOKEN is set and chains are available.)\n",
      "\n",
      "(No options suggestion, ensure TRADIER_ACCESS_TOKEN is set and chains are available.)\n"
     ]
    }
   ],
   "source": [
    "ticker = \"ANGO\"\n",
    "\n",
    "BASELINES_PATH = Path(\"sentiment_baselines.json\")\n",
    "\n",
    "def load_or_build_baselines(path: Path = BASELINES_PATH):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError\n",
    "    with path.open(\"r\") as f:\n",
    "        bl = json.load(f)\n",
    "    # Accept either flat or nested schema\n",
    "    some_bucket = next((k for k,v in bl.items() if isinstance(v, dict)), None)\n",
    "    if not some_bucket:\n",
    "        raise ValueError(\"Empty baselines file\")\n",
    "    b = bl[some_bucket]\n",
    "    ok_flat   = {\"intercept7\",\"slope7\",\"intercept28\",\"slope28\"}.issubset(b.keys())\n",
    "    ok_nested = (\"ret7\" in b and \"ret28\" in b and\n",
    "                 {\"intercept\",\"slope\"}.issubset(b[\"ret7\"].keys()) and\n",
    "                 {\"intercept\",\"slope\"}.issubset(b[\"ret28\"].keys()))\n",
    "    if not (ok_flat or ok_nested):\n",
    "        print(\"[warn] baselines present but unknown schema; rebuildingâ€¦\")\n",
    "        raise ValueError(\"bad schema\")\n",
    "    return bl\n",
    "\n",
    "try:\n",
    "    baselines = load_or_build_baselines()\n",
    "    print(\"[ok] loaded sentiment baselines from file\")\n",
    "except Exception as e:\n",
    "    print(f\"[info] {e}. Building baselines nowâ€¦ (this may take a while)\")\n",
    "    # If you have this helper, call it; otherwise comment this out and load from file instead.\n",
    "    baselines = run_full_baseline()            # <- your existing function\n",
    "    with BASELINES_PATH.open(\"w\") as f:\n",
    "        json.dump(baselines, f, indent=2)\n",
    "    print(f\"[ok] baselines saved to {BASELINES_PATH}\")\n",
    "\n",
    "pred = run_predict([ticker], w_short=\"auto\", suggest_spread=False, baselines=baselines, n_filings=3).iloc[0].to_dict()\n",
    "print(\"sector:\", pred[\"sector_used\"], \"| cap bucket:\", pred[\"cap_bucket\"], \"| w_short:\", pred[\"w_short\"])\n",
    "\n",
    "def print_valuation_summary(pred: dict):\n",
    "    print(\"=== Prediction Summary ===\")\n",
    "    print(f\"Ticker: {pred.get('ticker', 'â€”')}\")\n",
    "    print(f\"Cap bucket: {pred.get('cap_bucket', 'â€”')} | Market cap: {pred.get('market_cap', 'â€”'):,}\")\n",
    "    print(f\"Asent_overall: {pred.get('asent_overall', float('nan')):.4f}\")\n",
    "    print(f\"Pred 7d: {pred.get('pred_ret_7d_pct', float('nan')):.2f}% | Pred 28d: {pred.get('pred_ret_28d_pct', float('nan')):.2f}%\")\n",
    "    print(f\"Price now: {pred.get('price_now', float('nan')):.2f}\")\n",
    "\n",
    "    # renamed keys here:\n",
    "    if 'dcf_static' in pred:\n",
    "        print(f\"DCF (static) fair value: {pred['dcf_static']:.2f}\")\n",
    "    if 'multiples_mid' in pred:\n",
    "        print(f\"Multiples mid: {pred['multiples_mid']:.2f}\")\n",
    "    if 'price_28d_target' in pred:\n",
    "        print(f\"28d target: {pred['price_28d_target']:.2f}\")\n",
    "\n",
    "    # dynamic DCF diagnostics (new fields)\n",
    "    if 'dcf_bands' in pred:\n",
    "        b = pred['dcf_bands']\n",
    "        print(f\"DCF bands â€” Low: {b.get('low','â€”'):.2f} | Mid: {b.get('mid','â€”'):.2f} | High: {b.get('high','â€”'):.2f}\")\n",
    "    if 'disc_rate_r' in pred:\n",
    "        print(f\"r: {pred['disc_rate_r']:.2%} | g: {pred.get('growth_g', float('nan')):.2%} | g_term: {pred.get('terminal_g', float('nan')):.2%} | vol_ann: {pred.get('vol_annual', float('nan')):.2%}\")\n",
    "\n",
    "    # final blend\n",
    "    if 'final_blended_target' in pred:\n",
    "        print(f\"Final blended target: {pred['final_blended_target']:.2f}, {pred.get('final_direction','â€”')}\")\n",
    "\n",
    "    # optional options suggestion\n",
    "    ov = pred.get(\"options_vertical\")\n",
    "    if ov:\n",
    "        print(\"\\n== Suggested Options Vertical ==\")\n",
    "        print(f\"Type: {ov.get('type','â€”')}  |  Exp: {ov.get('expiration','â€”')}\")\n",
    "        print(f\"Buy: {ov.get('buy_strike','â€”')}  |  Sell: {ov.get('sell_strike','â€”')}\")\n",
    "        print(f\"Debit: {ov.get('debit','â€”')}  |  Width: {ov.get('width','â€”')}  |  Max Profit: {ov.get('max_profit','â€”')}\")\n",
    "        print(f\"R:R (profit/risk): {ov.get('R_by_Risk','â€”')}\")\n",
    "    else:\n",
    "        print(\"\\n(No options suggestion, ensure TRADIER_ACCESS_TOKEN is set and chains are available.)\")\n",
    "\n",
    "# call it\n",
    "print_valuation_summary(pred)\n",
    "\n",
    "ov = pred.get(\"options_vertical\")\n",
    "if ov:\n",
    "    print(\"\\n=== Suggested Options Vertical ===\")\n",
    "    print(f\"Type: {ov['type']}  |  Exp: {ov['expiration']}\")\n",
    "    print(f\"Buy {ov['buy_strike']}  /  Sell {ov['sell_strike']}\")\n",
    "    print(f\"Debit: {ov['debit']}  |  Width: {ov['width']}  |  Max Profit: {ov['max_profit']}\")\n",
    "    print(f\"R:R (profit/risk): {ov['R_by_Risk']}\")\n",
    "else:\n",
    "    print(\"\\n(No options suggestion, ensure TRADIER_ACCESS_TOKEN is set and chains are available.)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
