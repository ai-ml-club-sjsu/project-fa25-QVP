{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28f41ef",
   "metadata": {},
   "source": [
    "# 10Q Sentiment & DCF Analysis\n",
    "This notebook analyzes the DCF of a 10Q as well as the sentiment of the writings within the report for a given ticker and predicts its future price movement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e514451e",
   "metadata": {},
   "source": [
    "### Imports & Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbd2938",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, sys, time, math, json, warnings, requests\n",
    "from datetime import datetime, timedelta, timezone, date\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "    datefmt=\"%H:%M:%S\"\n",
    ")\n",
    "log = logging.getLogger(\"10Q-DCF\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "load_dotenv()\n",
    "\n",
    "SEC_EMAIL = os.getenv(\"SEC_EMAIL\")\n",
    "POLYGON_API_KEY = os.getenv(\"POLYGON_API_KEY\")\n",
    "\n",
    "TRADIER_ACCESS_TOKEN = os.getenv(\"TRADIER_ACCESS_TOKEN\")\n",
    "TRADIER_ACCOUNT_ID = os.getenv(\"TRADIER_ACCOUNT_ID\")\n",
    "TRADIER_BASE = os.getenv(\"TRADIER_BASE\")\n",
    "\n",
    "def _tradier_headers():\n",
    "    if not TRADIER_ACCESS_TOKEN:\n",
    "        raise RuntimeError(\"Missing TRADIER_ACCESS_TOKEN in env.\")\n",
    "    return {\n",
    "        \"Authorization\": f\"Bearer {TRADIER_ACCESS_TOKEN}\",\n",
    "        \"Accept\": \"application/json\",\n",
    "    }\n",
    "\n",
    "FINBERT_ID = \"yiyanghkust/finbert-tone\"\n",
    "SECTION_PATTERNS = [\n",
    "    (r\"item\\s+2\\.\\s*management[’']?s discussion and analysis.*?(?=item\\s+3\\.)\", \"MD&A\"),\n",
    "    (r\"item\\s+1a\\.\\s*risk factors.*?(?=item\\s+2\\.)\", \"RiskFactors\"),\n",
    "    (r\"results of operations.*?(?=liquidity|capital resources|item\\s+\\d)\", \"Results\"),\n",
    "]\n",
    "POS_PHRASES = [r\"strong demand\", r\"margin expansion\", r\"raised guidance\", r\"record (revenue|earnings)\", r\"cost (reductions|optimization)\", r\"share repurchase\", r\"cash flow (improved|growth)\"]\n",
    "NEG_PHRASES = [r\"decline in (sales|revenue)\", r\"margin compression\", r\"impairment charge\", r\"supply chain disruption\", r\"adversely affected\", r\"weaker demand\", r\"material weakness\"]\n",
    "\n",
    "def SEC_HEADERS():\n",
    "    return {\n",
    "        \"User-Agent\": f\"Severin Spagnola (contact: {SEC_EMAIL})\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate\",\n",
    "    }\n",
    "\n",
    "def cap_bucket(mc):\n",
    "    if mc is None or not np.isfinite(mc): return \"unknown\"\n",
    "    mc_b = mc / 1e9\n",
    "    if mc_b < 0.3:  return \"micro\"\n",
    "    if mc_b < 2:    return \"small\"\n",
    "    if mc_b < 10:   return \"mid\"\n",
    "    if mc_b < 200:  return \"large\"\n",
    "    return \"mega\"\n",
    "\n",
    "BACKTEST_CSV = \"10q_sentiment_event_returns.csv\"\n",
    "BASELINES_JSON = \"sentiment_baselines.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aec4208",
   "metadata": {},
   "source": [
    "### SEC + HTML Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "927bff7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik(ticker: str) -> str:\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    js = requests.get(url, headers=SEC_HEADERS(), timeout=30).json()\n",
    "    t = ticker.upper()\n",
    "    for _, rec in js.items():\n",
    "        if rec.get(\"ticker\",\"\").upper() == t:\n",
    "            return str(rec[\"cik_str\"]).zfill(10)\n",
    "    raise ValueError(f\"CIK not found for {ticker}\")\n",
    "\n",
    "def list_10q_with_dates(cik: str, max_n=8):\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    rec = r.json().get(\"filings\",{}).get(\"recent\",{})\n",
    "    out = []\n",
    "    for form, acc, prim, fdate in zip(rec.get(\"form\",[]), rec.get(\"accessionNumber\",[]), rec.get(\"primaryDocument\",[]), rec.get(\"filingDate\",[])):\n",
    "        if form == \"10-Q\":\n",
    "            out.append({\"accession\": acc.replace(\"-\",\"\"), \"primary\": prim, \"filing_date\": fdate})\n",
    "        if len(out) >= max_n: break\n",
    "    return out\n",
    "\n",
    "def fetch_filing_html(cik:str, accession:str, primary:str) -> str:\n",
    "    base = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession}\"\n",
    "    url  = f\"{base}/{primary}\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=60); r.raise_for_status()\n",
    "    return r.text\n",
    "\n",
    "def _make_soup(html: str) -> BeautifulSoup:\n",
    "    for parser in (\"lxml\", \"html5lib\", \"html.parser\"):\n",
    "        try:\n",
    "            return BeautifulSoup(html, parser)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "def _lower_clean(txt: str) -> str:\n",
    "    return re.sub(r\"[ \\t]+\",\" \", txt.lower())\n",
    "\n",
    "def extract_sections(html: str, patterns=SECTION_PATTERNS, fallback_full=True, cap=60000) -> dict:\n",
    "    soup = _make_soup(html)\n",
    "    txt  = soup.get_text(\"\\n\", strip=True)\n",
    "    low  = _lower_clean(txt)\n",
    "    out = {}\n",
    "    for pat, name in patterns:\n",
    "        m = re.search(pat, low, flags=re.S)\n",
    "        if m: out[name] = low[m.start():m.end()][:cap]\n",
    "    if not out and fallback_full: out[\"FullDocument\"] = low[:cap]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0f27e7",
   "metadata": {},
   "source": [
    "### FinBERT Loader + Long-Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7bdf501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_finbert():\n",
    "    tok = AutoTokenizer.from_pretrained(FINBERT_ID)\n",
    "    mdl = AutoModelForSequenceClassification.from_pretrained(FINBERT_ID)\n",
    "    pipe = TextClassificationPipeline(model=mdl, tokenizer=tok, top_k=None, truncation=True)\n",
    "    return pipe, tok\n",
    "\n",
    "def _token_chunks(text: str, tokenizer, max_tokens=512, stride=32):\n",
    "    ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    step = max_tokens - stride\n",
    "    for i in range(0, len(ids), step):\n",
    "        window = ids[i:i+max_tokens]\n",
    "        if not window: break\n",
    "        yield tokenizer.decode(window, skip_special_tokens=True)\n",
    "\n",
    "def finbert_sent_long(text: str, pipe, tokenizer, max_tokens=512, batch=16):\n",
    "    if len(text) < 4000:\n",
    "        rows = pipe([text], truncation=True, max_length=max_tokens)\n",
    "    else:\n",
    "        chunks = list(_token_chunks(text, tokenizer, max_tokens=max_tokens))\n",
    "        rows = []\n",
    "        for i in range(0, len(chunks), batch):\n",
    "            rows.extend(pipe(chunks[i:i+batch], truncation=True, max_length=max_tokens))\n",
    "    pos = neu = neg = 0.0\n",
    "    for r in rows:\n",
    "        d = {x[\"label\"].lower(): x[\"score\"] for x in r}\n",
    "        pos += d.get(\"positive\",0.0); neu += d.get(\"neutral\",0.0); neg += d.get(\"negative\",0.0)\n",
    "    n = max(1, len(rows))\n",
    "    return {\"pos\":pos/n, \"neu\":neu/n, \"neg\":neg/n, \"sent_score\":pos/n - neg/n}\n",
    "\n",
    "def phrase_boost(text: str, pos_list=POS_PHRASES, neg_list=NEG_PHRASES, w=0.1) -> float:\n",
    "    boost = 0.0\n",
    "    for p in pos_list:\n",
    "        if re.search(p, text, flags=re.I): boost += w\n",
    "    for n in neg_list:\n",
    "        if re.search(n, text, flags=re.I): boost -= w\n",
    "    return boost\n",
    "\n",
    "def score_sections(sections: dict, pipe_tok=None) -> dict:\n",
    "    pipe, tok = pipe_tok if pipe_tok else load_finbert()\n",
    "    feats = {}\n",
    "    for name, text in sections.items():\n",
    "        fb = finbert_sent_long(text, pipe, tok, max_tokens=512, batch=16)\n",
    "        boost = phrase_boost(text)\n",
    "        feats[f\"{name}_pos\"] = fb[\"pos\"]; feats[f\"{name}_neg\"] = fb[\"neg\"]\n",
    "        base = fb.get(\"sent_score\", 0.0)\n",
    "        feats[f\"{name}_sent\"] = base + boost\n",
    "        feats[f\"{name}_boost\"] = boost\n",
    "    sents = [v for k,v in feats.items() if k.endswith(\"_sent\")]\n",
    "    feats[\"sent_overall\"] = float(np.mean(sents)) if sents else np.nan\n",
    "    return feats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92490187",
   "metadata": {},
   "source": [
    "### Price Data from Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a7a2eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_daily(ticker: str, start: str, end: str) -> pd.DataFrame:\n",
    "    url = f\"https://api.polygon.io/v2/aggs/ticker/{ticker.upper()}/range/1/day/{start}/{end}\"\n",
    "    params = {\"adjusted\":\"true\",\"sort\":\"asc\",\"limit\":50000,\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=30); r.raise_for_status()\n",
    "    rows = r.json().get(\"results\", []) or []\n",
    "    if not rows: return pd.DataFrame(columns=[\"date\",\"close\"])\n",
    "    df = pd.DataFrame(rows)[[\"t\",\"c\"]]\n",
    "    df[\"date\"] = pd.to_datetime(df[\"t\"], unit=\"ms\", utc=True).dt.tz_convert(\"US/Eastern\").dt.date\n",
    "    return df.drop(columns=[\"t\"]).rename(columns={\"c\":\"close\"}).drop_duplicates(\"date\")\n",
    "\n",
    "def polygon_latest_close(ticker: str, lookback_days: int = 14):\n",
    "    \"\"\"Get the most recent daily close from Polygon within the last N days.\"\"\"\n",
    "    end = date.today()\n",
    "    start = end - timedelta(days=lookback_days)\n",
    "    df = polygon_daily(ticker, start.isoformat(), end.isoformat())\n",
    "    if df.empty: return None\n",
    "    return float(df.iloc[-1][\"close\"])\n",
    "\n",
    "def next_trading_close(df: pd.DataFrame, target_date: date):\n",
    "    s = df[df[\"date\"] >= target_date]\n",
    "    return None if s.empty else float(s.iloc[0][\"close\"])\n",
    "\n",
    "def event_closes(ticker: str, filing_date: str) -> dict:\n",
    "    d0 = datetime.strptime(filing_date, \"%Y-%m-%d\").date()\n",
    "    d7 = d0 + timedelta(days=7)\n",
    "    d28= d0 + timedelta(days=28)\n",
    "    start = (d0 - timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    end   = (d28 + timedelta(days=5)).strftime(\"%Y-%m-%d\")\n",
    "    df = polygon_daily(ticker, start, end)\n",
    "    if df.empty: return {\"close_0\":np.nan,\"close_7\":np.nan,\"close_28\":np.nan}\n",
    "    return {\"close_0\": next_trading_close(df,d0),\n",
    "            \"close_7\": next_trading_close(df,d7),\n",
    "            \"close_28\":next_trading_close(df,d28)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac8e16b",
   "metadata": {},
   "source": [
    "### CSV Backtest Builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0cf01121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_10q_sentiment(tickers, max_filings=6, sleep_sec=0.3):\n",
    "    pipe_tok = load_finbert()\n",
    "    rows = []\n",
    "    for t in tickers:\n",
    "        print(f\"[{t}] pulling 10-Qs…\")\n",
    "        cik = get_cik(t)\n",
    "        pairs = list_10q_with_dates(cik, max_n=max_filings)\n",
    "        for p in pairs:\n",
    "            try:\n",
    "                html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "            except Exception as e:\n",
    "                print(f\"  skip {p['accession']} ({e})\"); continue\n",
    "            secs  = extract_sections(html)\n",
    "            feats = score_sections(secs, pipe_tok=pipe_tok)\n",
    "            px    = event_closes(t, p[\"filing_date\"])\n",
    "            row   = {\"ticker\":t,\"cik\":cik, **p, **feats, **px}\n",
    "            c0,c7,c28 = row[\"close_0\"], row[\"close_7\"], row[\"close_28\"]\n",
    "            row[\"ret_7\"]  = (c7/c0 - 1.0)*100 if c0 and c7 else np.nan\n",
    "            row[\"ret_28\"] = (c28/c0 - 1.0)*100 if c0 and c28 else np.nan\n",
    "            rows.append(row)\n",
    "            time.sleep(sleep_sec)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(BACKTEST_CSV, index=False)\n",
    "    print(f\"Saved {BACKTEST_CSV} with {len(df)} rows.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419f5272",
   "metadata": {},
   "source": [
    "### Backtest Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67c51108",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "\n",
    "def run_legitimacy_checks(path=BACKTEST_CSV):\n",
    "    df = pd.read_csv(path)\n",
    "    df[\"filing_date\"] = pd.to_datetime(df[\"filing_date\"])\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\"])\n",
    "    sent_cols = [c for c in df.columns if c.endswith(\"_sent\")]\n",
    "    def winsorize(s, p=0.01):\n",
    "        lo, hi = s.quantile(p), s.quantile(1-p)\n",
    "        return s.clip(lo, hi)\n",
    "    df[\"ret_7_w\"]  = winsorize(df[\"ret_7\"])\n",
    "    df[\"ret_28_w\"] = winsorize(df[\"ret_28\"])\n",
    "\n",
    "    def corr_table(y_col):\n",
    "        rows=[]\n",
    "        for c in sent_cols + ([\"sent_overall\"] if \"sent_overall\" in df.columns else []):\n",
    "            x,y = df[c], df[y_col]\n",
    "            m = x.notna() & y.notna()\n",
    "            if m.sum() < 8: rows.append((c, np.nan, np.nan)); continue\n",
    "            r,p = pearsonr(x[m], y[m])\n",
    "            rows.append((c,r,p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    corr7, corr28 = corr_table(\"ret_7_w\"), corr_table(\"ret_28_w\")\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Δsent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    def corr_delta(y_col):\n",
    "        rows=[]\n",
    "        x,y = df[\"Δsent_overall\"], df[y_col]\n",
    "        m = x.notna() & y.notna()\n",
    "        if m.sum() >= 8:\n",
    "            r,p = pearsonr(x[m], y[m]); rows.append((\"Δsent_overall\", r, p))\n",
    "        return pd.DataFrame(rows, columns=[\"feature\",\"pearson_r\",\"p_value\"]).sort_values(\"pearson_r\", ascending=False)\n",
    "\n",
    "    dc7, dc28 = corr_delta(\"ret_7_w\"), corr_delta(\"ret_28_w\")\n",
    "    return {\"corr7\":corr7, \"corr28\":corr28, \"dc7\":dc7, \"dc28\":dc28}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3eb0ac",
   "metadata": {},
   "source": [
    "### Market Cap Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7ade05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_market_cap(ticker: str):\n",
    "    url = f\"https://api.polygon.io/v3/reference/tickers/{ticker.upper()}\"\n",
    "    params = {\"apiKey\": POLYGON_API_KEY}\n",
    "    r = requests.get(url, params=params, timeout=20)\n",
    "    try:\n",
    "        js = r.json().get(\"results\", {})\n",
    "        return js.get(\"market_cap\", None)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def fit_bucket_baselines(path=BACKTEST_CSV, min_per_bucket=8):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna(subset=[\"ret_7\",\"ret_28\",\"sent_overall\"])\n",
    "    df = df.sort_values([\"ticker\",\"filing_date\"])\n",
    "    df[\"Δsent_overall\"] = df.groupby(\"ticker\")[\"sent_overall\"].diff()\n",
    "\n",
    "    # attach current market cap + bucket\n",
    "    mcap = {}\n",
    "    for t in df[\"ticker\"].unique():\n",
    "        mcap[t] = polygon_market_cap(t)\n",
    "        time.sleep(0.2)\n",
    "    df[\"market_cap\"] = df[\"ticker\"].map(mcap)\n",
    "    df[\"bucket\"] = df[\"market_cap\"].map(cap_bucket)\n",
    "\n",
    "    out = {}\n",
    "    for bucket in df[\"bucket\"].dropna().unique():\n",
    "        d = df[df[\"bucket\"]==bucket].dropna(subset=[\"Δsent_overall\",\"ret_7\",\"ret_28\"])\n",
    "        if len(d) < min_per_bucket:\n",
    "            continue\n",
    "        X = np.c_[np.ones(len(d)), d[\"Δsent_overall\"].values]\n",
    "        for horizon, ycol in ((\"ret7\",\"ret_7\"), (\"ret28\",\"ret_28\")):\n",
    "            y = d[ycol].values\n",
    "            beta = np.linalg.pinv(X).dot(y)  # [intercept, slope]\n",
    "            intercept, slope = float(beta[0]), float(beta[1])\n",
    "            out.setdefault(bucket, {})[horizon] = {\"intercept\": intercept, \"slope\": slope, \"n\": int(len(d))}\n",
    "    with open(BASELINES_JSON,\"w\") as f:\n",
    "        json.dump(out, f, indent=2)\n",
    "    print(f\"Saved {BASELINES_JSON}\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d9a81b",
   "metadata": {},
   "source": [
    "### Fundamentals + DCF Snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ee4e413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# H) FUNDAMENTALS + DCF SNAPSHOT (TTM from SEC facts)\n",
    "\n",
    "def get_company_facts(cik: str) -> dict:\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik}.json\"\n",
    "    r = requests.get(url, headers=SEC_HEADERS(), timeout=30); r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def _ttm_sum(items, n=4):\n",
    "    if not items: return None\n",
    "    vals = [x.get(\"val\") for x in items][-n:]\n",
    "    vals = [v for v in vals if v is not None]\n",
    "    return float(np.nansum(vals)) if vals else None\n",
    "\n",
    "def build_ttm_metrics(facts: dict) -> dict:\n",
    "    usgaap = facts.get(\"facts\", {}).get(\"us-gaap\", {})\n",
    "    def get_series(tag):\n",
    "        return (usgaap.get(tag, {}).get(\"units\", {}).get(\"USD\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"USD/shares\", []) or\n",
    "                usgaap.get(tag, {}).get(\"units\", {}).get(\"shares\", []))\n",
    "    revenue_q   = get_series(\"Revenues\")\n",
    "    ni_q        = get_series(\"NetIncomeLoss\")\n",
    "    eps_q       = (usgaap.get(\"EarningsPerShareDiluted\", {}).get(\"units\", {}).get(\"USD/shares\", []) or [])\n",
    "    dil_sh_q    = get_series(\"WeightedAverageNumberOfDilutedSharesOutstanding\")\n",
    "    cfo_q       = get_series(\"NetCashProvidedByUsedInOperatingActivities\")\n",
    "    capex_q     = get_series(\"PaymentsToAcquirePropertyPlantAndEquipment\")\n",
    "\n",
    "    revenue_ttm     = _ttm_sum(revenue_q)\n",
    "    net_income_ttm  = _ttm_sum(ni_q)\n",
    "    eps_ttm         = _ttm_sum(eps_q)\n",
    "    diluted_sh_ttm  = _ttm_sum(dil_sh_q)\n",
    "    cfo_ttm         = _ttm_sum(cfo_q)\n",
    "    capex_ttm       = _ttm_sum(capex_q)\n",
    "    fcf_ttm         = (cfo_ttm or 0.0) - abs(capex_ttm or 0.0)\n",
    "\n",
    "    if eps_ttm and diluted_sh_ttm and net_income_ttm:\n",
    "        approx = eps_ttm * diluted_sh_ttm\n",
    "        if abs(approx - net_income_ttm)/max(1.0, net_income_ttm) > 0.15:\n",
    "            print(\"[warn] EPS*Shares != NetIncome by >15%. Check tags/periods.\")\n",
    "\n",
    "    return dict(\n",
    "        revenue_ttm=revenue_ttm, net_income_ttm=net_income_ttm,\n",
    "        eps_diluted_ttm=eps_ttm, diluted_shares_ttm=diluted_sh_ttm,\n",
    "        cfo_ttm=cfo_ttm, capex_ttm=capex_ttm, fcf_ttm=fcf_ttm,\n",
    "        rev_per_share=(revenue_ttm / diluted_sh_ttm) if (revenue_ttm and diluted_sh_ttm) else None,\n",
    "    )\n",
    "\n",
    "SECTOR_MULTIPLES = {\"Technology\":{\"PE\":30.0,\"PS\":6.8}, \"_default\":{\"PE\":18.0,\"PS\":2.5}}\n",
    "\n",
    "def multiples_anchor(metrics:dict, sector=\"Technology\"):\n",
    "    cfg = SECTOR_MULTIPLES.get(sector, SECTOR_MULTIPLES[\"_default\"])\n",
    "    eps = metrics.get(\"eps_diluted_ttm\")\n",
    "    rps = metrics.get(\"rev_per_share\")\n",
    "    pe_anchor = eps * cfg[\"PE\"] if eps else None\n",
    "    ps_anchor = rps * cfg[\"PS\"] if rps else None\n",
    "    anchors = [x for x in (pe_anchor, ps_anchor) if x is not None and math.isfinite(x)]\n",
    "    mid = float(np.mean(anchors)) if anchors else None\n",
    "    return {\"pe_anchor\":pe_anchor, \"ps_anchor\":ps_anchor, \"fair_value_mid\":mid, \"assumptions\":cfg}\n",
    "\n",
    "def dcf_anchor(metrics:dict, years=5, g=0.04, r=0.095, g_term=0.02):\n",
    "    fcf0 = metrics.get(\"fcf_ttm\")\n",
    "    sh   = metrics.get(\"diluted_shares_ttm\")\n",
    "    if not fcf0 or not sh or sh <= 0: return None\n",
    "    pv, fcf = 0.0, fcf0\n",
    "    for t in range(1, years+1):\n",
    "        fcf *= (1+g)\n",
    "        pv  += fcf / ((1+r)**t)\n",
    "    terminal = (fcf * (1+g_term)) / (r - g_term)\n",
    "    pv_term  = terminal / ((1+r)**years)\n",
    "    return (pv + pv_term) / sh\n",
    "\n",
    "def blended_fair_value(mult_mid, dcf_val, w=0.5):\n",
    "    if mult_mid is None and dcf_val is None: return None\n",
    "    if mult_mid is None: return dcf_val\n",
    "    if dcf_val  is None: return mult_mid\n",
    "    return w*mult_mid + (1-w)*dcf_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951ab4ed",
   "metadata": {},
   "source": [
    "### Single Ticker Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c8176f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def latest_two_10q_delta_sent(ticker: str):\n",
    "    cik   = get_cik(ticker)\n",
    "    pairs = list_10q_with_dates(cik, max_n=2)\n",
    "    if not pairs: return np.nan, np.nan, np.nan\n",
    "    pipe_tok = load_finbert()\n",
    "    scores = []\n",
    "    for p in pairs:\n",
    "        html = fetch_filing_html(cik, p[\"accession\"], p[\"primary\"])\n",
    "        secs = extract_sections(html)\n",
    "        feats= score_sections(secs, pipe_tok=pipe_tok)\n",
    "        scores.append(feats[\"sent_overall\"])\n",
    "        time.sleep(0.3)\n",
    "    if len(scores)==1: return scores[0], np.nan, np.nan\n",
    "    return scores[0], scores[1], scores[0]-scores[1]\n",
    "\n",
    "def load_baselines(path=BASELINES_JSON):\n",
    "    if not os.path.exists(path): raise FileNotFoundError(\"Run cell G to fit/save sentiment baselines first.\")\n",
    "    with open(path,\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def predict_ticker(ticker: str, w_short=0.30, sector=\"Technology\"):\n",
    "    sent_now, sent_prev, d_sent = latest_two_10q_delta_sent(ticker)\n",
    "    mc   = polygon_market_cap(ticker)\n",
    "    bkt  = cap_bucket(mc)\n",
    "    base = load_baselines().get(bkt, {})\n",
    "    a7,b7   = base.get(\"ret7\",{}).get(\"intercept\",0.0),  base.get(\"ret7\",{}).get(\"slope\",0.0)\n",
    "    a28,b28 = base.get(\"ret28\",{}).get(\"intercept\",0.0), base.get(\"ret28\",{}).get(\"slope\",0.0)\n",
    "    pred7   = a7  + b7  * (d_sent if np.isfinite(d_sent) else 0.0)\n",
    "    pred28  = a28 + b28 * (d_sent if np.isfinite(d_sent) else 0.0)\n",
    "\n",
    "    # Valuation\n",
    "    cik   = get_cik(ticker)\n",
    "    facts = get_company_facts(cik)\n",
    "    gaap  = build_ttm_metrics(facts)\n",
    "    mult  = multiples_anchor(gaap, sector=sector)\n",
    "    dcfv  = dcf_anchor(gaap, years=5, g=0.04, r=0.095, g_term=0.02)\n",
    "    blend = blended_fair_value(mult.get(\"fair_value_mid\"), dcfv, w=0.5)\n",
    "\n",
    "    # Price now from Polygon only\n",
    "    pnow = polygon_latest_close(ticker)\n",
    "    price_28d = pnow*(1+pred28/100) if pnow and np.isfinite(pred28) else None\n",
    "    final_target = (w_short*price_28d + (1-w_short)*blend) if (price_28d and blend) else (price_28d or blend)\n",
    "    direction = \"UP\" if (final_target and pnow and final_target > pnow) else \"DOWN\"\n",
    "\n",
    "    return {\n",
    "        \"ticker\": ticker.upper(),\n",
    "        \"market_cap\": mc, \"cap_bucket\": bkt,\n",
    "        \"sent_overall_now\": sent_now, \"sent_overall_prev\": sent_prev, \"Δsent_overall\": d_sent,\n",
    "        \"pred_ret_7d_pct\": pred7, \"pred_ret_28d_pct\": pred28,\n",
    "        \"price_now\": pnow, \"price_28d_target\": price_28d,\n",
    "        \"dcf_fair_value\": dcfv, \"multiples_mid\": mult.get(\"fair_value_mid\"),\n",
    "        \"final_blended_target\": final_target, \"final_direction\": direction,\n",
    "        \"baseline_used\": base\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47056749",
   "metadata": {},
   "source": [
    "### Options Spread Creator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fd9af39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tradier_expirations(ticker: str):\n",
    "    \"\"\"List available expiration dates (YYYY-MM-DD) for a symbol.\"\"\"\n",
    "    url = f\"{TRADIER_BASE}/markets/options/expirations\"\n",
    "    r = requests.get(url, headers=_tradier_headers(),\n",
    "                     params={\"symbol\": ticker.upper(), \"includeAll\": \"false\"},\n",
    "                     timeout=20)\n",
    "    r.raise_for_status()\n",
    "    data = r.json().get(\"expirations\", {})\n",
    "    exps = data.get(\"date\", [])\n",
    "    # API returns a string or list depending on count\n",
    "    return exps if isinstance(exps, list) else ([exps] if exps else [])\n",
    "\n",
    "def tradier_chain(ticker: str, expiration: str):\n",
    "    \"\"\"Get the option chain for a given expiration.\"\"\"\n",
    "    url = f\"{TRADIER_BASE}/markets/options/chains\"\n",
    "    r = requests.get(url, headers=_tradier_headers(),\n",
    "                     params={\"symbol\": ticker.upper(), \"expiration\": expiration, \"greeks\": \"false\"},\n",
    "                     timeout=30)\n",
    "    r.raise_for_status()\n",
    "    opt = r.json().get(\"options\", {}).get(\"option\", [])\n",
    "    return opt if isinstance(opt, list) else ([opt] if opt else [])\n",
    "\n",
    "def best_vertical_by_target(ticker: str, pred_pct: float, polygon_price_fn, horizon_days=28):\n",
    "    \"\"\"\n",
    "    If pred_pct > 0 => bull call vertical. If < 0 => bear put vertical.\n",
    "    'polygon_price_fn' should be a callable like polygon_latest_close(ticker)->float.\n",
    "    \"\"\"\n",
    "    # pick an expiration ~horizon_days out (nearest available)\n",
    "    today = date.today()\n",
    "    target = today + timedelta(days=horizon_days)\n",
    "    exps = tradier_expirations(ticker)\n",
    "    if not exps:\n",
    "        return None\n",
    "    # choose the expiration closest to 'target' that is >= today\n",
    "    def to_date(s): \n",
    "        y,m,d = map(int, s.split(\"-\")); \n",
    "        return date(y,m,d)\n",
    "    future_exps = [e for e in exps if to_date(e) >= today]\n",
    "    if not future_exps:\n",
    "        return None\n",
    "    exp = min(future_exps, key=lambda s: abs(to_date(s)-target))\n",
    "\n",
    "    chain = tradier_chain(ticker, exp)\n",
    "    if not chain:\n",
    "        return None\n",
    "\n",
    "    pnow = polygon_price_fn(ticker)\n",
    "    if not pnow:\n",
    "        return None\n",
    "    p_tgt = pnow * (1 + pred_pct/100.0)\n",
    "\n",
    "    # strikes universe\n",
    "    strikes = sorted({float(o[\"strike\"]) for o in chain if \"strike\" in o})\n",
    "    if not strikes:\n",
    "        return None\n",
    "    nearest = lambda x: min(strikes, key=lambda k: abs(k - x))\n",
    "\n",
    "    if pred_pct >= 0:\n",
    "        k_buy  = nearest(pnow * 0.99)\n",
    "        k_sell = nearest(p_tgt * 1.02)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"call\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bull_call\"\n",
    "    else:\n",
    "        k_buy  = nearest(pnow * 1.01)\n",
    "        k_sell = nearest(p_tgt * 0.98)\n",
    "        leg_buy  = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_buy]\n",
    "        leg_sell = [o for o in chain if o.get(\"option_type\")==\"put\" and float(o[\"strike\"])==k_sell]\n",
    "        spread_type = \"bear_put\"\n",
    "\n",
    "    if not leg_buy or not leg_sell:\n",
    "        return None\n",
    "\n",
    "    def mid(q):\n",
    "        b = float(q.get(\"bid\", 0.0)); a = float(q.get(\"ask\", 0.0))\n",
    "        return (b + a)/2 if (a and b) else float(q.get(\"last\", 0.0))\n",
    "\n",
    "    debit = max(0.01, mid(leg_buy[0]) - mid(leg_sell[0]))\n",
    "    width = abs(k_sell - k_buy)\n",
    "    max_profit = max(0.0, width - debit)\n",
    "    rr = (max_profit / debit) if debit > 0 else None\n",
    "\n",
    "    return {\n",
    "        \"type\": spread_type,\n",
    "        \"expiration\": exp,\n",
    "        \"buy_strike\": float(k_buy),\n",
    "        \"sell_strike\": float(k_sell),\n",
    "        \"debit\": round(debit, 2),\n",
    "        \"width\": float(width),\n",
    "        \"max_profit\": round(max_profit, 2),\n",
    "        \"R_by_Risk\": round(rr, 2) if rr else None,\n",
    "        \"price_now\": float(pnow),\n",
    "        \"price_target\": float(p_tgt),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd55847",
   "metadata": {},
   "source": [
    "### Final Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7a2bcd5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_baseline():\n",
    "    \"\"\"\n",
    "    1. Builds backtest CSV across a mixed-cap universe.\n",
    "    2. Runs legitimacy checks (optional).\n",
    "    3. Fits per-cap baselines and saves to sentiment_baselines.json.\n",
    "    \"\"\"\n",
    "    micro = \"HZO, TIRX, STRC, GWRS, UPXI, GCTK, VTSI, HCAT, OPRX, AOUT, FCEL, HITI, WKSP\".split(\",\")\n",
    "    small = \"BLKB, HQY, PIPR, HAYW, NVCR, SMPL, MGPI, BE, PRCT, SKYW, AVAV, INMD, VRTS, CNXN, REZI, ASTE, MHO, CELH, ABM, PCT\".split(\",\")\n",
    "    mid   = \"LULU, MAR, EA, FSLR, MLM, TTWO, TDY, ENPH, ALB, DAL, CHRW, WDC, AAP, CZR, CHD, SWKS, COHR, PTC, HOLX, MKTX\".split(\",\")\n",
    "    large = \"AAPL, MSFT, AMZN, NVDA, GOOGL, META, JPM, V, JNJ, PG, XOM, UNH, PEP, KO, COST, ORCL, DIS, HD, BAC, WMT\".split(\",\")\n",
    "    TICKERS = [x.strip() for s in (micro+small+mid+large) for x in [s] if x.strip()]\n",
    "    df = backtest_10q_sentiment(TICKERS, max_filings=6)\n",
    "    print(df.head())\n",
    "    _ = run_legitimacy_checks(BACKTEST_CSV)\n",
    "    baselines = fit_bucket_baselines(BACKTEST_CSV)\n",
    "    return baselines\n",
    "\n",
    "def run_predict(tickers, w_short=0.30, sector=\"Technology\", suggest_spread=True):\n",
    "    out = []\n",
    "    for t in tickers:\n",
    "        res = predict_ticker(t, w_short=w_short, sector=sector)\n",
    "        if suggest_spread and res.get(\"pred_ret_28d_pct\") is not None:\n",
    "            res[\"options_vertical\"] = best_vertical_by_target(\n",
    "                t, res[\"pred_ret_28d_pct\"], polygon_price_fn=polygon_latest_close\n",
    "            )\n",
    "        out.append(res)\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9dbf216",
   "metadata": {},
   "source": [
    "### Create Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cef60c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HZO] pulling 10-Qs…\n",
      "[TIRX] pulling 10-Qs…\n",
      "[STRC] pulling 10-Qs…\n",
      "[GWRS] pulling 10-Qs…\n",
      "[UPXI] pulling 10-Qs…\n",
      "[GCTK] pulling 10-Qs…\n",
      "[VTSI] pulling 10-Qs…\n",
      "[HCAT] pulling 10-Qs…\n",
      "[OPRX] pulling 10-Qs…\n",
      "[AOUT] pulling 10-Qs…\n",
      "[FCEL] pulling 10-Qs…\n",
      "[HITI] pulling 10-Qs…\n",
      "[WKSP] pulling 10-Qs…\n",
      "[BLKB] pulling 10-Qs…\n",
      "[HQY] pulling 10-Qs…\n",
      "[PIPR] pulling 10-Qs…\n",
      "[HAYW] pulling 10-Qs…\n",
      "[NVCR] pulling 10-Qs…\n",
      "[SMPL] pulling 10-Qs…\n",
      "[MGPI] pulling 10-Qs…\n",
      "[BE] pulling 10-Qs…\n",
      "[PRCT] pulling 10-Qs…\n",
      "[SKYW] pulling 10-Qs…\n",
      "[AVAV] pulling 10-Qs…\n",
      "[INMD] pulling 10-Qs…\n",
      "[VRTS] pulling 10-Qs…\n",
      "[CNXN] pulling 10-Qs…\n",
      "[REZI] pulling 10-Qs…\n",
      "[ASTE] pulling 10-Qs…\n",
      "[MHO] pulling 10-Qs…\n",
      "[CELH] pulling 10-Qs…\n",
      "[ABM] pulling 10-Qs…\n",
      "[PCT] pulling 10-Qs…\n",
      "[LULU] pulling 10-Qs…\n",
      "[MAR] pulling 10-Qs…\n",
      "[EA] pulling 10-Qs…\n",
      "[FSLR] pulling 10-Qs…\n",
      "[MLM] pulling 10-Qs…\n",
      "[TTWO] pulling 10-Qs…\n",
      "[TDY] pulling 10-Qs…\n",
      "[ENPH] pulling 10-Qs…\n",
      "[ALB] pulling 10-Qs…\n",
      "[DAL] pulling 10-Qs…\n",
      "[CHRW] pulling 10-Qs…\n",
      "[WDC] pulling 10-Qs…\n",
      "[AAP] pulling 10-Qs…\n",
      "[CZR] pulling 10-Qs…\n",
      "[CHD] pulling 10-Qs…\n",
      "[SWKS] pulling 10-Qs…\n",
      "[COHR] pulling 10-Qs…\n",
      "[PTC] pulling 10-Qs…\n",
      "[HOLX] pulling 10-Qs…\n",
      "[MKTX] pulling 10-Qs…\n",
      "[AAPL] pulling 10-Qs…\n",
      "[MSFT] pulling 10-Qs…\n",
      "[AMZN] pulling 10-Qs…\n",
      "[NVDA] pulling 10-Qs…\n",
      "[GOOGL] pulling 10-Qs…\n",
      "[META] pulling 10-Qs…\n",
      "[JPM] pulling 10-Qs…\n",
      "[V] pulling 10-Qs…\n",
      "[JNJ] pulling 10-Qs…\n",
      "[PG] pulling 10-Qs…\n",
      "[XOM] pulling 10-Qs…\n",
      "[UNH] pulling 10-Qs…\n",
      "[PEP] pulling 10-Qs…\n",
      "[KO] pulling 10-Qs…\n",
      "[COST] pulling 10-Qs…\n",
      "[ORCL] pulling 10-Qs…\n",
      "[DIS] pulling 10-Qs…\n",
      "[HD] pulling 10-Qs…\n",
      "[BAC] pulling 10-Qs…\n",
      "[WMT] pulling 10-Qs…\n",
      "Saved 10q_sentiment_event_returns.csv with 413 rows.\n",
      "  ticker         cik           accession           primary filing_date  \\\n",
      "0    HZO  0001057060  000095017025098321  hzo-20250630.htm  2025-07-24   \n",
      "1    HZO  0001057060  000095017025058103  hzo-20250331.htm  2025-04-24   \n",
      "2    HZO  0001057060  000095017025008305  hzo-20241231.htm  2025-01-23   \n",
      "3    HZO  0001057060  000095017024086479  hzo-20240630.htm  2024-07-25   \n",
      "4    HZO  0001057060  000095017024048260  hzo-20240331.htm  2024-04-25   \n",
      "\n",
      "   MD&A_pos  MD&A_neg  MD&A_sent  MD&A_boost  Results_pos  ...  sent_overall  \\\n",
      "0  0.000368  0.308191  -0.507823        -0.2     0.000974  ...     -0.259106   \n",
      "1  0.002357  0.290528  -0.388171        -0.1     0.000694  ...     -0.197669   \n",
      "2  0.100569  0.212953  -0.212384        -0.1     0.001046  ...     -0.113641   \n",
      "3  0.018109  0.333522  -0.415412        -0.1     0.000824  ...     -0.213394   \n",
      "4  0.010430  0.273983  -0.363554        -0.1     0.000590  ...     -0.186576   \n",
      "\n",
      "   close_0  close_7  close_28      ret_7     ret_28  RiskFactors_pos  \\\n",
      "0    22.71    22.68     25.58  -0.132100  12.637605              NaN   \n",
      "1    22.67    22.10     21.51  -2.514336  -5.116895              NaN   \n",
      "2    32.37    30.43     27.84  -5.993204 -13.994439              NaN   \n",
      "3    37.10    32.72     30.02 -11.805930 -19.083558              NaN   \n",
      "4    25.60    25.25     27.04  -1.367188   5.625000              NaN   \n",
      "\n",
      "   RiskFactors_neg  RiskFactors_sent  RiskFactors_boost  \n",
      "0              NaN               NaN                NaN  \n",
      "1              NaN               NaN                NaN  \n",
      "2              NaN               NaN                NaN  \n",
      "3              NaN               NaN                NaN  \n",
      "4              NaN               NaN                NaN  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Saved sentiment_baselines.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'mid': {'ret7': {'intercept': 0.6646190032519315,\n",
       "   'slope': 1.7611971977397842,\n",
       "   'n': 69},\n",
       "  'ret28': {'intercept': 2.3405542204124696,\n",
       "   'slope': 10.91955421084191,\n",
       "   'n': 69}},\n",
       " 'mega': {'ret7': {'intercept': 0.6368375467036415,\n",
       "   'slope': -3.967231884785469,\n",
       "   'n': 89},\n",
       "  'ret28': {'intercept': 2.044963250329099,\n",
       "   'slope': -3.623117976094409,\n",
       "   'n': 89}},\n",
       " 'large': {'ret7': {'intercept': 1.34865014417929,\n",
       "   'slope': 23.122288040311755,\n",
       "   'n': 93},\n",
       "  'ret28': {'intercept': 5.919645811842534,\n",
       "   'slope': 28.298360316565425,\n",
       "   'n': 93}},\n",
       " 'micro': {'ret7': {'intercept': -5.021574038998796,\n",
       "   'slope': 105.3251808168595,\n",
       "   'n': 40},\n",
       "  'ret28': {'intercept': -4.520283496135282,\n",
       "   'slope': 151.7534318627472,\n",
       "   'n': 40}},\n",
       " 'small': {'ret7': {'intercept': 2.2229207879115034,\n",
       "   'slope': -14.617579781703878,\n",
       "   'n': 40},\n",
       "  'ret28': {'intercept': 3.7918126913080634,\n",
       "   'slope': -78.66805774539746,\n",
       "   'n': 40}}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_full_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f270d12e",
   "metadata": {},
   "source": [
    "### Run Predictions on Given Ticker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9f96f903",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] EPS*Shares != NetIncome by >15%. Check tags/periods.\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Severin Spaghetti\\Desktop\\project-fa25-QVP\\venv\\Lib\\site-packages\\requests\\models.py:976\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    975\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcomplexjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\__init__.py:346\u001b[39m, in \u001b[36mloads\u001b[39m\u001b[34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[39m\n\u001b[32m    343\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    344\u001b[39m         parse_int \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m parse_float \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[32m    345\u001b[39m         parse_constant \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m object_pairs_hook \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kw):\n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_decoder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:338\u001b[39m, in \u001b[36mJSONDecoder.decode\u001b[39m\u001b[34m(self, s, _w)\u001b[39m\n\u001b[32m    334\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return the Python representation of ``s`` (a ``str`` instance\u001b[39;00m\n\u001b[32m    335\u001b[39m \u001b[33;03mcontaining a JSON document).\u001b[39;00m\n\u001b[32m    336\u001b[39m \n\u001b[32m    337\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m obj, end = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_w\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m end = _w(s, end).end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_3.12.2800.0_x64__qbz5n2kfra8p0\\Lib\\json\\decoder.py:356\u001b[39m, in \u001b[36mJSONDecoder.raw_decode\u001b[39m\u001b[34m(self, s, idx)\u001b[39m\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m JSONDecodeError(\u001b[33m\"\u001b[39m\u001b[33mExpecting value\u001b[39m\u001b[33m\"\u001b[39m, s, err.value) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj, end\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mJSONDecodeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Assuming you already ran the cells that define run_predict() etc.\u001b[39;00m\n\u001b[32m      2\u001b[39m ticker = \u001b[33m\"\u001b[39m\u001b[33mAAPL\u001b[39m\u001b[33m\"\u001b[39m  \u001b[38;5;66;03m# <-- set your ticker symbol\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pred = \u001b[43mrun_predict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_short\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msector\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTechnology\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuggest_spread\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m.iloc[\u001b[32m0\u001b[39m].to_dict()\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== Prediction Summary ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTicker: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred[\u001b[33m'\u001b[39m\u001b[33mticker\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mrun_predict\u001b[39m\u001b[34m(tickers, w_short, sector, suggest_spread)\u001b[39m\n\u001b[32m     21\u001b[39m     res = predict_ticker(t, w_short=w_short, sector=sector)\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m suggest_spread \u001b[38;5;129;01mand\u001b[39;00m res.get(\u001b[33m\"\u001b[39m\u001b[33mpred_ret_28d_pct\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m         res[\u001b[33m\"\u001b[39m\u001b[33moptions_vertical\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mbest_vertical_by_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m            \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpred_ret_28d_pct\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolygon_price_fn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpolygon_latest_close\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     out.append(res)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(out)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mbest_vertical_by_target\u001b[39m\u001b[34m(ticker, pred_pct, polygon_price_fn, horizon_days)\u001b[39m\n\u001b[32m     29\u001b[39m today = date.today()\n\u001b[32m     30\u001b[39m target = today + timedelta(days=horizon_days)\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m exps = \u001b[43mtradier_expirations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exps:\n\u001b[32m     33\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mtradier_expirations\u001b[39m\u001b[34m(ticker)\u001b[39m\n\u001b[32m      4\u001b[39m r = requests.get(url, headers=_tradier_headers(),\n\u001b[32m      5\u001b[39m                  params={\u001b[33m\"\u001b[39m\u001b[33msymbol\u001b[39m\u001b[33m\"\u001b[39m: ticker.upper(), \u001b[33m\"\u001b[39m\u001b[33mincludeAll\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mfalse\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m      6\u001b[39m                  timeout=\u001b[32m20\u001b[39m)\n\u001b[32m      7\u001b[39m r.raise_for_status()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m data = \u001b[43mr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.get(\u001b[33m\"\u001b[39m\u001b[33mexpirations\u001b[39m\u001b[33m\"\u001b[39m, {})\n\u001b[32m      9\u001b[39m exps = data.get(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m, [])\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# API returns a string or list depending on count\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Severin Spaghetti\\Desktop\\project-fa25-QVP\\venv\\Lib\\site-packages\\requests\\models.py:980\u001b[39m, in \u001b[36mResponse.json\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    976\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m complexjson.loads(\u001b[38;5;28mself\u001b[39m.text, **kwargs)\n\u001b[32m    977\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m JSONDecodeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    978\u001b[39m     \u001b[38;5;66;03m# Catch JSON-related errors and raise as requests.JSONDecodeError\u001b[39;00m\n\u001b[32m    979\u001b[39m     \u001b[38;5;66;03m# This aliases json.JSONDecodeError and simplejson.JSONDecodeError\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m980\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m RequestsJSONDecodeError(e.msg, e.doc, e.pos)\n",
      "\u001b[31mJSONDecodeError\u001b[39m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# Assuming you already ran the cells that define run_predict() etc.\n",
    "ticker = \"AAPL\"  # <-- set your ticker symbol\n",
    "\n",
    "pred = run_predict([ticker], w_short=0.30, sector=\"Technology\", suggest_spread=True).iloc[0].to_dict()\n",
    "\n",
    "print(\"=== Prediction Summary ===\")\n",
    "print(f\"Ticker: {pred['ticker']}\")\n",
    "print(f\"Cap bucket: {pred['cap_bucket']} | Market cap: {pred['market_cap']}\")\n",
    "print(f\"Δsent_overall: {pred['Δsent_overall']:.4f}\")\n",
    "print(f\"Pred 7d: {pred['pred_ret_7d_pct']:.2f}% | Pred 28d: {pred['pred_ret_28d_pct']:.2f}%\")\n",
    "print(f\"Price now: {pred['price_now']}\")\n",
    "print(f\"DCF fair value: {pred['dcf_fair_value']}\")\n",
    "print(f\"Multiples mid: {pred['multiples_mid']}\")\n",
    "print(f\"28d target: {pred['price_28d_target']}\")\n",
    "print(f\"Final blended target: {pred['final_blended_target']}  ->  {pred['final_direction']}\")\n",
    "\n",
    "ov = pred.get(\"options_vertical\")\n",
    "if ov:\n",
    "    print(\"\\n=== Suggested Options Vertical ===\")\n",
    "    print(f\"Type: {ov['type']}  |  Exp: {ov['expiration']}\")\n",
    "    print(f\"Buy {ov['buy_strike']}  /  Sell {ov['sell_strike']}\")\n",
    "    print(f\"Debit: {ov['debit']}  |  Width: {ov['width']}  |  Max Profit: {ov['max_profit']}\")\n",
    "    print(f\"R:R (profit/risk): {ov['R_by_Risk']}\")\n",
    "else:\n",
    "    print(\"\\n(No options suggestion – ensure TRADIER_ACCESS_TOKEN is set and chains are available.)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
